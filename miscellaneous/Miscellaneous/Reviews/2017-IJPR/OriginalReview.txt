TPRS-2017-IJPR-1071

Minimizing Total Weighted Completion Time on Batch and Discrete Machines with Incompatible Job Families
*= Required Fields [Yes/No]

* 1. Does the paper make a new and significant contribution to the Production Research literature?	
Yes

* 2. Does the paper provide evidence of real or potential application for Production Systems?	
Yes

* 3. Is adequate credit given to other contributors in the field and are references sufficiently complete?	
Yes

* 4. Does the paper appropriately compare the performance of proposed methodologies with those found in the published literature?	
No

* 5. Does the paper state what the author(s) propose to do in the future?	
No

* 6. Are the character and contents of the paper clear from the title and abstract?	
Yes

* 7. Is the paper clearly, concisely, accurately, and logically written?	
Yes

* 8. Could it benefit from condensing or expansion? If yes, please explain why in the comments to author section.	
Yes

* 9. Is the subject matter of relevance to Production Research and appropriate for IJPR?	
Yes

* 10. Are all references relevant? If not please indicate in your review not relevant references.	
Yes

 
Would you be willing to review a revision of this manuscript?
[X] Yes
[ ] No

Recommendation
[ ] Accept
[X] Minor Revision
[ ] Major Revision
[ ] Reject but allow Resubmission
[ ] Reject


Confidential Comments to the EIC
Only confidential information that you do not wish to be passed to the Authors should be entered here.

I'm really hesitating between recommending a Minor or a Major revision. The article is ok to me but I think it could be improved by providing additional details and experiments as mentioned in my review.


*Comments to the Author
Please give your remarks to the authors. Without this, your review will be cancelled.

The paper defines a flow-shop problem where each job consists of two operations, one on a batch machine (with incompatible job families) and the next one on a unary machine. The objective is to minimize the weighted completion time. It then proposes:
 (1) an ILP formulation of the problem, 
 (2) 3 lower bounds (LB1,LB2,LB3), 
 (3) an approximation algorithm (GBDS) with a proved approximation ratio bounded by 4, 
 (4) a protocol for generating random instances of the problem,
 (5) an experimental study of both the lower bounds and the approximation algorithm compared to optimal solution for small problems and compared to existing metaheuristics for larger instances

I think the paper makes a good contribution to the Production Research literature because the lower bound and approximation algorithm are original (even if they strongly build on existing results for the single batch machine and the single unary machine) and the experiments show that the gap between the proposed LBs and solutions are reasonably small even for large problems.

The article is well structured and in general easy to read.

The term "discrete machine" to qualify the second machine is a bit misleading. In many scheduling contexts a discrete machine or a discrete resource is a resource that can perform several activites in parallel provided that its discrete capacity if not execeed. In the present paper it would be better to call it a unary machine or a unit capacity machine.

Even if the proposed problem is clearly of interest in an industrial context (and of course from a theoretical point of view), I think it still has many limitations for tackling a real industrial problem. For instance typically in the semiconductor manufactory scheduling:
- the machines are more complex that just the batch machines or the unit capacity machines: there are also some concerns with sequence dependent setup times, with additional constraints like reticles, ...
- jobs generally consists of more than 2 operations and depending on the job, the process may be quite different
- of course, there are not single machines in the factory and part of the scheduling problem consist of machine allocation, with candidate machines for a given operation having very different characteristics
- there are in general some release dates for the jobs and a work-in-process, this will tend to jeopardize the fact that jobs of the same family and with same weight are equivalent which is an implicit assumption of the proposed algorithms for efficient batch formation

So it could be interesting to consider these limitations and see if and how they can be handled (maybe in a future work section?). In particular, both for the lower bounds and the approximation heuristics, there seem to be some possible generalization of the way the existing results for the individual machines are aggregated (e.g. for LB: GRWC for batch machines and WSPT for unit-capacity ones). Would it be possible to generalize this aggregation and propose LBs or heuristics for problems with more than 2 operations in the job ? This is of course more an open question.

There has been recent applications of Constraint Programming (CP) techniques to solve some related and in general more complex versions of the problem that I think should be cited. For instance:

A. Ham and E. Cakici. Flexible job shop scheduling problem with parallel batch processing machines: MIP and CP approaches. Computers & Industrial Engineering. 102 (2016) 160-165.

Given that you have been doing your experiments with CPLEX, it would be very interesting to see how a very simple CP Optimizer model compares to your heuristic (CP Optimizer is a CP based optimization engine available together with CPLEX). Also I would suggest using more recent versions of these engines as there has been some performance progress both in CPLEX and in CP Optimizer since version 12.6 (current version is 12.7.1). Your problem can be very easily formulated in CP Optimizer as follows (in OPL language):

using CP;

int n = ...; // Number of jobs
int m = ...; // Number of job families
int b = ...; // Batch capacity
 
float w[j in 1..n] = ...; // Job weight
int   p[j in 1..n] = ...; // Job processing time on discrete machine
int   f[j in 1..n] = ...; // Job family
int   q[k in 1..m] = ...; // Processing time of family k

dvar interval batch[j in 1..n] size q[f[j]];
dvar interval discr[j in 1..n] size p[j];

stateFunction batching;
cumulFunction capacity = sum(j in 1..n) pulse(batch[j],1);

minimize sum(j in 1..n) w[j]*endOf(discr[j]);
subject to {
  forall(j in 1..n) {
    endBeforeStart(batch[j], discr[j]);
    alwaysEqual(batching, batch[j], f[j], true, true);  
  }
  noOverlap(discr);
  capacity <= b;
}

Beside the description of the problem instance generation (which is very clear), it would be useful to make the actual instances available to the community and to provide the detailed results (for the 3 LBs and GBDS) on each instance in order to allow future comparisons on the same set of instances.

I also think that you should give more results on the individual lower bounds LB1-3. How do they compare depending on the different problem characteristics? When does one LB dominate others?

The description of Step 5 of the GBDS algorithm is not very clear:
- you say that you "Insert p_j-k_j extra units of time in the schedule at time C_j^P". what does that mean exactly?
- also, I'm wondering if there is not a mistake in the description of convertor II. Given that it considers the *first* scheduled piece of a job i, how can it be that this piece is scheduled from time C_j^P-k_j to C_j^P (given that C_j^P is the preemptive completion time of the job)

On Table 1 and its description, the naming Gap1 and Gap2 is not really explicit, it would be better to for instance use something like "Gap-GBDS" and "Gap-LB".

On Table 2, I would switch columns "Time" and "Gap-GBDS" so that the gaps are easier to compare. Also on tables 2-3-4, I would use a bold font for the "best" gaps in each row.



