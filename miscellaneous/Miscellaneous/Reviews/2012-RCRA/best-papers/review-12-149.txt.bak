Paper: Scheduling Countermeasures to Contamination Events by Genetic Algorithms
Author(s): Marco Gavanelli, Maddalena Nonato, Andrea Peano, Stefano Alvisi, Marco Franchini

RATINGS
(low/poor) 	1 	2 	3 	4 	5 	(high/excellent) 	N/A

Significance to Field 	
	4
  
Relevance to Journal 	
	4
  
Methodology 	
	2
  
Data Analysis 	
	3
  
Literature Review 	
	4
  
Writing Style/Clarity 	
	4
	
	
Recommendation: (choose the option that most closely summarizes your recommendation)
  O Accept as is
  O Accept Pending Minor Revisions
  X Revise and Resubmit After Major Revisions
  O Reject Outright

  
Narrative: (sent to corresponding author and other reviewers)

The article addresses a very original problem consisting in scheduling of the activation of devices on the water distribution networks in order to minimize the impact of some contamination event. If the decision problem in itself is well-known (it is a multiple TSP), the objective function used in the present problem (minimize the consumed contaminated water volume) is very complex to model and can only be estimated by running a expensive simulation. 

The description of the problem and its context is clear. 

I think a number of aspects should be clarified and improved in the paper:

- Some details are missing in the description of the proposed approach, in particular for the case of the encodings based on activation times (the main technique introduced in the paper), it is never clearly said how the initial population is computed. One can guess that it is based on a random sampling of the possible activation times followed by a feasibility restoration (FRP procedure) of each individual but it is not explicitly said. Furthermore, in the context of an encoding based on activation times, it seems that there should be a notion of parameter representing the "horizon" of the schedule, all activation times being constrained to be smaller than this horizon. I suppose the selection of a value for this horizon has an impact on the performances of the approach. Something should be said about it.

- It is mentioned on several occasions in the paper that the proposed approach performs better than usual "commonsense inspired criteria". It would be useful to provided some evidence of that in the experimental section. 

- The discussion about the pro and cons of using a high-performance MILP solver (p16) is weird. The bottom line is that if the MILP solver is too good and always finds the optimal solution, this can damage the diversity of the approach and thus, it is better to stop the search earlier by using an optimality tolerance as it will provide some kind of random effect on the generated individuals. I think that if this explanation is right, then it would be better to make this randomness effect explicit in order to better control it. For instance, you could randomly not consider some coordinate(s) j in the objective function that measures the distance to the parents.

- The justification for the use of a GA approach for the problem is that it can explore wide parts of the search space with a limited number of solution evaluations which indeed makes sense in the context of the application. A GA will be efficient if, within the limited number of solution evaluations, it is able to capture some building blocks of the problem. The experimental evaluation does not give enough evidence that the GA is able to find those building blocks. In particular, the GAs should be compared with a simple random sampling of the solution space. If there is a limit of 500 evaluations, why not just randomly draw 500 random solutions in the solution space (for instance 500 random vectors of activation times made feasible using FRP) and evaluate them. Are the GA variants significantly better than random sampling? A related question is the population size (question is related because random sampling is more or less equivalent to a population size of 500 for which no GA operators are applied). It is said on p15 that the initial population size of 20 was selected based on the only 2C^CS encoding. Usually the best population size highly depend on the encoding, so the best population size for the H encodings may be quite different. By the way, for 2C^CS, the efficiency seems to systematically increase when decreasing the population size on [20,100]. Did you try values smaller than 20? 

Some typos:
- p13: The equation (16) does not need to be numbered (not referenced) and there is a dot before the number
- In the experimental section, there are many occurrences of "analisys" instead of "analysis".
- p 18, col2: variable speed cosnfiguration


Comments for the Editor ONLY:
