Significance. Does the paper contribute a major breakthrough or an incremental advance? (Text is optional.)
 3: substantial contribution or strong impact
 2: modest contribution or average impact
*1: minimal contribution or weak impact

Soundness. Is the technical development accurate? (Text is optional.)
*3: correct
 2: minor inconsistencies or small fixable errors
 1: major errors
 
Scholarship. Is related work cited and discussed? (Text is optional.)
 3: excellent coverage of related work
*2: relevant literature cited but could be expanded
 1: important related work missing, or mischaracterizes prior research

Clarity. Is the paper clearly organized and clearly written? (Text is optional.)
 3: well organized and well written
*2: mostly readable with some room for improvement
 1: hard to follow
 
Reproducibility. Can the work be reproduced or verified based on the paper and any supporting information or data? (Text is optional.)
 5: code and domains (whichever apply) are already publicly available
 4: authors promise to release code and domains (whichever apply)
 3: authors describe the implementation and domains in sufficient detail
 2: some details missing but still appears to be replicable with some effort
*1: difficult to reproduce because of missing detail

Overall evaluation.* Please provide an overall score.
 3: strong accept
 2: accept
 1: weak accept
 0: borderline paper
 -1: weak reject
*-2: reject
 -3: strong reject

Review.

The article deals with the classical version of the permutation flowshop problem (PFSP). It proposes a (meta) Genetic Algorithm used to configure a GA. 

It is never clear along the paper if the objective is (1) to efficiently propose an approach to find a good solution to a particular instance of the PFSP or (2) to study the behavior of a family of GAs dedicated to solving the PFSP. But whatever the objective is, I think the paper suffers from important limitations.

One of these limitations is that it is not clear if the "optimized" GAs produced by the meta-GA are significantly better than other "non-optimized" ones (except for the fact the use of Local Search ingredient helps, but this is well known). The GAs are strongly randomized algorithms so running twice the same configuration of a GA on the same problem instance (unless using the same random seed for the random number generator) will produce different solutions of different quality. There is an inherent variability in the result, so when comparing 2 configurations (say A and B) of a GA on a given problem instance, one needs to take this variability into account in order to decide if version A is significantly better than version B. This notion of variability is not analyzed (and even not mentioned in the paper) so it is difficult to say how significant is the effect of "optimizing" the configuration of the GA by a meta-GA. The results on Table 4 is particular illustrative. Given the possibly large variability of the behavior of a particular configuration, it is not surprising that depending on the instance, the "best" configuration is very different, but this may be not relevant at all unless the evaluation of a particular configuration on a particular instance is run many times (with different random seeds) and some results are provided in terms of average and (at least) standard deviation of the solution quality.

The other issue is that it is very unfair to compare the proposed hyper-heuristic approach against a "simple" meta-heuristic as done in section 4.3. From my understanding, the effort taken by the meta-GA to configure the resolution GA on a particular instance is not considered in the comparison, and this effort seems to be much larger than just running a particular configuration of a GA. So I do not really see what is the point of the comparison in 4.3.

Some more detailed comments:

- In the introduction: I don't see how the hyper-heuristic approach works around the so called "No Free Lunch Theorem". In the end, when you have to solve a new problem instance that you have never seen and solved before, you will have to run a piece of algorithm (be it exact, heuristic, hyper-heuristic or hyper-hyper-heuristic). I think the NFLT also applies to hyper-heuristics.

- p2, col2: "C_{max}denotes": missing space

- p3, col1, and in general in section 3.1: you should describe in detail how you evaluate the different combinations. Currently all what is said about it is "The evaluation of the hyper-population (lines 10 and 23) is done in parallel to optimize the execution time." This is the part of the algorithm that, I guess, requires the largest effort and (as mentioned above), I think that it does not really make sense to perform only a single evaluation of a configuration (many evaluations should be performed with different random seeds so as to evaluate the variability). 

- p3, Algorithm 1: be careful with the typesetting in math mode (get_Random_Number)

- p4, col1: "The outline of the proposed HHGA ...", the lines numbers (10,23) do not match with the ones of the algorithm

- In Table 3: the CPU time when using the LS is very variable (for instance from 300s to 200h for problems of similar size). Why is it so? Do you think one can compare the results of GA with LS that take 200h (on tai100) against a version without LS that is run for 36s only?

- In section 4.3, you should say at some point how much time it takes to compute a "tailored" GA. And when comparing HHGA with other approaches, this time should be considered in the comparison.



Confidential remarks for the program committee:	


