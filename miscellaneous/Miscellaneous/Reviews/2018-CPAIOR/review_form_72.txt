PAPER  : #72
TITLE  : Revisiting the Self-Adaptive Large Neighborhood Search
AUTHORS: Charles Thomas and Pierre Schaus

SCORE:
    ==  3 strong accept
    ==  2 accept
    ==  1 weak accept
    ==  0 borderline paper
    == -1 weak reject
    == -2 reject
    == -3 strong reject

0

REVIEWER'S CONFIDENCE
    == 5 (expert)
    == 4 (high)
    == 3 (medium)
    == 2 (low)
    == 1 (none)

5

REVIEW

> Please provide a detailed review, including a justification for your scores. 
> Both the score and the review text are required.

The short paper presents two contributions:
1- a set of fairly general LNS relaxation heuristics and search heuristics to be used in an LNS framework
2- a variant of the learning scheme introduced by Laborie and Godard for self-adapting LNS

Both contributions are pretty incremental but the overall paper is interesting. 

The main idea behind the learning scheme variant is that operators with small running time will evolve faster as they are evaluated more often. It is not clear why this is really a problem and how this phenomenon can impact the performances. On Example 2, the weight of the fastest operator A indeed evolves faster (in this particular case: it decreases faster because the initial weight 10 is very far the operator efficiency 1/2). But if you continue a little bit, both weights of A and B will very quickly converge to their respective efficiency 1/2 and 1/4. 
The case of operators that never succeeded (or succeed extremely seldom) and thus, see their weight quickly converging to 0 can be considered as a different problem: the one of diversification against intensification as you need to at least select an operator from time to time in order to learn that it (still) does not work. This last issue can be fixed for instance by forbidding the weight of any operator to be lower than a certain diversification threshold value (strictly greater than 0). In any case I think that the idea behind the proposed variant could be explained with more details. Also, even if the experiments seem to suggest that the proposed variant improves on the original version, it would be interesting to provide additional experimental details to show that the performance improvement is indeed due to the different convergence rate of the operators weight. 

The experimental results on Table 1 are difficult to interpret. If I understand well, the relative distance (value in column rdist) depends on the value of the initial solution of the methods. What is this initial solution? The initial solution of each method? Is it the same for all methods? (I suppose that it is otherwise the comparison of these rdist does not make sense.) But the problem with this rdist is that it does not really measure how close the method is from the BKO and in any case, averaging these distances for all problems is not very informative. For instance for TSP instance kroA200, the rdist of the "Eval window" method is 0.06 which seems to be quite small but in fact, the method finds a solution of 50022 which is more than 70% worse than the BKO 29368. Also, it is not clear how the standard deviation is computed. Is it a standard deviation on the objective value over the 20 runs? In this case, this deviation looks very small. For instance for the same TSP instance kroA200, for the Laborie method, the std is 100 for an average objective value of 156239, which is a relative std of around 0.06%. This is very surprising. I think it would be interesting to select one of these instances and study in more details the difference of behavior of the different methods on this instance (different duration and efficiency of the different operators over time, evolution of their relative weight in the self-adapting method, which operators were selected and when, ...).

The K Opt heuristic is interesting. It is of course well known in local search for routing problems but it is the first time I see it used in LNS. 

On Fig.1, the mapping between the operators listed on the first column and their description in section 3 should be made easier. Maybe you should explicitly mention in section 3 the exact terms used on Fig. 1.

In section 3, in the description of the "Sequential" relaxation heuristic, there is a notion of "consecutive/successive variables". It is consecutive/successive according to which total order?


CONFIDENTIAL REMARKS FOR THE PROGRAM COMMITTEE 

> If you wish to add any remarks intended only for PC members please
> write them below. These remarks will only be seen by the PC members
> having access to reviews for this submission. They will not be sent
> to the authors. This field is optional.


