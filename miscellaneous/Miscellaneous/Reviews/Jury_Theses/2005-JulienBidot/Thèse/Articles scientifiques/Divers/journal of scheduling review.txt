Review for Journal of Scheduling
--------------------------------

Date: 16/04/02

Title: Predictive, stochastic and dynamic extensions to aversion
dynamics scheduling

Author(s): G.W. Black, K.N. McKay, S.L. Messimer

Reviewer: Chris Beck (cbeck@ilog.fr, c.beck@4c.ucc.ie)

--------------------------------------------------------
Ability to review: 90%


Reviewer's Recommendation
-------------------------
[ ] Accept
[ ] Reject
[X] Accept subject to minor alterations
[ ] Accept subject to major alterations

Reviewer's Rating of Paper
--------------------------
[ ] Excellent (A major contribution)
[ ] Very good (A continuing and useful advance in an area of
    importance)
[X] Good (Satisfactory and of sufficient importance to merit
    publication)
[ ] Poor (Of some interest but not significant enough to merit
    publication)
[ ] Very poor (Trivial, or incorrect, or of no interest, or not new,
    etc) 


Confidential Comments for the Editor
------------------------------------


Reviewer's Report
-----------------

The paper presents an Aversion Dynamics (AD) heuristic used to set the
dispatch priority of jobs in a single machine scheduling problem. The
basic idea is that after an unexpected event a machine is in a risky
state and may need some time period (e.g., for recalibration) to
return to its pre-event level of speed/efficiency/quality. Based on
previous work, it is claimed that human schedulers make decisions to
change job sequences in order to avoid placing jobs which will have a
large impact close (e.g., from a mis-calibration) within this risky
period. 

Overall, the paper is relatively well-written, the experiment is
well-designed and conducted, and the work is interesting. However,
there are a number of issues that deserve further explication in the
paper and perhaps some additional experimentation.

One of the main contributions of the paper is to add a predictive
component to a previous heuristic. Instead of just reacting to an
unexpected event which results in a "risky period" the current paper
presents an algorithm that can also use the human schedulers
predictions about when an event might occur and so proactively hedge
against negative secondary impact of the risky period. I have a number
of questions about the usefulness and "realisticness" of such a
proactive approach.

1) Is there any evidence that human schedulers proactively predict
   (with any accuracy) when an unexpected event may occur?  Some
   examples of such a situation would be very useful.  If the primary
   event is something like unexpected machine breakdown, I am
   skeptical as to the usefulness of proactive averse dynamics. On the
   other hand if the primary event is not really unexpected but is
   rather something like a maintenance activity that still may result
   in a secondary risky period, this does make more sense. However, in
   such a case there should be no need for uncertainty on the part of
   the scheduler: he/she knows when the maintenance event will
   occur. Therefore, the parts of the paper which talk about the human
   schedulers skill in predicting are irrelevant. It would be nice to
   present a clear, real-world example of a primary event where the
   proactive heuristic in this paper is useful. Such an example would
   provide useful motivation for the work in an introductory section
   of the paper.

2) Assuming there are clear situations where a human scheduler
   accurately predicts the primary event and then forms a subsequent
   schedule based on secondary "risky period" reasoning, what is the
   relative magnitude of the secondary impact? Is the impact of the
   primary even so great that anything that is done to cope with the
   secondary impact really only has a marginal contribution?  Note
   that again, I am talking about the proactive case. I can see that
   in the reactive case, when an event has happened and the primary
   impact has already been felt, it is useful to try, in a reactive
   manner, to minimize the secondary impact. However, proactively is
   there really any point in caring about the secondary impact?  If
   the primary impact is much greater, I would suggest that the
   scheduler should spend his/her time trying to proactively deal with
   the expected primary impact rather than the secondary one. It might
   even be the case that trying to deal with the secondary impact
   degrades the treatment of the primary impact.  In other words, I
   believe the interesting problem is to treat the scheduling
   performance as a whole taking into account both the primary and
   secondary impacts of unexpected events. Are proactive AD heuristics
   useful in such a situation or is their contribution lost in the
   need to deal with the primary impact?

My second general concern is the realisticness and actual details of
the modeling of the uncertainty associated with the occurrence (or
non-occurrence) of the primary event. The authors introduce the CV
factor which represents the accuracy of the human scheduler in
predicting the primary event and assume that the event will either
occur as predicted or not occur. There are two issues here:

1) Realism of the model. Is it realistic that the primary event occurs
   at the predicted time or doesn't occur at all? I can imagine
   situations where the likelihood of the event (e.g., machine
   breakdown) increases with time and do not see how the current work
   could be adapted for such a situation.  As a minor point, I note
   from Eq. 3.2 that the accuracy of the scheduler degrades with the
   time into the future: the realized value is an absolute time value
   and the standard deviation increases with the increased realized
   value. This strikes me as a good idea (accuracy should be greater
   in the nearer term than the longer term), but no comment is made
   about it in the paper, leaving me to wonder if the authors realized
   this.

2) Experimental design. It is unclear how the uncertainty of the
   scheduler is used in the experiments. The CV value does not appear
   to be a factor in the experimental design. Furthermore, Table 4.1
   defines the predicted event time, but that time does not appear in
   any of the heuristics (e.g., eq 3.11 or 3.12). I do not understand
   how the predicted event time is used in the experiments or in the
   heuristics. Furthermore, how is the actual event time calculated? I
   assume, if the event happens, it is not always equal to the
   predicted time, but this is unclear. Furthermore, it is unclear how
   the upper and lower-bounds presented in eq 3.7 and Figure 3.3 are
   used as they do not seem to be mentioned again in the paper. I
   understand that the scheduler uses the [L, U] interval to bound the
   predicted event time, but I do not understand how this is
   incorporated into the heuristics since the time of the event is the
   only thing used in the heuristics. If it is only the predicted
   event time that is used, why is there any mention of [L, U]?

Finally, I would like to take issue with the authors comment (p18)
that "'optimality' is not the goal". I do not understand this for two
reasons. First, in the real-world why isn't optimality the goal?
Clearly, the human schedulers are striving for a better schedule by
using AD-like approaches. The authors claim that they are interested
in trends that support or do not support aversion concepts, yet
clearly if use of AD heuristics help to approach or achieve optimality
this is a support.  The second reason for my disagreement with this
statement is that I believe that the experimental results should
really have an optimal baseline. The authors results are based on how
much better or worse heuristics perform relative to each other and
show that the AD heuristics are significantly better that the other
heuristics. However, in this case, significantly better ranges from
about 0% to 20% better.  This is impressive, but since we do not have
an idea of the optimal solution to each problem, it may be
mis-leading.  If the AD solutions are close to optimal, this is a very
impressive result. If however the AD solutions are 2 or 3 times above
the optimal value an improvement of even 20% isn't really that
interesting.  I am not suggesting that the authors introduce another
algorithm and try to perform a "race to optimality". Rather, I am
suggesting that they optimally solve each problem (with an off-line
algorithm that uses perfect information about the event timings,
activity availabilities, etc.). This algorithm should not be compared
against the heuristics in the paper, but rather the optimal solutions
should provide the baseline for the presentation of the experimental
results.  This would be especially useful for other researchers who
might want to follow-up this work. It is well-known (at least in the
AI community) that there exist more costly scheduling techniques
(e.g., constraint propagation with branch-and-bound) that out-perform
dispatch rules (under no uncertainty). An obvious step is to see if
these techniques can be adapted to the problems studied in this
paper. If the experimental results here demonstrate that the AD
solutions are often close or at the optimal, then we can conclude that
the problems addressed are "solved". On the other hand, if there is
significant room for improvement, different scheduling techniques
might be interesting to investigate.  I think it would greatly enhance
the quality and future usefulness of this paper, if such a baseline
could be provided in the experiments.


Minor comments
--------------
p3: In the first paragraph of Section 2 the word "structure" is
over-used ("The intent of this structural review is not to perform a
comprehensive review of each structure, but to illustrate the types of
structures that exist in the literature"). Aside from the need to
improve the sentence structure, there is a need to define exactly what
is meant by structure. It is unclear what constitutes structure and
even if it refers to the definition of the underlying scheduling
problem or some other characteristics of scheduling problems. Later,
by the examples given it is clearer that "structure" refers to the
former, however, it should be defined at the beginning of this
section.

p3: It would be useful to define each structure that is
presented. What exactly is a "single-machine, static arrival,
deterministic parameter problem"? (The first two components seem clear
to me, but what are parameters?) These are listed later but is this an
exhaustive list?

p4: What is "blocking in a flowshop environment"?

p5: At the end of the first paragraph, we have "the assumptions of
attribute and time independence have not been challenged." while at
the beginning of the next paragraph we have "The assumptions of
attribute and time independence are challenged in Aversion Dynamics
[ref to previous work]". Are they challenged or not?

p5: No definition of "impacted processing time" is given until later
in the paper.

p7: Eqn 3.1 would be clearer if it were written out on multiple
lines. Matching up parens manually is annoying. The same comment can
be made for Eqn (3.10) on p15.

p7: "p_j^*(t) is the total processing time" ... of what? The task? the
job?. In some scheduling areas "job" refers to a set of activities (or
operations or tasks). It appears here that job is a single operation,
but this does not become clear until later.

p13: How is the interval [L, U] used? It is presented here but its use
is left unexplained.

p16: It would be useful to include some sort of summary (maybe a
table) of the heuristics introduced in Sect 3.3 (i.e., WSPT, R&M,
X-Dispatch, (X-)Averse2). The discussion of the heuristics is good
(the way each one builds on the presumed weaknesses of the previous
one) but by the end of the section there is a problem with remembering
exactly what the names of the heuristics refer to. A table that
presented the name of the heuristic and a series of "characteristics"
that go into the rating might be useful. Alternatively, a paragraph
(or diagram!) reminding readers that R&M is WSPT plus some
incorporation of slack, X-Dispatch is R&M plus the ability to look at
the soon to be available jobs.

p16: Despite Sect 3.3, equations 3.11 and 3.12 are relatively
incomprehensible. Writing them on multiple lines so that the reader
does not have to try to match parens manually would help, but the main
problem is the number of terms: alpha, beta, tau, gamma, .... I
suggest adding some explanatory paragraphs for these terms (I realize
they have all been previously defined, but the reader is still left
with 8 to 10 different symbols to try to remember) and/or a table
defining each term and logical groupings of terms (e.g., 1 - B(r_j -
t)/P_{min}). This would provide something for the readers to refer to
in trying to decode the equations.

p20: Please fix the phrase ".. at various sample size levels to find
the level at which it levels off ..."