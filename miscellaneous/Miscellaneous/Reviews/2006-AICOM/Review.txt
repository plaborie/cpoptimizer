
Title: Constraint-based Temporal Reasoning with Preferences

Authors: L. Khatib, P. Morris, R. Morris, F. Rossi, A. Sperduti, K. Brent Venable

Reviewer Comments:

The article describes a framework for reasoning with temporal
constraints and preferences. A theoretical study of the complexity of
the overall framework is presented and a tractable fragment of
practical interest is identified. Two solvers for this fragment have
been implemented and evaluated in an experimental study. The last part
of the article presents a learning technique to elicit local
preferences from global ones. This approach has also been
experimentally evaluated.

The scope of the article is quite impressive, ranging from a
theoretical complexity study of the framework to the actual design,
implementation and assessment of algorithms, dealing both with the
resolution of the problem and the important question of how to obtain
the data (local preferences).

The article fits the scope and topics of the special issue of AI
Communications on Constraint Programming for Planning and Scheduling
(preferences, temporal constraints).

The architecture of the paper is fine. Although in general the paper
is easy to read, the proofs of some theorems, especially in section
3.3, could be moved to an appendix section so as to allow for a more
fluid reading.

As a final general comment, I think the work described in this article
is relevant to the special issue on Constraint Programming for
Planning and Scheduling and I recommand its publication in the AI
Communications journal.

Some more specific comments:

p4,col2: definition of a minimal constraint T_{ij}: shouldn't it be
that v_i and v_j must be feasible 'together', in the same solution.

p4, col2: in Definition 2, the definition of the intersection, where
K_k = I_i \cap J_j for some i,j is ambiguous as it does not say that
the set of K_k must contain ALL the intersections I_i \cap J_j.

p6, col1: second item at the top of the page: it is not clear what A_i
stands for. {A_i}_{i \in S} seems to be a set of subsets of A but it
is not clearly said.

p10, col2: section 3.2. Given two soft constraints T_1=<I_1,f_1> and
T_2=<I_2,f_2> ... One should precise that T_1 and T_2 hold on the same
pair of variables.

p12, col2: on Fig. 5, it is not clear what the two functions at the
bottom of the figure represent. It seems clear that they represent the
delay on the constraint <X_0,S_s> but it would be more clear if the
two functions were represented on the picture as a single function on
the interval [8,17] (even if the function is not actually defined on
[10,15]).

p13, col1: Discussion on TCSP with linear preference function: it is
clear that the preference for each interval can be captured by a set
of linear constraints but, there is still the problem of the
disjunction between intervals which, of course cannot be represented
in an LP problem. 

p13, col1: LP can still be used for piece-wise linear preferences as
far as the preference function is concave (convex in a minimisation
problem). For example, functions (a), (b), (e) and (f) can be solved
as an LP problem simply by adding a linear constraint for each
function segment. In particular, LP can solve problems where we want
to state that the distance between two events must be as close as
possible to a single value (case (e)).

p19, col2: it is not exactly precised how parameters pa,pb and pc are
used to modified the quadratic function. What exactly means
"Changing" the 3 parameters a, b and c by a relative change pa, pb,
pc? Increasing them, decreasing them ? I have the feeling that, doing
so, we can easily end up with a quadratic function that is always
negative or that can be greater than 1. Is this function truncated as
in section 6.1?

p21: in the experiments, the temporal networks seem to be very dense
(even a "density" of 20% on a network of 1000 nodes means that, in
average, each node is involved in 200 temporal constraints). Is it
really the kind of temporal network handled in the targeted
applications? Classically, in scheduling, temporal networks are sparse
(the number of temporal constraints on a given node grows sublinearly
with the size of the network). Of course, this has an impact on the
choice of the algorithms to solve the underlying STNs (basically, use
some variants of Bellman-Ford instead of Floyd-Warshall which is far
too expensive in these cases).

p21: which precision (parameter 'precision' in the Chop-solver
algorithm) was used in the experiments?

p22, col2: Te goal of the learning procedure is ...

p27: In the experiments, it is surprising that the global preference
function that needs to be approximated by learning is itself made of
quadratic local preferences, that is, the very same family of
functions as the ones that will be elicited. I think it would have
been much more informative to use other types of functions for the
global preference function in the experiments. What if the actual
preference function is made of local functions more general than
quadratic ones, what if it also involves some less local functions
that work on larger subsets of variables than just pairs, etc ?

p27: It is not said how the solutions in the training sets and in the
example sets are generated. This is very important to assess the
relevance of the experiments as the method to generate solutions can
introduce some bias. 

