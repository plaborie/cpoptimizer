__________________________________________________________________________________________
Evaluation

Significance (*). Does the paper contribute a major breakthrough or an incremental advance?
 3: substantial contribution or strong impact
*2: modest contribution or average impact
 1: minimal contribution or weak impact
 
Soundness (*). Is the technical development accurate?
 3: correct
 2: minor inconsistencies or small fixable errors
 1: major errors
 
Scholarship (*). Please provide a scholarship score.
 3: excellent coverage of related work
*2: relevant literature cited but could be expanded
 1: important related work missing, or mischaracterizes prior research
 
Clarity (*). Please provide a clarity score.
 3: well written
 2: mostly readable with some room for improvement
*1: hard to follow
 
Reproducibility (*). Please provide a reproducibility score.
 5: code and domains (whichever apply) are already publicly available
 4: authors promise to release code and domains (whichever apply)
 3: authors describe the implementation and domains in sufficient detail
 2: some details missing but still appears to be replicable with some effort
*1: difficult to reproduce because of missing detail
 
Overall evaluation (*). Please provide an overall score.
 3: strong accept
 2: accept
 1: weak accept
 -1: weak reject
*-2: reject
 -3: strong reject
 
Reviewer's confidence (*). Reviewer's confidence
 4: expert
 3: high
*2: medium
 1: low
 
Suitable for a demo? (*). Suitable for a demo?
 3: yes
 2: maybe
*1: no
 
Nominate for Best Paper Award (*). Nominate for Best Paper Award
 2: yes
*1: no
 
Nominate for Best Student Paper Award (if eligible) (*). Nominate for Best Student Paper Award (if eligible)
 2: yes
*1: no
 
 
__________________________________________________________________________________________
Additional scores

[Applications track ONLY]: Importance and novelty of the application (*). [Applications track ONLY]: Importance and novelty of the application
 6: N/A (not an Applications track paper)
 5: very significant - approach and/or application are new and important
 4: novel - approach and/or application are new but not so important
 3: moderate - moderate significance of approach and/or application
*2: marginal - parts of the application or approach are new
 1: none - similar approach to the same application has been used before

[Applications track ONLY]: Importance of planning/scheduling technology to the solution of the problem (*). [Applications track ONLY]: Importance of planning/scheduling technology to the solution of the problem
 5: N/A (not an Applications track paper)
 4: very important - essential to solving the problem
*3: moderately important - adds significant capabilities
 2: marginally important - improves usability, capabilities or performance
 1: not important - could be done without it

[Applications track ONLY] Maturity (*). [Applications track ONLY] Maturity.
 7: N/A (not an Applications track paper)
 6: study of the success, acceptance, impact and/or deficiencies of a fielded system
 5: fielded system
 4: mature system with testing on realistic problems
 3: early prototype system with testing on realistic problems
*2: early prototype system with testing on artificial data
 1: challenge problem

[Robotics track ONLY]: Balance of Robotics and Automated Planning and Scheduling (*). [Robotics track ONLY]: Balance of Robotics and Automated Planning and Scheduling
*6: N/A (not a Robotics track paper)
 5: excellent mix between the two fields
 4: good mix between the two fields
 3: fair mix between the two fields
 2: nothing Robotics specific, pure "classical" ICAPS paper
 1: nothing on Automated Planning, pure Robotics paper

[Robotics Track ONLY]: Evaluation on physical platforms/simulators (*). [Robotics Track ONLY]: Evaluation on physical platforms/simulators
*6: N/A (not a Robotics track paper)
 5: runs on real robots
 4: runs in a standard robot simulator (Gazebo, Morse etc.)
 3: could be used with a simulator or robot system
 2: still some work to have it running on robots
 1: cannot run on a real or simulated robot system in its current form

[Robotics Track ONLY]: Significance of the contribution (*). [Robotics Track ONLY]: Significance of the contribution
*6: N/A (not a Robotics track paper)
 5: significant in both Robotics and Planning
 4: significant in Planning (i.e. planning researchers would care about this paper)
 3: significant in Robotics (i.e. robotics researchers would care about this paper)
 2: marginally significant in one Robotics and/or Planning
 1: not significant

__________________________________________________________________________________________
Review


Review (*). Please provide a detailed review, including justification for your scores. This review will be sent to the authors unless the PC chairs decide not to do so.

The article describes a stochastic resource allocation problem in the domain of smart energy systems where computational power (in particular the RAM) is very limited. The authors propose a local search method to solve the problem and compare it to more computationally expensive methods like Mixed Integer Linear Programming.

The paper is hard to follow because it is lacking some formalism or, when formal definitions are involved, it is missing some important details and formal descriptions are not well linked together. For instance there is a general description of the problem in section "Stochastic Resource Allocation Problem" as an 8-uple but many of the elements described later on in this section do not clearly refer to the elements of this 8-uple: for instance the function f(G,D) seems related to the f_u but looks somewhat more general. I list some things that did not seem clear to me in the more detailed comments below.

From the application point of view, I think you should better describe how it is supposed to work in practice. In particular:
- How the user's requirements are supposed to be collected? How often? Is it on request of the users? Based on some periodical demands ? How do you get the parameters of the user's demands that are used as input of your approach?
- How would the policies generated by the system be transmitted and executed? Automatically by selecting which users are served, when and how, or would they be sent locally to each user ? 
I'm asking because without these answers, I do not see why users request are modeled as stochastic demands. Users express some preferences on their demand (preference on when to start the activity, on its duration) but if it is up to the system (through the output control policy) to decide when to start the activity and how long it should last, it looks like a deterministic demand with some objective function to maximize the user preference. Of course I understand that the energy availability is stochastic but as long as the demands are concerned this is much less clear. In fact these probability distributions on the demands seem to potentially describe 3 types of things:
1- some "classical" domains for decision variables (e.g. the user wants to have its activity start between 1PM and 3PM but does not really care when exactly)
2- some preference on some values (e.g. he prefers to start between 1PM and 3PM but, if needed he can start sooner or later even if it incurs some cost)
3- some uncertainties on the values (e.g. an activity to load an item will last between 2 and 3 hours depending on its estimated uncertain current load), but here one cannot really speak of "preference"
It is not clear what the probability distributions on the demands are supposed to represent.

The definition of the states handled in output control policies involves only two state variables: the battery charge b and the expected amount of solar energy E[G]. So, as mentioned in the paper, the state is global whereas the system generates a set of policies \pi_i, one for each user. As the state does not seem to contain any notion of time, I'm wondering how the policies allow to distribute the available energy over the users over time. For instance suppose a simple case with 8 users who posted the same request for, say 5W of power for a duration of 1h between 1PM and 8PM. If the battery is currently empty but assuming the amount of solar energy is 5W over the horizon (so that the requests can only be served one after the other), how would the 8 individual policies look like? My feeling is that here, the state does not really changes so I do not see how the policies would allow to sequence the 8 demand activities. There seem to be something missing in the description of states or in the description of policy execution. In general, I think it would help to give an example of policy produced by the approach on a toy example.


More detailed comments:

p2, col1: item defining G: why not just say that supp(G)=T ?

p2, col1: item defining set of resource-consuming activities A: while the definition of the problem "looks" formal here, you do not formally define the different types of probability distributions associated with an activity: you mention these distributions model different types of user's preferences (start time, resource quantity, later on in the paper you also mention activity duration). It seems to me that the semantics of these distributions is important for the definition of the problem so I think you should precisely distinguish them here.

p2, col2: a SAA is used to model f(G). It makes sense but it introduces an additional dimension in the size of the problem which is the number of scenarios. In your approach, you seem to project these scenarios on the MDP (see section Approximating Complex Scenarios as an MDP). But given that the state only contain two variables (see earlier comment), it seems that making a Markovian assumption is a very strong assumption here, in particular because one can expect some strong temporal correlation for the solar power generation profiles. Another argument against the Markovian assumption is the discretization itself. It would be necessary to discuss these assumptions.

p3, col2: if I understand well, the term c_1 in Eq 13 is a large enough factor so that constraints on critical activities are satisfied with the required probability. So the objective is lexicographical: first minimize penalty term for unserved critical activities, then maximize preference. Maybe it is better to state the objective this way, rather than introducing this artificial c_1 coefficient which is probably not very useful in your local search approach ? If the objective is really a weighted sum, it would be useful to give more details on how to select the value for this coefficient.

p4, col1: the notations are not clear: M_\pi is a time-window and a_i seems to be an action realization, so what a_i \in M_\Pi formally means?

p5, col1: it is not clear how (and even "if") the reward defined here is related with the objective function of Eq 12-13.

Response to author rebuttal
---------------------------

The authors answer about the input/output clarifies a little bit the application and should clearly be mentioned in the paper. But there are still some open questions, for instance concerning the execution of the policy: you mention a set of LED lights to indicate when the policy allows a user to turn on or sheds their loads. Does it means that the policy *prevents* the use of some loads over some time-windows or is it only *indicative*? If it is *indicative* only, the success of the application will depend a lot on the willingness of the users to apply the policies, and if it can *prevent* the use of some loads on some time-windows, then there is a risk it will bias what will be leraned in the future (extreme case: a user would prefer to use a given load usually during the daytime but because of conflicting access to energy, the policy may several time decide to allocate some time-window during the night, after some time because the user will use the load during the night, the learning algorithms may infer that this is a preference from the user). More generally, if the behavior of the user is influenced by the execution policies (and of course, they are), it is not clear if/how one can learn the user's preferences from the history of executions as the policies may have played *against* user's preferences.

Regarding the definition of the system state with two variables only (the battery charge b and the expected amount of solar energy E[G]), it is clear that the value of these states change over time. But the issue I'm pointing is that, at a given time t, because of the Markovian assumption, the policies can only rely on these 2 values (at time t) to select some action. I do not see how it is sufficient to discriminate and make some relevant choices. It would be good to give an example of policy produced by the system (like for the problem I suggested, but it could be another one of course).




_______________________________________________________________________________________________________________________________


Confidential remarks for the program committee. If you wish to add any remarks intended only for PC members, please write them below. These remarks will only be seen by the PC members having access to reviews for this submission. They will not be sent to the authors. This field is optional.

The paper is really hard to follow because of so many imprecisions all over the place. In the end, it is very possible that there is really some good contributions (on the modeling aspects, on the resolution with the interesting focus on limited computational power to solve the problem) but it would require some work to clean it up. The paper does not seem to me mature enough for acceptation and the answers rebuttal did not change it.



_______________________________________________________________________________________________________________________________


















Thank you for the reviews. Your insight is much appreciated. 

Generally, all three reviewers took issue with our lack of description regarding the implementation of the resource allocation algorithm. We have developed a very low cost Power Management Unit that relies on Microchip PIC controllers with the necessary networking / sensor / power electronics capabilities. Each Power Management Unit can connect to solar panels / batteries and a variety of user loads. One unit sits in each user’s household and collects the necessary data and outputs our policy to the end user. We focused the paper mainly on the algorithm but will include a description of the hardware in the final version. As noted in the paper, the algorithm also meets the processor / memory requirements of these $2 controllers. 

Regarding reviewer 1’s question of algorithm input / output. The input distributions are generated by observing user behavior (i.e. when they turn on loads) and we record user behavior using low cost voltage / current sensors. The sensor data can be turned into distributions through a variety of learning / load disaggregation algorithms. Description of the learning algorithms was out of the scope of this paper. One of the major innovations of our algorithms is that we can reason directly over these distributions while other algorithms rely on more sophisticated and expensive user input methods. As noted above, the Power Management Unit outputs policy decisions to the end user (we use a set of LED lights to indicate when the policy allows a user to turn on or sheds their loads). 

=> The authors answer about the input/output clarifies a little bit the application and should clearly be mentioned in the paper. But there are still some open questions, for instance concerning the execution of the policy: you mention a set of LED lights to indicate when the policy allows a user to turn on or sheds their loads. Does it means that the policy prevents the use of some loads over some time-windows or is it only indicative? If it is indicative only, the success of the application will depend a lot on the willingness of the users to apply the policies, and if it can prevent the use of some loads on some time-windows, then there is a risk it will bias what will be leraned in the future (extreme case: a user would prefer to use a given load usually during the daytime but because of conflicting access to energy, the policy may several time decide to allocate some time-window during the night, after some time because the user will use the load during the night, the learning algorithms may infer that this is a preference from the user). 

Regarding review 1’s question regarding the state not being a function of time: this may have been confusion regarding the wording. The state is a function of time and the policies are a function of time: Equations 10 and 11 relate the policy as a function of time and on the 4th page, second paragraph we describe the state as a function of time t. This will hopefully also clear up a comment regarding the Sample Average Approximation approach. 

Regarding reviewer 1’s question of specifying the distribution’s formalism: If you could provide further insight to how we could further specify the distribution, this would be helpful. Do you mean whether the distribution can be continuous vs. discrete? The parameterization is provided by Equation 3 and the algorithm is valid for both continuous and discrete distributions. We provide the example of a continuous uniform distribution but the distribution could equally well be Gaussian. We assume the probability law is provided by the learning algorithm described above that reasons about sensor data. 

Regarding reviewer 1’s comment on the objective’s penalty term: while one could first minimize the penalty of not meeting chance constraints, there is always a tradeoff between critical activities and non-critical activities. For example, how many non-critical activities should we shed to allow just 1 critical activity? In our view, this question is subjective and depends on the contracts that the grid operator has with the end user. This subjectivity is capture in the magnitude of our penalty constant, c. 

