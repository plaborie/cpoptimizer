Title: Solving Risk-Sensitive Stochastic Orienteering Problems using Optimization and Local Search
Author(s): Pradeep Varakantham, Akshat Kumar, Hoong Chuin Lau and William Yeoh

Overall evaluation
==================

[ ] Accept as is
[ ] Accept with minor revisions
[X] Reject with resubmission encouraged
[ ] Reject


Comments and suggestions
========================

First, a comment on the terminology: "dynamic" travel times (as other dynamic aspects in general) usually refer to estimated travel times that may change on-line during the execution of the route. Here, I think you are solving a "static" problem that is: computing off-line a maximum reward path. So I would rather speak of "time-dependent" travel times which is the classical terminology used in TSP/VRP. I will stick to this terminology in my review.

The article deals with a very challenging problem which is a stochastic version of a time-dependent orienteering problem. 

The description of the problem is very clear and, more generally the paper is well organized and easily accessible. 

The problem is motivated by a real application for sequencing the activities in a theme park. A weakness of the article in its current state is that the adequacy of the problem definition with the application should be better justified. That the theme park activity sequencing should be considered as an orienteering problem is clear, and it is a very good and original application of OPs by the way. The extensions to stochastic aspects make the problem much more complex and harder to solve. As a result, these problems must be somehow simplified (scenario sampling like for MILP-SAA or even reduction to a unique scenario, so to a deterministic problem for the proposed MILP-Percentile approach). For these stochastic aspects, the experimental results provide enough arguments to justify the model extension. But it is less clear for the time-dependent aspects: 
- What if you would compute the paths assuming time-independent travel times (for instance by considering average travel times or by integrating the time-dependency into the stochastic aspects). That way, the MILP approaches would probably allow solving the (simpler and less accurate) problem to optimality for the synthetic dataset. It would be very interesting to show in the experiments how it would compare with the local search. Or even, you could just decrease the number of time steps (100 intervals more or less correspond to 5mn time-windows, is such a precision really required?). In general, my point here is that solving a complex and accurate problem with a relatively poor method (like the Local search seems to be here) is not necessarily better than optimally solving a less accurate but simpler problem so the question to answer with a some additional experiments is "does it worth complexifying the problem?".
- A possible direction to simplify the problem while exploiting the particular features of the application is that it seems that the time-dependency (and the uncertainties) depends more on the activity itself (as mentioned in the article: queues at a particular attraction) than on the transition between attractions so that most of the variation is on the duration of the visit of attractions rather than on the travel time between attractions. Don't you think that considering time-dependent and stochastic durations of visits and simpler time-independent or/and deterministic travel times between visits would be a possible compromise?
- On the other hand, there are some aspects of a theme park activity sequencing that seem to be important and are not mentioned in the article like precedence constraints between visits or, even more important, specific time-windows (for instance the shows that run according to a predefined schedule).

The proposed local search approach does not perform well compared to the MILP models (when they can be solved). Do you have an idea why ? You mention that the LS is very fast. What if you let it run longer (maxIterNoImprove=50 seem quite small) ? Is it really trapped in local optima? In fact the local search does not seem very sophisticated. Did you try more complex moves like 3-opt? By the way, I do not get the description of the 2-opt move described as "randomly swapping two vertices". Usually, the 2-opt move consists in reordering routes that crosses so it is not just about swapping two vertices in a path.

Some more detailed comments
----------------------------

In the introduction, when mentioning the applications of OPs, you could also mention its application in planning, more precisely in over-subscription planning problems. See for instance: D. Smith. Choosing objectives in over-subscription planning. In Proceedings of the 14th International Conference on Automated Planning and Scheduling (ICAPS-2004), p 393–401, 2004. Here an OP is used as a relaxation to solve the original planning problem.

About the MILP-Percentile approach:

- In the section 4.2.2 on MILP-Percentile, you say that the percentile is computed over all the Q samples. Why don't you compute it from the original probability distributions? Wouldn't it be more accurate?

- In general, in the experimental section, you never directly compare MILP-SAA and MILP-Percentile. You just say that MILP-Percentile scales better than MILP-SAA and use it instead for the time-dependent problems. What about a direct comparison between MILP-SAA and MILP-Percentile on the SOP instances?

- It seems to me it should be possible to provide more theoretically grounded results related with the fact the MIP-Percentile approach is a good approximation. The total duration of the path is just the sum of all the individual travel times (the situation would be more complex if you would have waiting times due to time-windows on visits for instance) so one should probably be able to show some results for particular probability distributions or when the path involves many visits ...

In the section about the optimization of LS by using matrix computations, it seems that what you can get as a speed-up in average is more or less a factor 2 as you short-cut the computation for the steps before the first change (which can be considered as uniformly randomly distributed). In this case, does it really worth the additional memory consumption ?

In the experimental section:

- Is there a theoretical justification for the choice of different values for \alpha and \alpha' or were these values chosen empirically and, if yes, how?

- The default values for \alpha (0.3) and \alpha' (0.1) do not map to each other according to the values described in the second item in the list before. Is it expected?

- On p20 when discussing the effect of varying the violation probability \alpha', I'm very surprised that increasing \alpha' does not increase the reward. This is very counter-intuitive as for instance for an extreme case (\alpha'=1 !) one should be able to perform all the visits and achieve a very high reward (while of course being sure to violate the deadline). Do you have some explanation?

- In the generated problems described in the end of page 24 by randomly generating some theta parameters, you end up with mean travel times that do not satisfy the triangle inequality. But it does not really make sense in practice, right? (if for traveling from A to B it is shorter to go through C in between, then you will go through C and you can decrease the travel time between A and B). So you probably should rework these values, for instance by running a Floyd-Warshall algorithm ...

In the section 7 about related work, you do not cite any work in time-dependent travel times in deterministic versions of the OP problem. Is it because there is no work in this domain? You could mention some works on time-dependent travel times in TSP/VRP. 

Typos
-----

p5, section 3.1: Stocahstic
p22, close to the end of page: We also report the the percentage ...

Summary
-------

The topic of the article as well as the current material show that there is clearly the potential for a very good JAIR paper. I would recommend rejecting the current version of the paper with strong encouragement to resubmit. The two main points to be improved are:
- a better justification of the compromise that should be made between the accuracy/complexity of the model and the capacity to solve it (in particular as far as time-dependent travel times are involved), and
- a more detailed analysis of why the proposed Local search performs so poorly (and ideally, some improvements of the local search)





