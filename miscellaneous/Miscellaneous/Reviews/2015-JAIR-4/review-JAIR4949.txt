Title: Solving Risk-Sensitive Stochastic Orienteering Problems using Optimization and Local Search
Author(s): Pradeep Varakantham, Akshat Kumar, Hoong Chuin Lau and William Yeoh

Overall evaluation
==================

[ ] Accept as is
[ ] Accept with minor revisions
[X] Reject with resubmission encouraged
[ ] Reject


Comments and suggestions
========================

Even if this new version of the article has been improved, I'm not really convinced by the answers to several of my 
questions on the previous version:

>> Q:
>> First, a comment on the terminology: "dynamic" travel times (as other dynamic aspects in general) usually refer to 
>> estimated travel times that may change on-line during the execution of the route. Here, I think you are solving a 
>> "static" problem that is: computing off-line a maximum reward path. So I would rather speak of "time-dependent" 
>> travel times which is the classical terminology used in TSP/VRP. I will stick to this terminology in my review.

This point was not answered and no argument was brought in favor of the term "dynamic" that is used all along the paper. 

>> Q:
>> A weakness of the article in its current state is that the adequacy of the problem definition with the application 
>> should be better justified. That the theme park activity sequencing should be considered as an orienteering problem 
>> is clear, and it is a very good and original application of OPs by the way. The extensions to stochastic aspects 
>> make the problem much more complex and harder to solve. As a result, these problems must be somehow simplified 
>> (scenario sampling like for MILP-SAA or even reduction to a unique scenario, so to a deterministic problem for the 
>> proposed MILP-Percentile approach). For these stochastic aspects, the experimental results provide enough arguments 
>> to justify the model extension. But it is less clear for the time-dependent aspects: 
>>
>> - What if you would compute the paths assuming time-independent travel times (for instance by considering average 
>> travel times or by integrating the time-dependency into the stochastic aspects). That way, the MILP approaches would 
>> probably allow solving the (simpler and less accurate) problem to optimality for the synthetic dataset. It would be 
>> very interesting to show in the experiments how it would compare with the local search. Or even, you could just 
>> decrease the number of time steps (100 intervals more or less correspond to 5mn time-windows, is such a precision 
>> really required?). In general, my point here is that solving a complex and accurate problem with a relatively poor 
>> method (like the Local search seems to be here) is not necessarily better than optimally solving a less accurate 
>> but simpler problem so the question to answer with a some additional experiments is "does it worth 
>> complexifying the problem?".

> A:
> In other words, both the above questions pertain to whether reducing the number of
> intervals (by merging of intervals) helps. The reviewer is right in that we had not provided
> a comparison of MILP-Percentile for DSOP with reduced interval space against local
> search. To address this, we have now provided the results for multiple reductions in
> number of intervals for the same original DSOP on both real and synthetic problems.
> These results are provided in Tables 7, 8 and 9 with descriptions provided under the title
> “Approximating DSOP by merging consecutive intervals” in Section 6.2.2. Please note
> that if we have one interval, DSOP is equivalent to SOP (i.e. time independent travel
> times).
> Conclusion from these results is that MILP-Percentile is significantly better than local
> search if reduction in intervals is not significant (i.e. in real world problem). However, local
> search performs better in most cases if reduction in intervals is significant (i.e., in the
> synthetic problem).

If I understand well, the baseline tests both for the synthetic data were performed with 100 intervals 
whereas the total number of intervals in the real world example is 11. The new experiments with less intervals are 
interesting but it is missing some real conclusion from the point of view of the application:

On the real data, new experiments seem to suggest that reducing the number of intervals does not really help compared 
with the original MILP-Percentile on the full model with 11 intervals (only for Horizon=8 on peak days, there is a 
little improvement in using fewer intervals). As the results with fewer intervals can only be better  
if the original MILP does not terminate with optimality proof, I suppose it works better for peak days because the original
problems cannot be solved to optimality. In this context it would be good to see in which cases the MILP is solved to 
optimality and if not, have an idea of the gap.

On the synthetic data set, it is not clear what is the effect of reducing the number of intervals. Why did you start 
with 10 intervals (instead of 100), so a small 10% factor? Did you try with less extreme reductions 
(for instance 75% or 50%)? Is it because the MILP could not find any solution? I suppose it also depend on the 
problem size: synthetic data is of different size (20,32,63 nodes), it would be useful to see the impact of the 
problem size on the new results.

>> Q:
>> - A possible direction to simplify the problem while exploiting the particular features of the application is that 
>> it seems that the time-dependency (and the uncertainties) depends more on the activity itself (as mentioned in the 
>> article: queues at a particular attraction) than on the transition between attractions so that most of the variation 
>> is on the duration of the visit of attractions rather than on the travel time between attractions. Don't you think 
>> that considering time-dependent and stochastic durations of visits and simpler time-independent or/and deterministic 
>> travel times between visits would be a possible compromise?

> A:
> We agree with the reviewer that main component of travel time is wait time. However, that
> does not imply that travel time is time independent. In addition to the results above the
> following visitation patterns in user trajectories observed during the field test at the theme
> park provide a strong case for time dependence:
> (a) People do not prefer to go to thrill rides (large roller coasters) immediately after lunch.
> (b) There is a preference to go to dark and wet rides in the afternoon when the
> temperatures are the highest.
> (c) People tend to visit the most popular attractions early in the day or late in the evening.

My question is not about time-dependency in general but about the choice to represent time-dependency in travel times. 
The 3 examples you cite are very interesting from this point of view: they all deal with "absolute" time values of 
activities/visits/nodes ("after lunch", "in the afternoon", "late in the evening") rather than transition times 
between consecutive activities. And also the term "preference" you use seem to suggest that they should be part 
of the objective function rather than considered as hard constraints. I really do not see how you would integrate 
these aspects into the time-dependent travel times of the proposed model. Whereas if on the contrary the model 
was more focused on the uncertainties (and preferences) at the level of activities themselves it would be easier and 
probably more flexible to extend.

>> Q:
>> - On the other hand, there are some aspects of a theme park activity sequencing that seem to be important and 
>> are not mentioned in the article like precedence constraints between visits or, even more important, specific 
>> time-windows (for instance the shows that run according to a predefined schedule).

> A:
> Indeed, time windows and precedence constraints are interesting and important. They
> can be accounted for in our MILP formulation with minimal changes. We have described
> these changes in Appendix B and also here.

That's interesting indeed. In fact as in the MILP you already have the time variables a_i, it is quite clear that 
time-windows and precedences can be handled by the model (but having the full model as in the appendix is good). 
In fact my original question was more focused on the possible issues/extensions of the Local Search to handle these 
constraints and especially the time-windows. Do you have any idea about it? There is a problem of "feasibility" 
of the permutation of nodes in the LS due to the time-window constraints. In this context, and also in relation 
with the "related work" section, I think you should also mention the Constraint Programming approaches. 
These approaches solve this "feasibility" issue thanks to the notion of constraint propagation. 
These approaches have shown their interest in some problems close to the time-dependent OP, for instance a recent 
paper here on the time-dependent TSP:

P. Melgarejo, P. Laborie, C. Solnon. A Time-Dependent No-Overlap Constraint: Application to Urban Delivery Problems. 
Proc. CPAIOR 2015. p1-17.


>> Q:
>> The proposed local search approach does not perform well compared to the MILP models (when they can be solved). 
>> Do you have an idea why ? You mention that the LS is very fast. What if you let it run longer 
>> (maxIterNoImprove=50 seem quite small) ? Is it really trapped in local optima? In fact the local search does 
>> not seem very sophisticated. 

> Indeed, the solution seems to be trapped in local optima. For small increases in
> numIterNoImprove, there was no improvement in value. When we increased
> numIterNoImprove to 500 and total number of iterations to 3500, the solution improved.
> However, this increase was not substantial (maximum difference of 30 on peak and non-
> peak data sets) and came at a significant improvement in runtime (more than 400
> seconds).
> This could be because of the time-dependent travel times that require significant amount
> of backtracking (many nodes in the current best solution to be changed) to identify a
> better solution.

Indeed, and probably the neighborhood graph is not even connected so one could show that there is no way out 
of some local optima.

>> Q:
>> By the way, I do not get the description 
>> of the 2-opt move described as "randomly swapping two vertices". Usually, the 2-opt move consists 
>> in reordering routes that crosses so it is not just about swapping two vertices in a path.

> A:
> We have not used the standard method of reordering routes that crosses because
> of the following reasons:
> 1. Travel times between nodes are time dependent, so one could construct an
> example where un-crossed routes (in the sense of travel time) does not always
> produce a better solution than than crossed routes.
> 2. Our problem is not restricted to the 2-dimensional Euclidean space, i.e.,
> DSOPs can also represent more abstract problems where routes and crossing
> of routes are not purely based on distance. Travel times in our theme park
> problem is one such example, where waiting times at attractions are merged
> together with the travel times. 

I do not understand. 3-opt (or 2-opt) moves are independent on how travel times between nodes are modeled and 
whether or not the travel times is a 2-D euclidian distance. These moves can be defined for any solution 
represented as a sequence of nodes. Example:

Current solution:     A -> B -> C -> D -> E -> F -> G
2-opt((B->C),(F->G):  A -> B -> F -> E -> D -> C -> G

>> Q:
>> Did you try more complex moves like 3-opt? 

> We did try 3-Exchange operators. However, it had the same impact as increasing
> the numIterNoImprove. That is to say, there was a small improvement in solution
> quality. For the fixed 1500 iterations and 50 numIterNoImprove, 3-Exchange
> resulted in a maximum improvement of 20 on peak and non-peak data sets. More
> specifically, the performance of 3-Exchange with 1500 iterations and 50
> maxIterNoImprove was very similar to 2-Exchange with 3500 iterations and 500
> maxIterNoImprove.
> The reason for this result is that 3-Exchange can be represented as two 2-
> Exchange operators. Given sufficient number of chances to find the right sequence
> of 2-Exchange operations, 3-Exchange based local search is simulated using 2-
> Exchange based local search.
> Results provided in the section on “Improving Local Search” in 6.2.

I guess this is because 3-Exchange really is too close to 2-Exchange and that you should try more diverse neighborhoods.

Typos
-----

p6, section 3.1: "For instance, large roller coaster ..." I do not see the relation between "large roller coaster are 
not preferred after lunch" and the previous sentence that it is supposed to illustrate which is about waiting times. 

p31, table 7: "Hoirizon"

p 33, figure 8: I would swap the colors between MILP-Percentile and Local Search to be consistent with other figures 
(e.g. figure 7) for which LS is represented with a darker color.

Summary
-------

Although the new version of the article improves on the previous version, I'm still not convinced by the way the 
two main issues I raised are addressed:

> - a better justification of the compromise that should be made between the accuracy/complexity of the model 
> and the capacity to solve it (in particular as far as time-dependent travel times are involved), and

It is still not clear to me that the targeted application really needs both time-dependency and stochasticity 
at the level of travel times themselves. From an application perspective, I would tend to think that handling 
time-dependency and stochasticity at the level of the nodes/visits would be simpler (shortly speaking, they 
hold on a linear number of objects - the nodes - rather than on a quadratic number of them - the transitions.) 
and more flexible and efficient to extend to specific application needs (like time-dependent preferences on 
activities these preference could be aggregated in the cost function, stochastic waiting times at the individual 
attraction lines, etc.). 

> - a more detailed analysis of why the proposed Local search performs so poorly (and ideally, some improvements 
> of the local search)

The results of the LS are disappointing. In general one design a meta-heuristic approach when an exact approach 
does not scale well (this is the case here) but one uses the results of the exact approach to show that, at least on 
small problems, the meta-heuristic achieve good results. This is not the case here as even on small problems, 
the reward of the LS can be less than half of the one of the exact approach. This is a huge gap. In the article, 
no clear conclusion is drawn on the usability of the LS approach for the application but for me it means that it is not
usable. Disappointing results are not an issue per se, they can be useful if they shed some light on some particular 
aspects of the approach, but this is not the case here beside the fact that the only 2/3-exchange moves are not 
sufficient to escape local optima.








