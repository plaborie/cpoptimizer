Industrial-Size Job Shop Scheduling with Constraint Programming

Recommandation:
 [ ] Accept
 [X] Minor Revision
 [ ] Major Revision
 [ ] Reject

Overall Manuscript Rating (1 - 100) : XX

For your convenience, and to take advantage of word processing features (e.g., spell-check, bullets, numbering), we suggest you use your regular word processing program (e.g., Microsoft Word, WordPerfect) when typing your review. You should then Copy and Paste your comments into the boxes provided. Click the Save & Submit Later button to save your comments and continue working.

 [X] In the event that this manuscript is rejected by this journal and transferred to another Elsevier journal I agree that this reviewer report, my name and email address may be transferred alongside it.

   Reviewer Guidelines

Please rate on a scale of 1-3 whether the Graphical Abstract is a meaningful and an accurate representation of the article. 1 = Meaningful; 2 = Not Meaningful; 3 = Not Provided. For more information, see www.elsevier.com/graphicalabstracts.

3

Please rate on a scale of 1-3 whether the Highlights are a meaningful and accurate representation of the article. 1 = Meaningful; 2 = Not Meaningful; 3 = Not Provided. For more information, see www.elsevier.com/highlights.

1

Reviewer Blind Comments to Author
Insert Special Character

The article studies the applicability of Constraint Programming (CP) solvers to the resolution of industrial size scheduling problems.

After an interesting introduction to scheduling problems (both from an academical and an industrial perspective), the article makes the statement that there is still a huge gap between academical work on exact problem solving (often using MILP formulations) and real-world scheduling applications (that often need to resort to heuristics or dispatching rules). One of the reasons identified by the authors is the focus of academia on pretty small instances in classical benchmarks that are order of magnitude smaller than real problems. Considering that some approaches like CP have made important progress in the recent years and that these techniques are often not known enough (both in industry and academia), the paper proposes an update on the performance of two state-of-the-art CP engines (OR-Tools and CP Optimizer) on the most classical scheduling problem (the JSSP) using classical benchmarks as well as some new benchmark consisting of larger problems closer to actual industrial size.

Of course, the JSSP is a very simplified problem. Playing on the problem size to make it more realist is an interesting dimension. It would also be very interesting to play on other dimensions like the ones suggested in the paper: what about introducing machine allocation (like in the flexible jobshop), together with sequence dependent setup times and more realistic optimization criterions than the makespan (like total weighted tardiness) ? This type of scheduling problem can very easily be formulated in CP. But of course, it is understandable that one has to set some limits and I think the classical JSSP is fine for a first step.

"The instances with 1 million operations, while not having a correspondence to real life cases, are useful to test the limits of the solvers". I think that these instances have correspondence in industrial applications. For instance I have seen some users solving a problem with 1 million operations in the context of aircraft assembly using CP Optimizer. The problem was more of a Resource-Constrained Project Scheduling Problem (RCPSP) than a JSSP but this type of problem clearly occurs in the real life.

The generated instances with known optimal makespan are "very" specific in that there is no slack in the optimal solution. In fact it makes it very easy for a CP engine to find the optimal solution at 600000. First 600000 is a trivial lower bound that can be computed by taking the max for each machine of the sum of the operation durations on that machine. This is what CP naturally does and, at the root node or very early in the search, both engines know that there does not exist solutions better than 600000. Given the particularity of the instances with known optima, a simple approach (that is probably used by OR-Tools in multi-worker) is to set the upper bound to the Lower bound (600000), let the constraint propagation propagate and try to complete the solution by fixing some variables if needed. This works very well if doing it by hand (I tried), both for OR-Tools and CPO and almost immediately produces an optimal solution.


"The results on group 4 are particularly significant, in the sense that CPO systems were not able to solve any of the instances of this group to optimality": My understanding is that ORT in multi-core is using (among others) the above strategy that tries to find a solution with the upper bound on the makespan equal to the lower bound (which is trivially computed as 600000). And indeed on this very particular instances, this pays off because there is a feasible solution at the lower bound and constraint propagation fixes most of the decisions. CPO does not use this type of strategy, at least not in the used version (12.10) (the reason is that, on real problems, this type of strategy is deemed to fail). But it can of course be emulated by running a resolution with an additional constraint makespan<=600000.

"However, ORT seems to scale very well with an increased number of cores, which can be beneficial in scenarios where computational power can be easily scaled. Moreover, ORT/quad-cores the best performer on the instances of group 4, by a non-negligible margin (about 33~ better than CPO).": As explained, I think that this behavior is due to a particularity of the instances with known optima, not by a general ability of ORT to take better advantage on the number of cores per se. I think the ability for the systems to take advantage of different cores is better illustrated on the set of large instances with unknown optimal makespan (Table 7).

"ORT uses a much more simplistic approach based on random variables and constraint selection": Still, given the behavior of ORT on instances with known optima, it is probable that in multi-core ORT also uses some particular strategies like "fix the objective to the current lower-bound and try to fix the remaining decision variables if propagation did not fail".

"This can be attributed to the fact that CPO enforces determinism in the solution search process": I think that at some point ORT also introduced (or planned to introduce) deterministic search.

"since the randomness is “disturbed” by the need to synchronize": I would not say that the determinism "disturbs" the randomness because the randomness works just the same with or without determinism. The drawback of determinism (from a performance point of view) is that is introduces some "waiting time" between the cores when "fast" cores have to stop and wait for "slow" ones to finish their quota of work.


"The most remarkable improvement is in the group of the largest instances (1000x1000), where CPO/quad-core was not only capable of finding solutions for all instances in the group, but it was capable to do it within 10 minutes. In comparison, CPO/single-core took almost 6 hours to fin solutions for just six of the instances.": This can be explained by the use of an additional technique (Iterative Diving) when 4 cores or more are used. This technique was designed to quickly produce feasible solutions on large problems. It could also be turned on in single-core using SearchType=IterativeDiving and then, I guess, first solutions would be found very fast even for 1000x1000 jobshops, but convergence would probably be slower than the default behavior on smaller problems.

I think the article could be improved by providing more insight on the behavior of the solvers like the ones I mention above.


More detailed comments:

p4, col2: "A special case of the JSSP is the open shop scheduling problem (OSSP). In fact, an OSSP can be seen as a JSSP where the ordering of the operations of a job is arbitrary [41]." So it is not really a special case of JSSP, because you cannot formulate an instance of OSSP as a JSSP ...

p6, col: in the mathematical formulation of the job-shop problem, maybe it would be easier to describe the no-overlap constraint as a disjunction: (end_x <= start_y) || (end_y <= start_x)

p10, col2: In the description of the CP models: "Two integer matrices are expected as input: for each operation, opLength contains the operation length and opSuccessor contains the machine index of its successor." This "opSucessor" is not very conventional. Why don't we simply have an opMachine that denotes the machine of a given operation ? As you also do not assume (in the instances) that the problems are rectangular, maybe you could just describe the model with a variable number of operations op[i][j] per job j and just two array data: pt[i][j] (processing time of operation i,j) and machine[i][j] (machine of operation i,j).

p11, col2: "CP Optimizer uses its own modelling language, OPL, and does not support MiniZinc". CP Optimizer models also can be written in C++, Java and Python, not only OPL. Furthermore, a connection to Minizinc was recently implemented (but not really advised for modeling complex scheduling problems as Minizinc does not allow for several efficient concepts of CP Optimizer like complex cumul functions or state functions).

p15, col1: typo: "emphall"


Reviewer Confidential Comments to Editor
Insert Special Character

For each question, please use the following scale to answer (place an x in the space provided):

"To what extent does the article meet this criterion?"

0	Fails by a large amount
1	Fails by a small amount
2	Succeeds by a small amount
3	Succeeds by a large amount
4	Not applicable

The subject addressed in this article is worthy of investigation.

3


The information presented was new.

2



The conclusions were supported by the data.

3



Is there a financial or other conflict of interest between your work and that of the authors?

YES __  NO _X_

Please note that your recommendation and reviewer report are expected to cover the Highlights and Graphical Abstract if submitted with the manuscript.

Please give a frank account of the strengths and weaknesses of the article:

Strengths:
- The paper gives evidence that current CP engines can handle job-shop scheduling problem of a size similar to the one usually found for industrial problems

Weaknesses:
 - The new benchmark with known optimal values is very specific. So specific that, in my opinion, it can hardly be used to compare the ability of engines to find good quality solutions because there is an easy way to take advantage of the specificities of the instance to quickly produce a proves optimal solution
