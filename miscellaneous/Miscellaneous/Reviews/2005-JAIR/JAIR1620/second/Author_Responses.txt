> ==================================================================
> Review #1
> ==================================================================
> Overall Evaluation
> ------------------
> 
> Reject
> 
> It is important to take a deeply empirical research approach to interesting
> real-world problems. This style of research should be encouraged. The danger
> of such an approach is that a paper can sometimes resemble lists of detailed
> experiments and results without clear motivations or conclusions. In my
> view, this paper is too far in that direction. Not enough effort has be
> spent on clearly motivating and explaining the hypotheses that are made and
> in explaining and interpreting the results of the experiments. It reads too
> much like a chronological series of experiments rather than a presentation
> of the new understanding that has arisen and the experiments that support
> that understanding. At least a significant re-writing to make the paper less
> of a report on a series of experiments and more a presentation of
> hypotheses, insights, and experimental evidence is necessary for acceptance.
> 
> 
> Comments to Authors
> -------------------
> 
> This paper presents a series of experiments on heuristic algorithms (local
> search, GA, Squeaky Wheel) applied to an oversubscribed satellite request
> scheduling problem based on real world data. The paper takes an nice
> empirical approach of trying to investigate the problem, algorithm, and
> search space characteristics that lead to the observed performance.
> 
> General Comments
> 
> - This is an interesting research approach that should be
>   encouraged. However the presentation is somewhat unclear, the motivations
>   and explanations of the individual experiments are hard to understand, and
>   a central theme does not come out clearly. For example, the main thrust of
>   the paper seems to be something like: we have a set of algorithms that all
>   perform reasonably well on a set of problems but these algorithms seem
>   quite different - from studying the details of the algorithm performance
>   in different experimental conditions, we hope to identify underlying
>   characteristics that explain the performance of each algorithm. More
>   ambitiously, I suspect you also want to try to find characteristics that
>   explain the performance of *all* algorithms despite their surface
>   differences. This is a very nice approach but it is never clearly
>   stated. Rather (especially in Section 1 and Section 6 when the gory
>   details are presented) what comes across is a set of relatively
>   independent experiments with no overriding unifying theme. Something needs
>   to tie the experiments together.

Your comments made perfect sense. We tried to re-cast the paper more
or less along these lines. We also used the results to motivate the
design of a new algorithm that we show does well on this application.

> - If you indeed are striving toward identification of characteristics that
>   explain all the algorithms' performance you also need to argue that it is
>   reasonable to believe that the algorithms are "different" at a surface
>   level. Whether these algorithms are in fact different does not seem clear
>   as they use the same search space and use the same greedy scheduler. Given
>   the importance of these two characteristics, it is not clear to me that
>   one should expect the algorithms to perform very differently. It would be
>   useful to show that another algorithm using the same search space and
>   schedule generator performs much differently. (The anytime graphs
>   presented at the end of Section 6 are useful in this respect but come too
>   late as it is necessary to make this point early in the paper).

The algorithms do use the same search space, but it is fairly large
(N!). HBSS did poorly in the same space. Other algorithms did poorly,
but we did not report their results because we could not exert enough
control over them to feel justified in any conclusions. We tried to
emphasize the differences between the algorithms in our presentation
and moved up the anytime graphs.

> - Discussion of the results of the experiments (e.g., Sect 5) is just a
>   list. I found the textual description of results confusing as it was
>   simply a list of what could be observed in the tables. It would be useful
>   to add some explanatory text about what is concluded from these
>   experiments rather than requiring the reader to make their own
>   conclusions.

The performance results are simply included to show that three algorithms
perform fairly well. Section 5 is not intended to be an experiment in
a hypothesis testing framework. 

> - Clear motivation is lacking for some of the experiments (see comments
>   below on Section 6). Such motivation is critical for this type of work:
>   it must be clear that you are making and testing hypotheses rather than
>   just trying stuff. Special care should be taken to differentiate
>   experiments designed to reveal underlying reasons for particular behaviour
>   of algorithms and other experiments (e.g. those reported in Section 6.1.2)
>   that seem more motivated by inventing algorithm variations that will
>   perform better.
> 
> - Very detailed analysis means that at points the reader loses the forest
>   for the trees. For example on p29, the detailed discussion means that I am
>   no longer aware of the big picture - why are you doing these experiments?
> 
> - The writing is unclear and confusing in places (see detailed comments).
> 
> - It would be very useful if the problem instances could be made publicly
>   accessible.

We agree. We have approval to make AFIT problems available and have
done so. We do not have approval for the new problems, but have run
other people's code on them. We added a footnote explaining data
availability. 

> 
> Detailed comments
> -----------------
> 
> Abstract:
> "we identify a set of ... algorithms that perform best relative to
> several constraint directed ...". A pedantic point, but how does a set of
> algorithms "perform best"?

Changed.

> "proclivity of plateaus" - if this is a specialized usage, it does
> not seem terribly common (I am not familiar with it) and should be
> defined. If it simply means the size and number of plateaus, the saying "the
> size and number of plateaus" is much clearer than "proclivity"

Removed.
 
> "by operating multiple changes" - do you mean "by making multiple changes"?
> Operating is an awkward and confusing word choice.

Changed.

> I found the abstract lacking in a clear message: "we tried some stuff,
> various things had different levels of importance". What is the message of
> the paper? I felt this lack of a clear focus throughout the paper.

Re-wrote abstract.

> Section 1:
> "Clearly, factors such as the size of the search space, the presence of
> plateaus ..., the distribution and size of plateaus favor some algorithms to
> the detriment of others". I find this a strange statement. Is this a
> hypothesis you are planning to test? Is it an assumption you are making? Is
> it something that has already been demonstrated in the literature (no
> references are provided)? It seems to me that this is a reasonable idea and
> that one of the goals of this paper is to identify and build a deeper
> understanding of such reasonable ideas. Starting the sentence with "clearly"
> confuses me as it seems possible (if the goal of the paper is as I just
> stated it) that the reasonable idea is false and that is something you are
> interested in uncovering.
>
> "We found that narrowing the distance to the optimal solution helps but is
> not by itself enough". Enough for what? What are you trying to say here?
> 
> "We identify problem characteristics that influence algorithm performance,
> which is likely to hold on similar problems" - There is an English problem
> here that leads to some confusion. What is likely to hold? What does "which"
> refer to? You need to more clearly state your claim. On what basis can you
> make the claim that the characteristics that influence the performance of
> the tested algorithms will also be observed as influential for similar
> problems? This is an important point as it goes to the generality of the
> work.

Section is re-written with new focus. The statements mentioned here are gone.

> Section 2:
> "Of the 500 requests, often about 120 conflicts remain ...". What is a
> conflict here? Does this mean that 120 of the requests cannot be scheduled
> (since they conflict with other scheduled requests) or does it mean that 120
> events conflict with each other (i.e., about 60 cannot be scheduled)?

Have defined conflict.

> Since a request is a set of pairs of (R_i, T_i^Win), I guess that the task
> durations are constant and independent of resource choice. It would be
> useful to state this explicitly. Do all requests have the same fixed
> duration?

Re-phrased.

> Table 2: the definition of #requests is not completely clear. "with
> probability one these requests require the particular time slot". However
> the #requests seem to be the number of tasks competing for the resource at
> the time *point* with the max SumHeight. So what is the time slot? More
> specifically, must each request be executing at the max time point (ie., the
> probability is 1 for the time point) or is it the case that activities may
> be shifted within their time windows and so do not necessarily have to
> execute at the time point?
> 
> Table 2: "The last column is the mean percentage ..." Given that all the
> values are 1.0 or less do you mean that the column is the mean proportion?
> (i.e., one should multiply by 100 to get the mean percentage?)
> 
> The explanation of Figure 4 and Table 2 on p. 8 is confusing as there seems
> to be two different notions that are called requests: the number of requests
> that contribute to the contention and the number that must be
> executing. This is related to my confusion about about Table 2 above.
> 
> "any constructive algorithm that orders by contention will have a high
> branching factor". Again a pedantic point, however a constructive algorithm
> that sequences the requests will have a branching factor of 2 (Ri before Rj
> OR Rj before Ri) independent of the heuristic used to identify the requests
> or preferred sequence.
> 
> The fact that many requests share the same resource alternatives suggests
> that many symmetric search states might exist and that another approach to
> scheduling may be to identify and break these symmetries.

For the clarity of the paper, we removed section 2.1.  Table 2 and
Figure 4, as well as the statement above are gone.

> Section 3:
> There seems to be a typo or inadvertently included text fragment on
> p. 10 in the discussion of the Lemaitre work.

Fixed.

> p. 11: It is claimed that the neighborhood size for the AFSCN problems is
> O(N^2) but the actual neighborhood has not yet been defined.

Quite right. We removed that paragraph. 

> Section 4:
> "each task is assigned to the first available resource" - this suggests some
> ordering over the resources. What does "first" mean? Does it mean the
> resource that is available at the earliest time within the time window of
> the request? How are ties broken? Later you say that the generator does
> favor the order of the resources: what order is this? Given the problem with
> plateaus in the search, perhaps randomizing the resources before each
> generation would be useful in diversifying the schedule generator?

We tried re-phrasing to clarify. 

As to the proposed randomization, we tried variants on the positioning
strategy used in the schedule builder and found no other that did
better. Given the need to focus the paper, we did not include those
results. We include a discussion about randomizing the schedule builder in 
Section 6.1, third paragraph.

> p.13: typo: "we chose the position x by random"

Fixed.

> Section 5:
> p15 typo: "CPU times are dominated by the number of evaluations and [are]
> therefore similar"

Fixed.

> Section 6:
> The motivation for the pairwise interaction experiments is unclear. You have
> argued that the search space has plateaus. Are you seeking
> empirical data to backup the argument?  Since you later do measure the size
> of the plateaus using a random walk length, I am further confused as to the
> motivation for the pairwise experiments.
> 

We tried to fix this by reorganizing the whole section

> [As an aside, one might be able to use the existence of non-interacting
> pairs to reason that a specific pairwise shift will lead to exactly the same
> schedule without having to evaluate the schedule. Something similar is done
> by Bent & van Hentenryck in the context of their regret heuristic for online
> problems. (And in fact they comment on the interesting connection with
> local search moves).]
> 
> p20: A diagram for the example would be very helpful. I had to draw one to
> understand the argument.

This is part of Section 6.1.2, which has been removed.

> 
> The motivation for the experiment in Section 6.1.2 is similarly
> obscure. What characteristics are you trying to uncover? What hypotheses are
> you testing? What does this have to do with "Redundancy in the Search Space"
> (the title of Section 6)? The ideas and experiments in Section 6.1.2 are
> nice but would seem to fall into the "competitive testing" mode of trying to
> build a better algorithm (by restricting the neighborhood) rather than the
> goal of developing an understanding of the performance of the existing
> algorithms.

We removed Section 6.1.2

> 
> p25: I am a bit confused by the complicatedness of the experiments in
> Section 6.3.1. If you want to evaluate if Genitor (or any of the other
> algorithms for that matter - why did you focus on Genitor?) was learning to
> schedule the low altitude requests first, you can just look at the
> permutation and measure the locations of the low altitude requests. Does
> this simple measure fail in some way, requiring the comparison with the
> split heuristic generator?

We removed Section 6.3.1.

> 
> p30: The conclusion to Set 6.3.2 would seem to be the opposite of what is
> suggested in the discussion of the data. I understood that you found little
> evidence of "patterns of request ordering" in the data yet you claim that
> Genitor finds them. I don't see how the conclusion follows from the data. As
> a related point, I would think that your earlier evidence that a large
> number of the pairwise shifts have with no impact on schedule cost would
> mean that the existence of such patterns is quite unlikely. Since many of
> the relative orderings lead to the same cost (and more of them for good
> solutions), I would think the existence of backbone like patterns to be
> relatively rare. If Genitor has really found them, you should explain why
> their existence does not conflict with the earlier empirical results.

We offer a probabilistic argument as to why the number of common pairs
found in Genitor solutions is indicative of patterns present in the
solutions.  We found that most common request ordering chains have a
length of 2. However, we also found that longer chains are common for
subsets of solutions.  We make the connection with the building block
hypothesis: while such longer chains are needed to build a good
solution, different chains can result in exactly the same solution,
and therefore when looking at *all* the solutions common longer chains
can not be found.

> 
> p32: I do not think your experiments are conclusive in showing that large
> moves are by themselves necessary. An alternative explanation is that moving
> only a small number (or even one) of the requests earlier is all you have to
> do, but that it is difficult to find which request you need to move. By
> moving all of them earlier you will obviously find the right one.


We agree that multiple moves might not be necessary if we knew which
request to move. We mention this (as well as attempts to identify such
a request) in Section 6.5 (second paragraph) as a possible alternative
explanation, as well as in Section 8 as future work. Also, in Section
6.5 we mention some of our more recent work in finding where to move a
request to always get a change in the schedule.

> 
> p35: pedantic English comment: "We found that such initialization helps, but
> is no by itself enough" ... enough for what?
> 

Fixed

> ==================================================================
> Review #2
> ==================================================================
> 
> Summary: This paper considers incomplete algorithms for the Air Force
> Satellite Communication Network (AFSCN) problem, which involves
> scheduling downlinks from satellites.  The problem features more
> requests than can be satisfied (e.g. it is oversubscribed) and
> multiple resource options, and is posed with two objective functions
> (minimize bumped jobs and minimize overlaps). It has time windows, but
> no other metric temporal distance constraints (e.g. minimum or maximum
> task separation).  The paper combines results from previous work,
> includes new and deeper analysis of existing approaches, and sets out
> to explain the reasons for the success of good algorithms.  The
> authors introduce numerous variants of existing algorithms to prove
> and disprove hypothesis concerning the features of good algorithms.
> 
> Recommendation: I recommend that the paper be accepted.
> 
> General comments: This paper is the latest in a string of good papers
> identifying features of promising incomplete algorithms for the AFSCN
> problem.  This paper provides a good roundup of existing work done on
> the subject; the authors perform a good comparison (with a few
> exceptions, noted below) of the techniques they have tried, and
> provide a reasonable explanation for why the good algorithms work
> well.
> 
> There is some hyperbole early in the paper that should be removed (see
> the specific comments for details).  Further, the analysis performed
> is good, but incomplete in some respects; the authors can manage
> readers' expectations better up front (again, some
> specific comments address this).
> 
> Some of the material presented in this paper has appeared in previous
> work on this subject by the same authors.  In a few cases, it is hard
> to tell what has been previously published; examples are the
> contention discussion on p. 7, the discussion of whether most task
> interactions are pairwise beginning on p. 17, and assessing whether or
> not GENITOR exploits domain knowledge, starting on p. 25 (this isn't
> an exhaustive list).  Some more care should be taken to clearly
> identify new analyses of previous results.
> 
> The authors' hypotheses on "better algorithms" are satisfied largely
> by determining how often a particular algorithm found the best
> possible results for particular problems, or occasionally looking at
> mean and variance data for quality of solutions produced.  Rigorous
> comparisons of outcome distributions would be nice (e.g. Wilcoxon sign
> ranked test with significance results).  No CPU comparisons between
> algorithms are done, ostensibly because the elementary operation
> (building schedules from new permutations) is the most expensive
> operation, and each algorithm gets a fixed number of permutations to
> build.  However, no justification for this is given in the paper.  In
> fact, we have no indication in this paper of how long it takes to
> solve these problems.


These are all good points. We changed the introduction to better
reflect the intent of the paper. We performed statistical comparisons
of the values produced by the different algorithms and included
significance results in Section 5 as well as Section 7. We also
include in Section 5 a justification for using a fixed number of
evaluations and mention the CPU time needed for these problems.

> 
> The paper suffers from some minor organizational problems.  At times
> the discussion jumps from algorithm to algorithm with no transitions.
> Some top-level consideration of the best organization might be in order.  
> 
> Specific comments:
> 
> p. 1: The introductory paragraphs are both incorrect and somewhat
> off-putting.  Numerous authors have attempted to explain the good
> performance of heuristics and incomplete algorithms, some recently
> (e.g. [3][4][5]) and some not so recently ([1][2]).  It is true that
> often authors do not do as good a job as they should at this, but the
> current text appears to be a more universal condemnation of the field
> than is warranted.  Furthermore, the authors miss the important point
> that practitioners must be able to *quickly* select the most
> applicable algorithm from a set based on features of the problem in question; 
> neither they (nor anyone else) has had much success there.

We rewrote the introduction: the two paragraphs are not there anymore.

> 
> p. 2: "We formulated several hypotheses about why particular
> algorithms excel."  Throughout the paper, the authors seem to mix
> features of the search space (the permutations of jobs to be
> scheduled), and the algorithms that operate on this space (and the
> constructor that generates scheduled from the permutations).  It is
> worth pointing out here that the features of the search space are
> fixed (e.g. the set of permutations, the mapping from permutations to
> solutions induced by the particular greedy constructor used, and
> features thereof such as the number of identical schedules, plateaus,
> etc).  The paper only covers variations in how algorithms modify the
> permutations; it does not cover different inputs to the constructor
> (e.g. instructions about resource orderings) or changes to the
> constructor, which might (in fact, likely will) impact which
> algorithms work well.

We agree that the distinction between search space features and
algorithm characteristics was not clear in our previous version of the
paper.  In the introduction for Section 6 (second paragraph) we now
identify the schedule builder and the objective function as part of
the problem specification. We separate the redundancy of the search
space as a dominant feature of the search space and then introduce
hypotheses about algorithm performance based on the presence of the
plateaus and algorithm features.

> 
> p. 3: "However, the simple domain knowledge is not enough to account
> for the performance..."
> 
> awkward; perhaps this would work better:
> 
> "However, heuristics derived from this simple domain knowledge are not
> enough to account for the performance..."

This has been changed (it was in the Introduction and the introduction changed)

> 
> p. 7:  "Four points characterize the individual demand:" 4 equations.
> 
> I had to go back to (Beck et al.) to figure out that what you are talking
> about here is the probability distribution used to sample the start times
> when computing the contention.  This ought to be rephrased to make it clearer
> othwerwise Table 2 and Figure 4 will be impossible to understand.

This part has been removed from the paper

> 
> p. 9: Iterative flattening [8] is another local search-style scheduling
> algorithm; even though you didn't try it, it might be worth mentioning.

> 
> p. 11: MAXSAT is not the problem studied by Gent et al. MAXSAT is the
> problem of maximizing the number of satisfied clauses; Gent et
> al. study SAT, the decision problem of maximizing all clauses, but use the
> # of satisfied clauses as an objective function to drive search.

We fixed this.

> 
> p. 11: "Second, MAXSAT has a fast partial evaluation that allows one
> to quickly evaluate a move. Our scheduling domain requires that a new
> schedule be built from scratch and evaluated after every move."
> 
> In both cases, move evaulation is polynomial time in the problem size;
> in SAT it is usually linear in the number of clauses a variable
> appears in, while in AFSCN it depends on the position a task is moved
> to in the permutation.  That is, if a task is moved to position i,
> only the permutation after i needs to be recomputed.  As all
> conflicting or overlapping tasks are moved, it is likely that most of
> the schedule will be rebuilt, but it need not be.  Does your
> implementation rebuild only the necessary parts of the schedule?

We agree we could have rebuilt only the necessary parts of the
schedule. In our implementation, we do rebuild the whole schedule.
This seemed at the time the natural approach for the genetic
algorithm. In fact, building one whole schedule only takes fractions
of a second.

> 
> p. 12: "Note that our schedule builder does favor the order of the
> resources even though no preference is specifed for any of the
> alternatives."
> 
> This is an invitation to randomly sample the resources; did you try
> this?  Or did you try constructors that searched over resource
> orderings?  Or bulldozing [6]?

We ran some experiments with a schedule builder that randomly samples
the resources; the performance results were worse than for the greedy
scheduler. We also tried variants on the positioning strategy and
found they either did not perform better than our greedy scheduler or
there was some improvement but it took at least one order of magnitude
longer to build a schedule.  Since we needed to focus the paper, we
did not include results.  We include a discussion about why
randomization in the schedule builder might be detrimental to
algorithm performance in Section 6.1, paragraph 3.


> 
> p. 13: "Genetic algorithms were found to perform well in some early studies..."
> 
> should read
> 
> "Genetic algorithms were found to perform well on the AFSCN problem in
> some early studies..."

Fixed

> 
> p. 14 and 17: Your SWO critics are relatively simple.  SWO's
> performance suggests that these simple critics are good enough;
> however, there seems to be considerable latitude for understanding SWO
> by changing its critics, especially given that the analysis of
> SWO1Move on p. 32 (more comments on this later).  Some suggestions:
> a. In your SWO, two overlapping tasks are both moved forward, 
> but only one needs to move to eliminate the overlap.  How does this impact
> performance?

We only move one of 2 overlapping tasks forward: the one which could
not be scheduled and therefore overlapped with something already
scheduled. We explicitly state this now, in Section 4.4.


> b. In your SWO tasks are always moved forwards; why not move some backwards? 
> Especially since the shift operator of your local search allows tasks to
> move either forwards or backwards, and your restricted shift only moves
> tasks backwards. 

Consider two overlapping tasks, A and B. A was scheduled, B could not
fit in the schedule and overlaps with A. Moving B backwards means B
would still not fit in the schedule, since A would preserve its
originally scheduled slot. Similarly for bumps.

> c. Can you do well moving fewer than all conflicting or overlapping tasks?  

We address this question in Section 6.5.2

> d. Your critics are deterministic; would making them probabilistic improve 
> them?  (i.e. a blend of HBSS and SWO) 

Good question, we mention this in the future work section 

> e. (pertinent to the discussion on p. 17): A schedule,
> permutation and bumped job or overlapping job can be analyzed to
> determine where this job would need to be moved in order to ensure
> that it is scheduled or doesn't overlap with the same job
> (similarities to the bulldozing idea come to
> mind.)  A critic based on this would be "directed" in that you could
> guarantee that every move changed a schedule, but need not necessarily
> provide a schedule whose value was different.  Obviously you pay a
> computational cost, but it might be worth all the "dead" time
> generating the same schedule?

We have performed some research in this direction - we implemented a
search operator that only moves jobs in positions in the permutation
such that we could guarantee that the schedule changes. The results
show that search with randomly choosing the position where to move a
job still outperforms the version where such a restricted neighborhood
is used. We mention this in Section 6.5, second paragraph.

> 
> Your future work section is pretty lightweight; perhaps putting these
> ideas into the future work section?

> 
> p. 20: "The restricted shift operator starts with a permutation of the
> requests in increasing order of their earliest starting
> times. Iteratively, we randomly select a position pos in this
> permutation and shift the corresponding request to the right, in
> positions pos+1, pos+2,..., pos + k, where pos + k + 1 is the firrst
> position after pos corresponding to a request with a non-intersecting,
> later time window."
> 
> Why not also shift the position to pos-1, pos-2...until pos-l is the
> last position before pos corresponding to a request with a
> non-intersecting, earlier window?  This is the "symmetric" condition
> to that you imposed.  It seems odd that SWO "promotes" jobs in
> permutations while your restricted shift only "demotes" them.  I don't
> think this solves the problem of "unreachable" permutations though.
> 

We removed the restricted shift operator from the paper.

> p. 21: "We show that as search progresses the random walks become
> longer before finding an improvement."
> 
> You might point out that Gent and Walsh [7] and Frank et al. JAIR 1997
> both observed this phenomenon for SAT.

We included this point (in Section 6.4, first paragraph).

> 
> p. 21: "2) Best known solutions have slightly more same-value
> neighbors than do random solutions."
> 
> Replace "solutions" with "permutations".

Fixed.

> 
> p. 23: "Our initial implementation of the shifting search neighborhood
> iteratively chose by random..."
> 
> should read
> 
>  "Our initial implementation of the shifting search neighborhood
> iteratively chosen at random..."


> 
> p. 25: "[is GENITOR scheduling low-alt first?] We show that such an
> approach can produce good solutions for AFSCN scheduling."
> 
> Furthermore, you show that GENITOR is learning something more complex
> than that.
> 
> p. 25: "Also, the split heuristic is similar to the contention
> measures de ned in (Frank et al., 2001)."
> 
> as well as prominent in CSP heuristics; notably, B. Smith (Trying hard
> to fail first) showed that a "minimum values remaining" heuristic was
> effective because it actually creates deep search trees with few
> branches, while fail-first created fat trees with more branches and
> more nodes!  (A historical note, not essential to include this in your
> paper.)
> 
> p. 27: "...we selected the best solution values that occurred at least 30 times out of the 1000 runs."
> 
> Why?  The best results you report for the other algorithms don't have
> such requirements on them, so why impose this requirement now?
> Especially since you impose different requirements for the R1 problem.
> There's no apparent justification for this, so I can't conclude much
> from Tables 12 and 13.

Table 12 was part of the section "Identifying Domain Knowledge", which
is trying to determine if GENITOR is learning a "schedule low before
high" heuristic.  We removed this section (see below) and Table 12
therefore.

> 
> The purpose of this section's experiments is to determine whether
> GENITOR is learning a "schedule low before high" heuristic.  Section
> 6.3.2 is not, to my mind, a compelling experiment to discover this;
> the experiments in Section 6.3.3 are much better designed, and gives
> better indications of what GENITOR is learning.  You could safely lose
> section 6.3.2 and the paper would not suffer, in my opinion.

We agree. In fact, we removed the section you indicated.

> 
> p. 30: "While Genitor does seem to discover patterns of request
> ordering, multiple different patterns of request orderings can result
> in the same conflicts (or even the same schedule). We could think of
> these patterns as building blocks. Genitor learns to identify good
> building blocks (orderings of requests resulting in good partial
> solutions) and propagates them into the nal population (and the final
> solution)."
> 
> My goodness, a problem for which the Schemata Theorem applies?  And
> you don't even mention it?  :-)

We now mention the classic building block hypothesis for GAs, in the
introduction of section 6.2.

> 
> p. 31: Since you compare results for GENITOR when initialized from
> greedy permutations to SWO and the best known results, it might be
> worth adding these columns to Table 15.

For the clarity of the paper, we removed the experiments where
GENITOR was initialized from greedy permutations

> 
> p. 32: "To test the effect of multiple moves in traversing the search
> space, we implemented a version of SWO where only one request is moved
> forward at each step. For minimizing the conflicts, one of the bumped
> requests is randomly chosen; for minimizing overlaps, one of the
> requests contributing to the sum of overlaps is randomly chosen. For
> both minimizing the conflicts and minimizing the sum of overlaps the
> chosen request is moved forward for a constant distance of five. We
> called this new algorithm SWO1Move."
> 
> This looks like a very simple SWO; there is no guidance for which of
> the conflicting or overlapping jobs to move, nor where to move it.  I
> am surprised it does as well as it seems to; I am also surprised that
> random is better than preferring the maximum overlapping task.
> Referring back to the comment on rebuilding the schedules, I am
> curious if it runs faster?  Also, if a combined HBSS/SWO were used and
> the job to move was chosen based on its overlap, how would this
> compare?

We rebuild the schedule from scratch.  The question you are asking is
very interesting. Would the HBSS/SWO combination achieve better
performance results?  We do mention that if we knew which job to move
forward, multiple-moves might not be necessary (Section 6.5, second
paragraph).  We combined our findings and designed ALLS (Section 7),
an algorithm which was not in the previous version of this paper. ALLS
performs great by exploiting the multiple-move idea in a local search
framework.  It is still an interesting question. Multiple moves are
instrumental, but are they necessary? Is there an algorithm that
performs well by only moving one request at each iteration?  We
mention the HBSS/SWO combination in the future work section.

> 
> References:
> [1] D. Clark et al.  Local Search and the Number of Solutions.
> Proc. 2d Intl. Conf. Constraint Programming, 1996.
> [2] J. Singer and I. P. gent and A. Smaill.  Backbone Fragility and
>     the Local Search Cost Peak.  JAIR, v. 12, 2000, p. 235-270  
> [3] R. Holte et al. Multiple Pattern Databases.  Proc. ICAPS 2004.
> [4] W. Zhang and X. Zhang.  An Improved Local Search For Complex
>     Scheduling Problems.  Proc. ICAPS 2004. 
> [5] R. Zhou and E. hansen.  Breadth First Search Heuristic.  
>     Proc. ICAPS 2004.
> [6] T. Smith and J. Pyle. "An Effective Algorithm for Project Scheduling
>     with Arbitrary Temporal Constraints"  AAAI 2004 p. 544-550
> [7] I. Gent and T. Walsh.  "An Empirical Analysis of Search in GSAT.
>     JAIR v 1 1993
> [8] A. Cesta, A. Oddi, and S. Smith. Iterative Flattening: A Scalable
>     Method for Solving Multi-Capacity Scheduling Problems.  (AAAI-00),
>     July, 2000. 
> 
> 
> ==================================================================
> Review #3
> ==================================================================
> 
> 2. overall evaluation of the paper
> --------------------------------------
> 
> Reject.
> 
> I think the paper still has some important merits if correctly
> improved and the approach followed by the authors should be
> encouraged. They could resubmit the paper after modifications.
> 
> 
> 3. comments to the authors. 
> --------------------------------------
> 
> Overall comments
> -------------------------------
> 
> The paper represents an original contribution to understand why a
> given set of optimisation techniques work (or do not work) for a
> particular problem (here, an oversubscribed scheduling problem).
> 
> As mentioned in the paper, understanding "why" an algorithm works or
> doesn't work well on a given problem is of the highest interest as it
> helps to map problem features with suitable algorithms, and it is true
> that this issue has been underestimated in the scheduling literature.
> 
> The architecture of the paper is good. After a short introduction, it
> describes the scheduling problem (AFSCN application) and then analyses
> related work on this problem. A set of algorithms for solving the
> problem are then described and their performance is given. The main
> section of the paper then successively formulate several hypothesis
> about how some problem features influences the algorithms and check
> them on the basis of some experiments.
> 
> All the algorithms studied in the article rely on the same schedule
> builder which is extremely simple (this is a good property in itself)
> but seem to introduce some bias in the results. Indeed, the schedule
> builder (described in section "4. Solution Representation") is
> allocating resources using a static order that do not depend on the
> context of the schedule that is being built. This certainly increases
> the redundancy of the search space and thus, the importance of
> plateaus which is a major issue when comparing the algorithms. Were
> some trials made using different approaches for the schedule builder
> that would reduce this redundancy, for instance choosing to allocate
> the resource that would allow the task to start as early as possible
> or choosing the resource that is the less used over the time window of
> the task given the tasks already scheduled, etc. ? The study seems to
> heavily depend on which greedy algorithm is used in the schedule
> builder and this is never even mentioned in the paper. Some additional
> experiments should be performed to see which conclusions of the paper
> are still valid when another schedule builder is used.
> 

We agree: we considered the features of the search space to be fixed
(number of neighbors of equal value, plateau size and so on).  As you
pointed out, these features are highly dependent on the schedule
builder. We did implement other strategies for the schedule builder to
place a request in the schedule, and did not find one to perform
better without paying a high price (at least a one order of magnitude
slow-down) in computational cost.  Given the need to focus the paper,
we did not include those results. We include a discussion about
randomizing the schedule builder in Section 6.1, third paragraph.

> Experiments in section 6.3 to check that the GA is learning some
> patterns are not really convincing: first, the GA is used to find very
> good solutions. Then, an analyse of those very good solutions is
> performed to detect some common features. The existence of such common
> features only tells that there exists common features shared by very
> good or optimal solutions, it doesn't say anything on the fact the GA
> actually learned them (for instance the very good solutions found by
> the GA may be due to a last-minute very successful cross-over), it
> only shows that there is potentially something to learn. I think it
> would be much more convincing to study how patterns corresponding to
> good solution tend to become more and more present in the population
> when evolution advances.
> 

We found that only some of the low altitude requests have to be
scheduled before high altitude requests.  We could not figure how to
define in a controlled way tracking common request patterns across
multiple runs of Genitor, as evolution advances.

While it is still true that all our results rely on the same greedy
schedule builder, we tried to re-cast the paper by stating and
investigating hypotheses about algorithm features that explain the
performance of each algorithm.  We used the results to motivate the
design of a new algorithm that we show does well on this application.

> 
> Detailed comments
> -------------------------------
> 
> In section 2, page 4, it is said that "each task request T_i specifies
> j>=0 pairs of the form (R_i,T_i^Win)". There seem to be some index j
> missing in "(R_i,T_i^Win)".

We fixed this.

> 
> In section 2.1, end of page 7, it is mentioned that "The aggregate
> demand for a resource is then obtained by summing the individual
> demands for all the tasks that can be scheduled on that
> resource". Shouldn't the individual demands be first divided by the
> number of possible resources to account for the probability that the
> task will be assigned another resource than the one that is being
> considered?

For clarity purposes, we removed section 2.1.

> 
> It would be interesting to see how all the approaches experimented in
> the article globally compare with the results described in section 3
> in terms of percentage of scheduled tasks requests on the problems of
> type "A".

We added a paragraph about the percentage of scheduled tasks 
for the "R" problems versus "A" problems, in Section 5 (paragraph 4).

> 
> In the end of section 3, the last paragraph uses the notions of
> "moves" and "neighbourhood size" whereas the actual local search
> approach that is used has still not been defined. In particular, it
> makes it hard to understand why the neighbourhood size is in O(N^2).

We agree. We removed these references.

> 
> In section 4.1, it is not clear at all why the greedy algorithm used
> to schedule low altitude requests instead of the first phase of
> Gooley's original algorithm results in an optimal schedule. Something
> should be added to better define what is meant by "optimal" in this
> context.

Gooley's algorithm is trying to optimize the number of low altitude
requests scheduled before scheduling high-altitude; our approach
guarantees the optimality for the first phase of his algorithm
(considering only low-altitude requests). We revised the prose to
hopefully better explain this.

> 
> The description of the HBSS method in section 4.5 is really not
> clear. In particular the notion of "rank of a move" does not seem to
> be defined anywhere.

We revised the description of the HBSS.

> 
> As noted above, section 6.1 mentions the fact that plateaus are partly
> due to the way the schedule builder operates. This really seems to be
> a critical issue and alternative approaches to build a schedule should
> be studied. One really has the feeling that the article studies
> different approaches to work-around the deficiencies of the greedy
> algorithm used to build the schedule completely leaving aside the
> possibility to use a different heuristics for the schedule builder.
> 

We tried variants on the positioning strategy and found they either
performed worse, or the improvements were small while the
computational cost increased (a heuristic using the constrainednes of
the resources to decide where to place the requests resulted in a 16
to 20 times increase in the CPU time for the schedule builder).  Given
the need to focus the paper, we did not include those
results. Randomizing the schedule builder also did not seem to help
with performance results, we include a discussion about this in
Section 6.1, third paragraph.  For the purposes of this paper, we
consider both the schedule builder and the objective function to be
part of the problem description; therefore the features of the search
space are considered invariable.

> In section 6.1.1, the explanation of the decrease of the number of
> non-interacting pairs when minimising overlaps makes sense but it does
> not seem to be very much supported by the experiments. Indeed in
> average, for example on optimal permutations, it only decreases from
> 41.73% when minimising conflicts to 40.95% when minimising overlaps
> which only represents a relative decrease of less than 2%!
>

In service of better focusing the paper, section 6.1.1 was taken off.
In 6.1.1 we only looked at the schedules that are identical to the
current one; in the new version, in Table 10, Section 6.4, we count
all the schedules of the same value as the current one -- which
include the identical schedules.

> In table 6 (as well in fact as in table 15), it would be very nice to
> recap the results of the original version of the algorithm to make the
> comparison easier.

These tables are not in the paper anymore

> 
> A small figure to illustrate the example in the end of section 6.1.2
> would be useful. Furthermore, as mentioned, this example is an example
> of "bad" schedule that indeed, can't be reached with the restricted
> shift operator. It would certainly be more informative to exhibit an
> example where a good schedule can be reached using the general shift
> operator but not with the restricted one.

To better focus the paper, section 6.1.2 was also removed.

> 
> In section 6.1.3, it is mentioned but not explained why when
> minimising overlaps, best known solutions have slightly more
> same-value neighbours than do random solutions.

We found that improving moves result in longer walks on lower plateaus,
and the fact that good solutions have more same-valued neighbors than
random solutions supports this idea; we added a note.


> 
> Why is the total number of neighbours in table 7 different from the
> total pairs in table 5?

The total number of neighbors for a shifting neighborhood is (n-1)^2.
The total number of pairs (n(n-1)/2) was part of a study to investigate
pairs of requests which do not interact. The revised version of the
paper does not include this anymore.


> As noted above, the proof that Genitor is learning some patterns
> (split heuristics, precedences) would be much more convincing with an
> analyse of how such patterns tend to become more and more present in
> the population when evolution advances. For instance in the case of
> learning the split heuristics in section 6.3.1, it would be
> interesting to define how much a given individual in the population
> follows the split heuristics (e.g. by measuring the minimal number of
> task request permutations in \pi so that \pi follows the split
> heuristics, of course other measures are possible), and then to
> average this measure over all the population and to look at how this
> average evolves with the population (it should increase). A similar
> study could be done in section 6.3.2 given a set of "common orderings"
> in very good solutions computed beforehand.

We removed section 6.3.1 because we believe it didn't add much to the
content of the paper.  We agree that to check if Genitor is learning
to schedule low before high altitude requests, a study like the one
you suggest would have been more informative.

It seems that only some of the low altitude requests have to be
scheduled before high altitude requests.  We could not figure how to
define in a controlled way tracking common request patterns across
multiple runs of Genitor, as evolution advances.


> In section 6.3.2, the study of the existence of common request
> ordering in good solutions is very biased by the number of solutions
> that are considered. If there wouldn't be any preference ordering
> between tasks in good solutions (so no patterns to learn), taking s
> solutions to a problem with n tasks would give an average number of
> common ordering of the order of (n^2)/(2^s) as for each n^2
> precedences, there is a probability of (1/2)^s that it will be present
> in all s solutions. For the A problems and for R5, where s>700, this
> number of expected common precedences would no pattern be present is
> negligible so the number of actually detected common orderings (around
> 100 low/high pairs, 10 other pairs) really seems to be due to actual
> patterns. For the other R problems this also seems to be the
> case. This bias should be at least mentioned in this section.

Thank you for pointing this out. We included this observation in our paper.

> 
> In section 6.4, I do not find it surprising that the study of section
> 6.3.2 applied to SeededGenitor shows much more common orderings than
> the one of Genitor. SeededGenitor simply produces better solutions
> than Genitor and it is normal that the variability of a set of very
> good solutions is smaller than the one of a set of less good ones.
> 
> The main result after section 6.4 is that (> means is better than);
> SeededGenitor > (Seeded)SWO > RandomSWO > (Random)Genitor. Is there an
> explanation to the fact that starting from greedy permutations instead
> of random ones has more effect on Genitor (RandomGenitor was the worse
> approach, SeededGenitor become the best one) than on SWO?
> 
> In section 6.6, I suppose the reason why SWO and SeededGenitor start
> from a much better solution than RandomNeigh LS and Genitor is because
> they are initialised from greedy permutations. Were some experiments
> performed using Local Search starting from some solution computed with
> the greedy initial permutations of SWO?

Yes, we did, in fact, perform analyses of performance results including
Local Search started from the same greedy initial permutations as SWO.
We do not include the analyses for the Seeded versions of our algorithms
in the new version of the paper. Starting from a greedy initial permutation
results in better performance than starting from a random one; however,
as we show for SWO, it does not seem that initialization is the decisive
performance factor. Instead, it seems that the multiple moves, present both
in Genitor and SWO are driving the performance results. We added new results
to support this (section 6.5.1 and 6.5.2) and implemented a new algorithm (ALLS)
based on our findings (section 7).

> 
> Do the curves on figures 7 and 8 correspond to an average over all R
> problems or a particular R, this doesn't seem to be explicitly
> mentioned.

The curves are for a specific problem; we corrected this by mentioning
it explicitly in our new version of the paper.
