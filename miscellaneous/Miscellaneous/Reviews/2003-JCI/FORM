
PURPOSE OF REVIEW:

The purpose of the special issue is to provide a selection of 
journal-quality articles about new methods for dealing with 
preferences in AI and CP. The goal is to present interesting
results on different aspects on preferences to a large readership.
In order to achieve this purpose, we ask the reviewers to evaluate
submissions according to standard reviewing criteria such as 
relevance, significance, originality, technical quality, and clarity 
of presentation. Please justify you judgments by detailled 
comments and give recommendations for improvements wherever possible. 
Since different methods for handling preferences are regrouped together
in a single issue, it is important that the papers give some general 
motivations for the considered approach and describe which kind of 
preferences they can treat. Finally, we would also like to ask the 
reviewers to check whether the length of the paper is appropriate.


PAPER TITLE: Multi-agent Constraint Systems with Preferences: Efficiency, Solution Quality, and Privacy Loss

AUTHOR(S): M.S. Franzin, E.C. Freuder, F. Rossi, R. Wallace

RELEVANCE AND CLASSIFICATION

Does the paper well fit into a special issue on preferences in AI and CP?

 (X) yes

 ( ) only of limited interest

 ( ) no

Which topics on preferences are covered by the paper?

 - preferences for soft constraints
 - preferences for search and optimization
 - preferences in a multi-agent framework
 - comparison of approaches
 - applications of preferences

....................................................

ORIGINALITY

Does the paper present original results on handling preferences?

 ( ) yes, the paper presents a new theoretical result /
     new algorithm / new empirical results /
     an original application of existing methods /
     an original synthesis or comparison of existing approaches
 
 (X) interesting improvement of existing approaches
 
 ( ) minor improvement of existing approaches

 ( ) not original 

SIGNIFICANCE

Are the problems, approaches, and results of the paper significant for current
work on preferences in AI and CP?

 ( ) very significant

 (X) significant

 ( ) only of minor significance

 ( ) not significant

TECHNICAL QUALITY

Is the considered problem well-defined and the approach well-developed?
Are the results of the paper correctly validated? (Are proofs of theorems correct?
Are algorithms implemented? Are benchmarks for implemented systems reported?)

 ( ) technical quality is very good

 ( ) some minor technical issues need to be clarified

 (X) some major technical issues need to be improved

 ( ) technical quality is insufficient

PRESENTATION

Is the paper well-presented, well-organized and self-contained? Is the length of 
the paper appropriate to the results presented?

 (X) the paper reads fine for a large AI or CP readership

 ( ) the paper reads fine for a specialist, but could be improved
     in order to address a larger readership

 ( ) readability can be improved by minor revisions 

 ( ) major revisions are needed to make the paper comprehensible

 ( ) the paper is not comprehensible 

MOTIVATION 

Does it well motivate the considered approach on preferences and give some 
description of the kinds of preferences it can treat? 

 ( ) yes

 (X) the given motivation can be improved 

 ( ) no sufficient motivation is given

OVERALL RECOMMENDATION

 ( ) Accept with minor revisions

 (X) Accept with major revisions

 ( ) Reject 

DETAILLED COMMENTS AND RECOMMENDATIONS FOR THE AUTHOR 

The  paper proposes a general  framework  for representing and solving
multi-agent   optimization   problems  with   preferences   using  CSP
techniques (namely: common assignment  problem with preferences). This
framework is instantiated on a meeting  scheduling problem where agent
privacy loss  may  be  an issue.  In  this  context,  it  studies  the
compromises between solution quality, efficiency and privacy loss.

General framework.

The general   framework builds on  the CSP   and  the soft  constraint
framework [Bistarelli et  al, 1997] and  is really interesting because
powerful enough   to  represent  a   large   spectrum  of  multi-agent
constraint problems with individual agent preferences.

I think that it may be  interesting to extend  the way preferences are
combined to get a global preference. Generally  - and unlike suggested
in  the  meeting  scheduling  example,  each agent   will have several
private variables   (not only one)   and several soft  constraints. It
would be interesting to have two levels for combining preferences: one
for  each agent that  allows  computing an agent  preference value for
each agent by  combining its  private soft  constraints into a  single
indicator and  one  at the  global level  that  allows aggregating all
agent preferences  into    a single  global   preference  level.   The
combination operators could be different: for instance fuzzy operation
at the agent level and pareto optimality at the global level.

In the introduction,  as well as in section  5, it  is said that  each
agent  maintains  an    "approximation" of   other  agents  constraint
networks.  Is    this really  an   "approximation" or  is  it   not  a
"relaxation".  If it is a general  "approximation", it means that some
(optimal) solutions may be missed.   Furthermore, it  is not clear  if
this approximation  is represented as a  constraint network and if, in
this context, "inference" means constraint  propagation or if it means
something else. In particular  in the meeting scheduling problem, were
CSP techniques used to infer information on this "approximation" ?

Meeting scheduling.

*** The concept of privacy loss is misleading in this example.  In the
description of  the general framework, after definition  1, it is said
that "agents may want to communicate as little information as possible
about their local  constraint problem".  I  completely agree with this
definition of  the privacy issue: loss  of privacy is directly related
with the information that is  communicated to the  other agents but it
is NOT  linked with  what other agents   will do  with  this piece  of
information, in particular whether other agents are intelligent enough
to draw all  the consequences of  what  was communicated to them.   In
this context, I do not understand why,  in section 9.1, inferring more
information  (I would rather    say   being smart  enough  to    infer
"redundant" information) represents a loss  of privacy.  I think  that
the concept of privacy  loss could be  introduced in the general model
as a function of the reduction of the set  of possibilities (the basic
entropic formula)  a "maximally  smart" agent  could perform  given  a
piece of information that is communicated.  This way, the privacy loss
only depends on the piece of  information that are communicated (thus,
on  the   agent communication  protocol) and  not   on  the ability of
individual  agents  to infer  redundant  information.  The compromises
between quality (or efficiency) and  privacy loss can then be explored
by changing the agent strategies and the agent communication protocol.

*** The conclusion drawn on the Pareto  optimization only holds in the
very  restricted  context of the  experiment.   Indeed,  as the Pareto
optimization doesn't consider at all  the fuzzy criterion, it does not
make a  lot of sense to  compare (according to  the fuzzy criterion) a
Pareto solution with  a solution  found  by an  approach that aims  at
minimizing the fuzzy criterion.  For instance  the solution (0.001, 1,
1) may turn out to  be Pareto optimal although the  value of the fuzzy
criterion is very  bad.  So I  think the general  conclusion about the
interest of the Pareto optimization should be changed.

*** In   section 7, the  description of   the strategies for  avoiding
interpersonal  comparison of utility is not  clear.  If each agent has
n+1 ordered preference  values,   it seems to  me  that  the "step-up"
strategy is exactly the same as  the basic strategies but where agents
would communicate a scale of preferences (0, 1/n,  2/n, ..., n-1/n, 1)
instead  of their actual preferences.  This  will  clearly work if the
distortion of the scale of the agent with  respect to the linear scale
above  is  small.  This may  be  the  case  if preferences are uniform
random variables in [0,1] as it seems to be the case in the experiment
but,  again, the  relevance   of these strategies   seems  to  be very
dependent on the problem.

I think  that  the three points  above  should absolutely be clarified
and, depending on the answers, it may  change the way the experimental
study is presented and the general conclusions that  can be drawn from
these experiments.

Some more specific comments:

The system at the URL address given in the paper doesn't work: I tried
it   on several  computers with  different   configurations (a HP  and
several PCs) and the applet  always  throws a  Java exception when  it
opens.

In the end of   section  9.1: how do   you  explain that the    strong
inference (illustrated on   figures 5 and  6) has  few  effect  on the
number of proposals ? On figure 6, I do not catch why privacy loss for
"future meetings"  and  "preferences" is smaller when  using inference
than  with  the baseline  strategy.    Furthermore, the definition  of
privacy loss for preferences does not seem to be defined.

In section  9.2, it would be interesting  to see whether when limiting
to  first solutions, inference leads  to  better results than baseline
strategy and if privacy loss is similar to the one of section 9.1.

Some typos:

p3, paragraph 3, "(I)n many cases"
p5, paragraph 3, unbalanced parenthesis "(That such issues ..."
p11, paragraph 3, "knoweldge"
p13, paragraph 1, "in certain in certain"


----- The following will not be forwarded to the author -----

CONFIDENTIAL COMMENTS FOR THE CO-EDITORS




Reviewer's Name: Philippe Laborie

