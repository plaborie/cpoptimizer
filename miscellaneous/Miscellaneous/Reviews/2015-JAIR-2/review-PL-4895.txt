Title: A new proactive-reactive approach to hedge against uncertain processing times and unexpected machine failures in scheduling problems
Author(s): Donya Rahmani


Overall evaluation
==================

[ ] Accept as is
[ ] Accept with minor revisions
[ ] Reject with resubmission encouraged
[X] Reject


Comments and suggestions
========================

The article describes a proactive-reactive approach to handle a specific non-deterministic scheduling problem: 2-machines flow shop problem with uncertain machine breakdowns and stochastic processing times. The proactive part of the approach consists in generating scenarios and solving an Integer Programming model that minimizes a combination of the expected makespan and a deviation to the expected makespan. The proactive part only handles stochastic processing times and assumes machines never break down. Machine breakdowns are handled in the reactive part: during the execution of the proactive schedule, if a machine breakdown occurs a re-scheduling step is performed that tries to optimize a combination (weighted sum) of different indicators (nervousness, effectiveness, robustness, stability). For that, two competing methods are run (simple right shift and re-optimization with a variant of the initial proactive model) and the schedule with best combined indicator value is selected. The proposed approach is evaluated on a benchmark.

While the style and the English could be improved, the article is is general easy to understand. 

Section 2 presents a literature review that mainly lists a set of articles. It would be useful to provide more structure in this section and in particular do some form of classification of the different approaches. 

The overall proactive-reactive scheduling approach presented in the paper is not really original (as acknowledged in the article in the bibliographical study and in the references) so one expects some new ideas in each of the stages (proactive and reactive) for the particular scheduling problem being addressed. Unfortunately, the article falls short from this point of view:

Proactive stage:

The stochastic IP model is quite straightforward and is a specialization of the Yu and Li model on the 2-machines flow shop. 

For some aspects, this model is not clear. For instance the items indexed by 'r' are called 'orders' while it would be more clear to call them 'positions' and they are indeed called 'positions' in some other sections of the paper. The mention of using only 3 scenarios is also disturbing. You say there are "one pessimistic, one optimistic and one normative" scenario, what does it means formally? How are these scenarios defined? One could think for instance that processing times in the "pessimistic" scenario are all greater or equal to their counterpart in the "normative" and "optimistic" scenario, but this seems to be contradicted by the values on Table 1. Furthermore, I do not see the point of limiting oneself to 3 scenarios here.

Reactive stage:

The two heuristics used in the reactive stage are also quite simple: (1) right shift the operations affected by the machine breakdown (without changing the sequence of operations on the machines) or (2) re-optimize using the model of the proactive stage on the still unperformed operations. And then, use the best of the two schedules according to an indicator value. 

First, although it seems to be clear, I think it should be mentioned that the solution provided by the proactive stage is a sequencing of the operations on the machines (the start/end values of operations is not used).

Then it should be explicitly said that the execution policy for executing the proactive schedule s to schedule the operations as soon as possible on the machines (until a breakdown occurs).

The behavior of the jobs in case of breakdown is also not clearly defined. What happens if a machine break downs while executing a job operation? Does the job need to be restarted from scratch again? Or only the affected operation? Or does it only delay the operation until the machine gets repaired? The example on Figure 2 seems to suggest the last case.

On page 12 (and in several other places later on), you say that "when the production process begins, it is certain which scenario has occurred". I do not get that. Scenarios are only a sampling of what can happen in the reality, how can one know at the beginning of the execution which scenario has occurred? Even at the end of the execution it is very probable that the real values of the uncertain variables (processing times in our case) do not correspond to any scenarios ... 

The NERS indicator value is formulated as a weighted sum of different criteria (formula 25). Does this aggregation makes sense given that the criteria are very heterogeneous? 

Given that you have a formal definition of the NERS criterion to be optimized when rescheduling, a legitimate question is why are you using an heuristic here? Is it not possible to build an optimization model that would find the optimal solution to this criterion? It is very possible that such a criterion is difficult to represent in an Integer Programming model but (1) there exists other optimization frameworks (like Constraint Programming) that are more general than IP and may handle this type of objective function and (2) you are dealing in the study with very small instances (at most 25 jobs) so even a not so efficient IP model could be sufficient.

The problem with the IP model that is run in the reactive stage is that it does not consider at all the NERS indicator objective function itself. And the right shift is just really very naive. Assuming the problem is really too hard to be formulated and solve to optimality by an exact approach and that you indeed need to look for heuristic approaches, then why not to use meta-heuristics like Local Search (or any other) in order to directly minimize the NERS indicator when rescheduling?

In the experimental section, the proposed approach (PRM) is compared with two "Classical approaches": CA-Ri and CA-Re. It is not clear what is the difference between the proposed approach and these classical approaches. I understand that in the classical approach, the IP model only minimizes the expected makespan and not a combination of expected makespan and deviation to the expected makespan (the second term in formula 14). But except for this difference, the PRM approach seems only to take the best solution between CA-Ri and CA-Re. Is it really so?


Some more detailed comments
----------------------------

The scheduling problem studied in this paper is only formally introduced in section 3. It could be better to introduce a formal definition of the problem before the bibliographical study.

In the numerical example in section 3.4, it would be useful to introduce a figure showing the solution of the proactive stage.

In the experimental section in section 4.1, the way the scenarios are generated is weird. Why not selecting for each operation ij of a job a range of processing time [a_ij,b_ij] and randomly select a value in this range for each scenario \lambda ? Because with the current definition, it is not clear what type of phenomenon the scenarios are sampling as the sampled stochastic variable depends on the scenario \lambda through the parameter \eta^\lambda .

There is an overlap between the experiments described in section 4.2 (Table 6) and the ones described on Table 4. Do we really need these two sets of experiments?


Typos:
------

p3, section 2: real-wor[l]d production environments

p5: Ultimately, they solved and analyzed the problem[.] by considering a linear

p7: equations (10) and (11), there is a sum over \lambda [NOT] \in \Omega

p10: equation 14: remove extra parenthesis '('

p20: Because for a more robust solution [it] may be necessary to change [the] sequence of jobs which can
increase [decrease?] stability

p21, section 4.2: sentence is partly repeated : It should be mentioned ... It should be mentioned ...


Summary:
--------

In its present state, the contribution is not original and significant enough to be published in JAIR. 

The overall approach is very classical and there are several shortcomings, both in the proactive and in the reactive stage.




