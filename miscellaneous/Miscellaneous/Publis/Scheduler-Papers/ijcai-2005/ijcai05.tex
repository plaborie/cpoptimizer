\documentclass{article}
\usepackage{ijcai05}
\usepackage{times}
\usepackage{latexsym}

\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage[pdftex]{graphicx}
\usepackage{algorithm}
\usepackage[noend]{algpseudocode}

\newcommand{\subjectto}{\mathrm{subject\ to:}}
\newcommand{\minimize}{\mathrm{minimize}}

\newtheorem{complexity}{Complexity}
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{definition}{Definition}
\def\qed{\hfill \quad \vrule height4.17pt width4.17pt depth0pt} 
\newenvironment{proof}[1][Proof]{\noindent \textbf{#1:} }{\qed}
\newcommand{\maxflow}{\mathrm{maxflow(}n,m\mathrm{)}}
\newcommand{\ixtet}{I{\scriptsize X}T{\scriptsize E}T\hspace*{1mm}}
\def\subsetnoteq{\subsetneq}

\title{Complete MCS-Based Search:\\ Application to Resource Constrained Project Scheduling}

\author{Philippe Laborie \\ ILOG \\ 9, rue de Verdun, 94253 Gentilly Cedex, France \\ plaborie@ilog.fr}

\begin{document}

\maketitle

\begin{abstract} 
This paper describes a simple complete search for cumulative scheduling based on the detection and resolution of {\em minimal critical sets} (MCS). The heuristic for selecting MCSs relies on an estimation of the related reduction of the search space. An extension of the search procedure using {\em self-adapting shaving} is proposed. The approach was implemented on top of classical  constraint  propagation algorithms and tested on resource constrained project
scheduling  problems (RCPSP). We were able  to close more than 15\%    of   the  previously open    problems    of the  PSPLIB
\cite{kolisch-sprecher-96} and improve more than 31\% of the best known lower bounds on those heavily studied problems. Other new results on open-shop and cumulative job-shop scheduling are reported. 

\end{abstract}

\section{Introduction}

The  resource constrained  project  scheduling problem  (RCPSP) is one of the most general scheduling problem that is extensively studied in the literature. It consists in scheduling a project, which is a set of activities linked with precedence constraints, by means of a set of limited resources while minimizing the total duration of the project. The decision variant  of the RCPSP, i.e.,  the  problem of determining
whether  there exists a  feasible project  of makespan  smaller than a
given deadline, is NP-hard  in the strong sense.  The  RCPSP is a very
popular  and frequently  studied  NP-hard optimization  problem and the
last  20   years  have  witnessed  a  tremendous improvement   of both
heuristic and exact solution  procedures (cf.  e.g. the recent surveys
given in \cite{demeulemeester-herroelen-02,hartmann-kolisch-04}).   The currently  best  lower bounds  on the
makespan  for the general RCPSP   are based on  solving  linear programs using
adequate cutting planes \cite{brucker-knust-00,baptiste-demassey-04}. State-of-the-art techniques for upper-bounds rely on meta-heuristics such as Genetic Algorithms, Ant Colony Optimization or Large Neighborhood Search. Many scheduling problems such as job shop, cumulative job shop and open-shop can be modeled as special cases of RCPSP.

In  this article, we  present  a pure constraint  programming approach
based on the  exploration of a complete search  tree to prove that the
project  cannot be achieved  within a given  deadline or to exhibit a
feasible project if one exists.  The search  procedure is based on the
detection and     resolution   of   {\em  minimal    critical    sets} (MCS)
\cite{laborie-ghallab-95} at each node of the search. MCSs are carefully  chosen using a  heuristic that tries  to minimize the size of  the search space. During  the search, strong  constraint
propagation  is  enforced   using  classical   scheduling constraint  propagation
techniques such   as  {\em   time-tabling}, {\em edge-finding}, 
{\em precedence energy} and {\em balance} constraints \cite{laborie-03}.

Next section recap the definition of the resource constrained  project scheduling problem and introduces some notations. Section   \ref{basic-search} describes our basic search  procedure  as well  as the heuristic to
select MCSs. Section \ref{shaving} extends the basic   search procedure to  perform {\em self-adapting shaving}. The last part of the paper consists of experimental results on classical benchmarks (general RCPSP, open-shop and cumulative jobshop problems). For general RCPSP, we show that our approach closes more than 15\% of previously open  instances and improves more than 31\% of best known  lower bounds of the famous PSPLIB instances \cite{kolisch-sprecher-96}. The same approach using exactely the same settings was used to close all the hard open-shop instances of \cite{guere-prins-99} in less than 5s CPU time and to improve the best known lower bounds and close several instances of cumulative jobshop \cite{Nuijten1996}. 

\section{Model and notations} \label{notations}

The resource constrained  project scheduling  problem (RCPSP)  can  be
formally stated as follows. A project is made of a set of activities ${\cal A}$ linked by precedence constraints. Precedence constraints can be represented by a directed acyclic graph $G=({\cal A},{\cal E})$ where each node in ${\cal A}$ represents an activity and each arc $(A,B) \in {\cal E}$ represents a precedence constraint between $A$ and $B$. Let $d(A)$ denote the fixed duration of activity $A \in {\cal A}$ and $s(A)$ (resp. $e(A)$) denote the decision variable representing the start (resp. end) time of activity $A$. A set of discrete capacity resources ${\cal R}$ is considered, each resource $R \in {\cal R}$ having a maximal available capacity $Q(R)$ over the entire scheduling horizon. Each activity $A \in {\cal A}$ requires a non-negative quantity $q(A,R)$ of resource $R$. The problem is to find a feasible instantiation $s$ of the activity start times such that precedence and resource constraints are satisfied and the schedule makespan is minimal. More formally:
\begin{eqnarray}
	\minimize & & \max_{A \in {\cal A}} e(A) \nonumber \\
  \subjectto & &\nonumber \\
  \forall A \in {\cal A}, & &0 \leq s(A)  \nonumber \\
                          & &e(A) = s(A) + d(A) \nonumber\\
  \forall (A,B) \in {\cal E}, & &e(A) \leq s(B) \nonumber\\
  \forall R \in {\cal R}, \forall t \in {\mathbb{Z}^+}, & &\sum_{A \in {\cal S}(t)} q(A,R) \leq Q(R)\nonumber
\end{eqnarray}

where ${\cal S}(t)$ is the set of activities executing at time $t$:

\[{\cal S}(t) = \{ A \in {\cal A}, s(A)\leq t < e(A) \}\]


A resource requirement of activity $A$ on resource $R$ is a triple $u=(A,R,q)$ where $q=q(A,R)>0$. If $u=(A,R,q)$ is a resource requirement, we will denote  $A(u)=A$ the activity of $u$, $R(u)=R$ the required resource, $q(u)=q$ the required quantity,  $s(u)$ (resp. $e(u)$) will 
denote the start (resp. end) time of the activity of $u$.  We    will    also    denote 
$U(R)=\{u / R(u)=R \}$ the set of resource requirements on resource $R$. If   $\varphi \subseteq  U(R)$  is a  subset  of resource   requirements  on a   resource  $R$,  we   will   denote $q(\varphi)=\sum_{u \in \varphi}{q(u)}$ the global resource consumption of $R$ by activities in $\varphi$.

\section{Search}
\label{basic-search}

\subsection{Branching scheme}

Our branching scheme assumes that a temporal network representing the relations between the time-points of all activities (start and end) using the point algebra of \cite{vilain-86} is maintained during the search. In our implementation, this is ensured by the precedence graph constraint of {\sc ILOG Scheduler} \cite{scheduler61}.  We denote $\{\emptyset, \prec, \preceq, =, \succ, \succeq, \neq, ?  \}$  the set of qualitative relations between  time points. If $u$ and $v$ represent two resource requirements, we will denote $u \preceq v$ if and only if $e(u) \preceq s(v)$.

The branching scheme relies on the notion of minimal critical sets (MCS) and
their resolvers as  introduced in \cite{laborie-ghallab-95}. A MCS is  a minimal set of
resource requirements on the same resource $R$ that could be executed simultaneously and would, in this case, over-consume  resource $R$. MCSs are a natural generalization to cumulative scheduling of the pairs of activities conflicting for the same unary resource in disjunctive scheduling.

\begin{definition}[Minimal critical set]  
A minimal critical set on  a  resource $R$  is a subset $\phi\subseteq U(R),$ such that:
\begin{enumerate}
\item $Q(R)<q(\phi)$
\item $\forall\varphi\subsetneq\phi,q(\varphi)\leq Q(R)$
\item $\bigwedge_{(u,v)\in\phi\times\phi}s(u) \prec e(v)$
is consistent with the current temporal network
\end{enumerate}
\end{definition}

Informally,   the  different ways to  resolve  a  minimal critical set
consist   in posting a  precedence  constraint between any  two of its
activities.

\begin{definition}[Resolvers of a minimal critical set] 
If $\phi\subseteq   U(R)$ is a MCS,   we call
resolvers   of $\phi$  the set  of  temporal constraints
$Res(\phi)=\{u\preceq v: (u,v)\in\phi^2,u\neq v \}$.
\end{definition}

As  described  in  \cite{laborie-ghallab-95},   the set  of  resolvers
$Res(\phi)$ of a MCS  $\phi$ can be simplified  so  as to remove  those
resolvers  $\rho\in Res(\phi)$ for which there exists another
resolver $\rho'\in Res(\phi)$ such  that $\rho\Rightarrow\rho'$ given the
current temporal  network.  Indeed, in  such case, the resolver $\rho$
is redundant.  Such  a  simplification procedure  can be   achieved  in
$O(k^{3})$ if $k$ is the  size of the  MCS using the naive Algorithm \ref{simplify}. Line \ref{impl} allows removing resolver $u\preceq v$ if there exists $w$ such that $u\preceq v \Rightarrow u\preceq w$ or $u\preceq v \Rightarrow w\preceq v$. In what follows, we assume that the set of resolvers of a MCS has been simplified.

\small
\begin{algorithm}
\caption{Resolver simplification algorithm} \label{simplify}
\begin{algorithmic}[1]
\Procedure{SIMPLIFY\_RESOLVERS}{$\phi$}
\State $Res(\phi) \leftarrow \emptyset$
\ForAll{$u$ {\bf in} $\phi$}
\ForAll{$v$ {\bf in} $\phi \setminus \{u\}$}
\State $keep\_uv \leftarrow$ TRUE
\ForAll{$w$ {\bf in} $\phi \setminus \{u,v\}$}
\If{$s(v) \preceq s(w)$ {\bf or} $e(w) \preceq e(u)$ } \label{impl}
\State $keep\_uv \leftarrow$ FALSE
\State {\bf break}
\EndIf
\EndFor
\If{$keep\_uv$}
\State $Res(\phi)\leftarrow Res(\phi) \cup(u\preceq v)$
\EndIf
\EndFor
\EndFor
\State {\bf return} $Res(\phi)$
\EndProcedure
\end{algorithmic}
\end{algorithm}
\normalsize

At each search node, our branching scheme consists in selecting a MCS $\phi$ and branching on its possible resolvers in the children nodes until there is no more MCS. This approach is clearly complete.

\subsection{Heuristics}\label{heuristics}

As all the  resolvers consist of temporal constraints of
the form $x \preceq  y$ where $x$  and $y$ are  two time-points (start  or end of an activity), we are interested in  an estimation of the size of the search space after posting  such a precedence constraint. The 
fraction of the  search  space that is  preserved  when     adding a precedence
constraint is estimated using the complementary of the commitment measure
introduced in \cite{laborie-03}.

Let  $x$ and $y$  be two  time-points with  respective lower and upper
bound  for time value:  $[x_{min},x_{max}]$ and $[y_{min},y_{max}]$. The size of
the search  space is estimated by  the Cartesian product of the domain
of  the  two   variables,  that  is,  the    area  of the    rectangle
$[x_{min},x_{max}],[y_{min},y_{max}]$.  The  size of the search space that
is preserved when adding the constraint $x\preceq y$ is  the part of that
rectangle above the line $x=y$ as illustrated in Figure \ref{fig1}. The fraction of the search space that is preserved can thus be estimated
as follows. Let:
\[A=(y_{max}-y{}_{min}+1)(x_{max}-x_{min}+1)\]
\[B=(y_{max}-x_{min}+1)(y_{max}-x_{min}+2)\]
\[C_{min}=\max(0, (y_{min}-x_{min})(y_{min}-x_{min}+1))\]
\[C_{max}=\max(0, (y_{max}-x_{max})(y_{max}-x_{max}+1))\]

The fraction is then equal to: 

\[preserved(x\preceq y)=\frac{B- C_{min} - C_{max}}{2\cdot A}\]

In the example of Figure \ref{fig1}, this gives $A=30$, $B=56$, $C_{min}=6$, $C_{max}=2$, and $preserved(x\preceq y)=24/30$.

\begin{figure}
\centerline{\includegraphics[width=10pc]{figures/fig-preserved.pdf}}
\caption{Preserved search space when adding $x\preceq y$}
\label{fig1}
\end{figure}

If $\omega$ is the  size of the search  space below the current search
node, the size of the search space after posting a temporal constraint
$x\preceq y$ can be estimated by $\omega~\cdot~preserved(x\preceq y)$. If $\phi$ is the  MCS that is selected to  be resolved at the  current
search node, the size  of the search  space to explore below the current node can
thus be estimated as the sum of the sizes of the  search space below each child
node,               that             is:        $\omega \cdot \sum_{\rho\in
Res(\phi)}preserved(\rho)$. Therefore,    $preserved(\phi)=\sum_{\rho\in
Res(\phi)}preserved(\rho)$  estimates  the fraction of the search  space that is
preserved when choosing $\phi$ as the next MCS to solve\footnote{Note
that  this is of  course only a rough estimate  and in particular, the
estimated fraction  $preserved(\phi)$  can  be greater  than  1.}. Our heuristic  simply   chooses  to  resolve  next  the  MCS $\phi$  that
minimizes $preserved(\phi)$ that is, the one that minimizes our estimation of the size of the explored search space. Next section provides more details about the MCS selection algorithm.  

\subsection{MCS selection algorithm}
\label{mcs-algo}

At each search node, the MCS selection procedure develops
a tree of  partial MCSs where the  current partial MCS $\phi$ is extended  adding one
resource requirement in each child node.  By definition of
$preserved(\phi)$,    it  is clear  that  $\phi'\subset\phi\Rightarrow
preserved(\phi')\leq  preserved(\phi)$.    Thus, if $\phi^{*}$  is the
best MCS found so far, once a partial MCS $\phi$ has been reached such
that $preserved(\phi^{*})\leq preserved(\phi)$, the sub-tree of the MCS
selection tree  rooted at $\phi$ can  be abandoned.  

The algorithm for  selecting and branching on a  MCS is more precisely
described  in Algorithm \ref{selectmcs} using the following  notations:  $id(u)$ is a  unique
index associated with resource   requirement $u$ used to  break  ties.
$Unranked(u)$  represents   all the  resource  requirements $v$ that  can
possibly overlap $u$ given the current temporal constraints, and that are 
such that $q(v)<q(u)$ or $q(v)=q(u)$ and $id(v)<id(u)$. $\phi$ is
the (partial) MCS that is currently being extended. A (partial) MCS  is represented by a list
of resource   requirements: $\phi=(u_1,...,u_k)$.  We  denote  $u_k  =
Last(\phi)$ and define the operator  $\oplus$ as follows: $\phi \oplus
u=(u_1,...,u_k,u)$; $q=q(\phi)$  is the consumption   of the current
(partial)  MCS; $p=preserved(\phi)$ is the   preserved search space so
far in the current (partial) MCS; $\phi^*$ is  the best MCS so far and
$p^*=preserved(\phi^*)$ is the preserved  search space of the best MCS
so far. The  procedure at line \ref{selectmcs0}  calls  the  MCS  rating and
selection process on each resource. At line    \ref{selectmcs1}, to rate 
and select MCSs for a given resource, the procedure first sorts the relevant sets of
requirements  $v$ at  lines \ref{sort1} and \ref{sort2}  by decreasing order of
$q(v$), using $id(v)$ to break ties in order to ensure that each MCS  is scanned only
once,  starting with the  smallest MCSs, that  is, the ones containing
the most greedy requirements. The procedure at line \ref{unranked} returns TRUE if and only if a given
requirement $u$ can possibly overlap all the requirements of a partial
MCS given the current temporal network. The procedure at line \ref{deltapreserved}  computes  the incremental
increase of preserved space due to  the insertion of a new requirement
$u$ in  the current  MCS $\phi$.   The  value of $preserved$  has been
described in section \ref{heuristics}. The main recursive function for selecting MCSs is described at line \ref{recselectmcs}.

\small
\begin{algorithm}
\caption{MCS selection algorithm}\label{selectmcs}
\begin{algorithmic}[1]

\Procedure{SELECT\_MCS}{}\label{selectmcs0}
\State $p^* \leftarrow +\infty$
\For{$R$ {\bf in} $\cal{R}$}
\State SELECT\_MCS($R$)
\EndFor
\State {\bf return} $\phi^*$
\EndProcedure
\Statex

\Procedure{SELECT\_MCS}{$R$}\label{selectmcs1}
\State Sort $U(R)$ \label{sort1} by decreasing $q$
\ForAll{$u$ {\bf in} $U(R)$}
\State Sort $Unranked(u)$ by decreasing $q$ \label{sort2}
\EndFor
\For{$u$ {\bf in} $U(R)$}
\State RSELECT\_MCS($R$,$(u)$,$q(u)$,$0$)
\EndFor
\EndProcedure
\Statex

\Procedure{RSELECT\_MCS}{$R$,$\phi$,$q$,$p$}\label{recselectmcs}
\If{$q>Q(R)$} \Comment{$\phi$ is a MCS}
\If{$p<p^*$} \Comment{$\phi$ is the best MCS so far}
\State $p^*$ $\leftarrow$ $p$
\State $\phi^*$ $\leftarrow$ $\phi$
\EndIf
\Else \Comment{$\phi$ needs to be extended}
\State $u$ $\leftarrow$ Last($\phi$)
\ForAll{$v$ {\bf in} Unranked($u$)}
\If{IS\_UNRANKED($v$,$\phi$)}
\State $dp$ $\leftarrow$ DELTA\_PRESERVED($v$,$\phi$)
\State RSELECT\_MCS($R$,$\phi \oplus v$,$q+q(v)$,$p+dp$)
\EndIf
\EndFor
\EndIf
\EndProcedure
\Statex

\Procedure{IS\_UNRANKED}{$u$,$\phi$} \label{unranked}
\ForAll{$v$ {\bf in} $\phi$}
\If{$u \preceq v$ {\bf or} $v \preceq u$}
\State {\bf return} FALSE
\EndIf
\EndFor
\State {\bf return} TRUE
\EndProcedure
\Statex

\Procedure{DELTA\_PRESERVED}{$u$,$\phi$}\label{deltapreserved}
\State $dp$ $\leftarrow$ $0$
\ForAll{$v$ {\bf in} $\phi$}
\State $dp$ $\leftarrow$ $dp$ + preserved($u$,$v$) + preserved($v$,$u$)
\EndFor
\State {\bf return} $dp$
\EndProcedure
\end{algorithmic}
\end{algorithm}
\normalsize

The best MCS $\phi^{*}$ that  has been scanned  by the above procedure
is selected as the one  to be solved  at the current search node. This
MCS is simplified using Algorithm \ref{simplify} and the search explores all of its resolvers $\rho\in
Res(\phi^{*})$ in  the    child   nodes   by   decreasing   order   of
$preserved(\rho)$. This order has no effect when the schedule is not feasible as in this case the complete search tree needs to be explored but it helps finding a solution quicker when a solution exists. 

\section{Self-adapting shaving}
\label{shaving}

Shaving techniques \cite{torres-lopez-00} provide a good framework for strengthening constraint propagation and avoiding late failures to be discovered in the search tree. They are all based on the following principle: 
if adding a constraint $C$ in the current node of the search leads to a failure of 
the propagation, then, constraint $\neg C$ can be inferred. Due to the cost of propagating 
a constraint $C$ and the potential number of constraints $C$ to try to shave on, 
shaving techniques are in general computationally expensive.  

To improve the pruning of the search tree, we implemented the following shaving technique based on MCSs. If a MCS $\phi$ 
with resolvers $Res(\phi)=\{\rho_1,...,\rho_k,\rho_{k+1}\}$ is such that 
$\forall i \in [1..k]$, adding $\rho_i$ in the current schedule leads to a failure 
of the propagation, then $\rho_{k+1}$ can be inferred. The complexity for shaving a given 
MCS $\phi$ of size $n$ is thus in $O(n^2P)$ where $P$ is the cost 
of full constraint propagation at the current node. Potentially, there is of course an exponential number of MCSs to shave on at 
each search node and we can expect that many of those MCS do not allow inferring any precedence constraint. An idea to speed-up the shaving process 
is thus to only try shaving on a subset of MCSs for which the probability to 
infer a precedence constraint is greater than a given threshold $\alpha$. 
Parameter $\alpha$ is an input of the shaving algorithm. We can roughly estimate that the probability that adding a precedence constraint $x \preceq y$ in 
the current schedule will lead to a failure of the propagation is proportional\footnote{Note that this estimation is exact at the extreme points when 
$preserved(x \preceq y)=0$ (propagation will fail for sure) and when 
$preserved(x \preceq y)=1$ (propagation cannot fail because $x \preceq y$ has already been discovered given the current domains of $x$ and $y$).} to 
$1-preserved(x \preceq y)$. If $\rho_{m}=\textrm{argmax}_{\rho \in Res(\phi)} preserved(\rho)$ is the resolver of the 
MCS $\phi$ with maximal preserved search space, we are interested in the MCSs 
that get a high probability that all their resolvers but $\rho_{m}$ will fail, 
that is, if we assume all the probabilities are independent, the ones such that 
$\Pi_{\rho \in Res(\phi)\setminus\{\rho_m\}}(1-preserved(\rho))$ is greater than 
a given threshold. For those MCSs, if the threshold is close enough to 1, we can assume that 
$preserved(\rho)$ is small enough so that the first order approximation 
$\Pi_{\rho \in Res(\phi)\setminus\{\rho_m\}}(1-preserved(\rho)) \approx 1 - \sum_{\rho \in Res(\phi)\setminus\{\rho_m\}} preserved(\rho)$ is reasonable. 

To summarize, we thus only consider for shaving those MCS scanned by the procedure 
described in Algorithm \ref{selectmcs} that are such that 
$preserved(\phi)-preserved(\rho_{m}) \leq \beta$, $\beta$ being a threshold.  
The computation of this criterion only adds a negligible overhead related with 
the maintenance of $\rho_{m}$ for each MCS in the MCS selection procedure. Due to the numerous approximations, $\beta$ is not taken to be constant (theoretically equal to $1-\alpha$). The threshold $\beta$ is computed by self-adaptation in such a way that on average, among the last $h$ shaving attempts, $\alpha h$ lead to 
the inference of a new precedence. Whenever the number of successful shaving $s$ among 
the last $h$ ones deviates from $\alpha h$, the parameter $\beta$ is adapted accordingly: 
if $s < \alpha h$, $\beta$ is decreased by $\epsilon\beta$ and if  $s > \alpha h$, $\beta$ is increased by $\epsilon\beta$. For all our experiments with shaving, we took $h=20$, $\alpha=0.75$, $\epsilon = 0.01$ and start with $\beta=1$.

\section{Experimental evaluation}
\label{experiments-1}

The  approach  has  been implemented   on top of   {\sc ILOG Scheduler}  6.1
 using the {\em timetable}, {\em disjunctive}, {\em
edge-finder},    {\em   precedence   energy}    and    {\em   balance}
constraints \cite{scheduler61}. All the experiments described in this section were run on a Dell Latitude D600 laptop, 1.4 GHz. Detailed results such as individual lower bounds for each problem instance and new optimal solutions are available at {\tt http://scheduler.ilog.fr/}.

\subsection{Results on general RCPSP}

We evaluated our approach on the instances of the PSPLIB \cite{kolisch-sprecher-96} with 60, 90 and 
120 activities (resp. sets J60, J90, J120). For each instance, we solve 
the feasibility problem of finding a schedule with a makespan lower than $T$, 
starting with a legal lower bound for $T$\footnote{For instance the lower-bound of the PERT of temporal constraints} and incrementing $T$ until either the problem is shown to be feasible (in this case, $T$ is the optimal makespan) or a given time limit for solving the problem with makespan $T$ is exceeded (in this case, $T$ is a legal lower-bound but the search stops without providing any legal upper-bound). 

In a first series of experiments, we use the basic search described in section \ref{basic-search} without shaving with a time limit of $300s$. The previous best lower and upper bounds we compare with are the ones reported in the PSPLIB together with the recent improvements on the J60 instances reported in \cite{baptiste-demassey-04}. The results are summarized on Table \ref{tab:Results1} with the following columns:

\begin{table}[h]
\begin{tabular}{l@{}cp{7cm}}
{\bf \#O} &:& number of instances previously open \\
{\bf \#I} &:& number of improved lower bounds (\% of \#O) \\
{\bf AGR} &:& average gap (distance from the lower to the upper bound) reduction when a bound is improved \\
{\bf \#C} &:& number of closed instances (\% of \#O) 
\end{tabular}
\end{table}
\vspace*{-3mm}
\begin{table}[h]
	\centering
    \scriptsize
		\begin{tabular}{|l|r|rr|r|rr|} \hline
			Inst. & \#O   & \#I &(\%I)    & AGR    & \#C &(\%C)   \\ \hline \hline
			J60		&  98		&  39 &(39.8\%) & 62.9\% & 21  &(21.4\%) \\ \hline
			J90		& 129		&  51 &(39.5\%) & 58.4\% & 26  &(20.2\%) \\ \hline
			J120	& 390		&  88 &(22.6\%) & 47.0\% & 38  &(9.7\%) \\ \hline
			ALL   & 617   & 178 &(28.8\%) & 53.0\% & 85  &(13.8\%) \\ \hline
		\end{tabular}
	\caption{Results on RCPSP without self-adapting shaving with a time-limit of 300s}
	\label{tab:Results1}
\end{table}
\normalsize

Out of the $617$ previously open instances, we are able to improve $178$ lower-bounds with 
an average gap reduction of $53\%$ and to close $85$ instances. 

\begin{table}[h]
	\centering
    \scriptsize
		\begin{tabular}{|l|r|rr|r|rr|} \hline
			Inst. & \#O   & \#I &(\%I)    & AGR    & \#C &(\%C)   \\ \hline \hline
			J60		&  98		& 40  &(40.8\%) & 62.6\% & 22 &(22.4\%) \\ \hline
			J90		& 129		& 52  &(40.3\%) & 58.3\% & 26 &(20.2\%) \\ \hline
			J120	& 390		& 90  &(23.1\%) & 47.3\% & 38 &(9.7\%)  \\ \hline
			ALL   & 617   & 182 &(29.5\%) & 53.1\% & 86 &(13.9\%) \\ \hline
		\end{tabular}
	\caption{Results on RCPSP with a self-adapting shaving and a time-limit of 300s}
	\label{tab:Results3}
\end{table}
\normalsize

To show the effect of self-adapting shaving, we run a version of our approach using self-adapting shaving with the same time-limit of 300s. The results are summarized on Table \ref{tab:Results3}. Out of the $617$ previously open instances, we are able to improve $182$ lower-bounds with an average gap reduction of $53.1\%$ and to close $86$ instances. The main conclusion is that, within the same time-limit, the addition of self-adapting shaving slightly increases the performances. The two curves below respectively show, on a particular instance ({\tt J60\_5\_2}), the number of search nodes with a given depth in the search tree and, for each node depth, the ratio between the number of selected MCSs that effectively lead to the inference of a new precedence and the total number of MCSs selected for shaving. One clearly see that in the region of the search space where most of the nodes are concentrated (between depths 10 and 35), this ratio is effectively close to the target ratio of 0.75. In this instance, 4667 nodes where explored, 4843 MCSs where selected for shaving and among them, 3291 effectively lead to the inference of a new precedence.

% INSERT CURVES HERE
% GNUPLOT: LaTeX picture
\setlength{\unitlength}{0.240900pt}
\ifx\plotpoint\undefined\newsavebox{\plotpoint}\fi
\begin{picture}(900,450)(0,0)
\font\gnuplot=cmr10 at 10pt
\gnuplot
\sbox{\plotpoint}{\rule[-0.200pt]{0.400pt}{0.400pt}}%
\put(120.0,123.0){\rule[-0.200pt]{4.818pt}{0.400pt}}
\put(100,123){\makebox(0,0)[r]{0}}
\put(819.0,123.0){\rule[-0.200pt]{4.818pt}{0.400pt}}
\put(120.0,164.0){\rule[-0.200pt]{4.818pt}{0.400pt}}
\put(100,164){\makebox(0,0)[r]{50}}
\put(819.0,164.0){\rule[-0.200pt]{4.818pt}{0.400pt}}
\put(120.0,205.0){\rule[-0.200pt]{4.818pt}{0.400pt}}
\put(100,205){\makebox(0,0)[r]{100}}
\put(819.0,205.0){\rule[-0.200pt]{4.818pt}{0.400pt}}
\put(120.0,246.0){\rule[-0.200pt]{4.818pt}{0.400pt}}
\put(100,246){\makebox(0,0)[r]{150}}
\put(819.0,246.0){\rule[-0.200pt]{4.818pt}{0.400pt}}
\put(120.0,287.0){\rule[-0.200pt]{4.818pt}{0.400pt}}
\put(100,287){\makebox(0,0)[r]{200}}
\put(819.0,287.0){\rule[-0.200pt]{4.818pt}{0.400pt}}
\put(120.0,328.0){\rule[-0.200pt]{4.818pt}{0.400pt}}
\put(100,328){\makebox(0,0)[r]{250}}
\put(819.0,328.0){\rule[-0.200pt]{4.818pt}{0.400pt}}
\put(120.0,369.0){\rule[-0.200pt]{4.818pt}{0.400pt}}
\put(100,369){\makebox(0,0)[r]{300}}
\put(819.0,369.0){\rule[-0.200pt]{4.818pt}{0.400pt}}
\put(120.0,410.0){\rule[-0.200pt]{4.818pt}{0.400pt}}
\put(100,410){\makebox(0,0)[r]{350}}
\put(819.0,410.0){\rule[-0.200pt]{4.818pt}{0.400pt}}
\put(120.0,123.0){\rule[-0.200pt]{0.400pt}{4.818pt}}
\put(120,82){\makebox(0,0){0}}
\put(120.0,390.0){\rule[-0.200pt]{0.400pt}{4.818pt}}
\put(210.0,123.0){\rule[-0.200pt]{0.400pt}{4.818pt}}
\put(210,82){\makebox(0,0){5}}
\put(210.0,390.0){\rule[-0.200pt]{0.400pt}{4.818pt}}
\put(300.0,123.0){\rule[-0.200pt]{0.400pt}{4.818pt}}
\put(300,82){\makebox(0,0){10}}
\put(300.0,390.0){\rule[-0.200pt]{0.400pt}{4.818pt}}
\put(390.0,123.0){\rule[-0.200pt]{0.400pt}{4.818pt}}
\put(390,82){\makebox(0,0){15}}
\put(390.0,390.0){\rule[-0.200pt]{0.400pt}{4.818pt}}
\put(480.0,123.0){\rule[-0.200pt]{0.400pt}{4.818pt}}
\put(480,82){\makebox(0,0){20}}
\put(480.0,390.0){\rule[-0.200pt]{0.400pt}{4.818pt}}
\put(569.0,123.0){\rule[-0.200pt]{0.400pt}{4.818pt}}
\put(569,82){\makebox(0,0){25}}
\put(569.0,390.0){\rule[-0.200pt]{0.400pt}{4.818pt}}
\put(659.0,123.0){\rule[-0.200pt]{0.400pt}{4.818pt}}
\put(659,82){\makebox(0,0){30}}
\put(659.0,390.0){\rule[-0.200pt]{0.400pt}{4.818pt}}
\put(749.0,123.0){\rule[-0.200pt]{0.400pt}{4.818pt}}
\put(749,82){\makebox(0,0){35}}
\put(749.0,390.0){\rule[-0.200pt]{0.400pt}{4.818pt}}
\put(839.0,123.0){\rule[-0.200pt]{0.400pt}{4.818pt}}
\put(839,82){\makebox(0,0){40}}
\put(839.0,390.0){\rule[-0.200pt]{0.400pt}{4.818pt}}
\put(120.0,123.0){\rule[-0.200pt]{173.207pt}{0.400pt}}
\put(839.0,123.0){\rule[-0.200pt]{0.400pt}{69.138pt}}
\put(120.0,410.0){\rule[-0.200pt]{173.207pt}{0.400pt}}
\put(479,21){\makebox(0,0){Search depth}}
\put(120.0,123.0){\rule[-0.200pt]{0.400pt}{69.138pt}}
\put(280,164){\makebox(0,0)[l]{Number of nodes}}
\put(160.0,164.0){\rule[-0.200pt]{24.090pt}{0.400pt}}
\put(120,124){\usebox{\plotpoint}}
\put(120,123.67){\rule{4.336pt}{0.400pt}}
\multiput(120.00,123.17)(9.000,1.000){2}{\rule{2.168pt}{0.400pt}}
\put(138,124.67){\rule{4.336pt}{0.400pt}}
\multiput(138.00,124.17)(9.000,1.000){2}{\rule{2.168pt}{0.400pt}}
\multiput(156.00,126.61)(3.811,0.447){3}{\rule{2.500pt}{0.108pt}}
\multiput(156.00,125.17)(12.811,3.000){2}{\rule{1.250pt}{0.400pt}}
\multiput(174.00,129.60)(2.528,0.468){5}{\rule{1.900pt}{0.113pt}}
\multiput(174.00,128.17)(14.056,4.000){2}{\rule{0.950pt}{0.400pt}}
\multiput(192.00,133.59)(1.575,0.482){9}{\rule{1.300pt}{0.116pt}}
\multiput(192.00,132.17)(15.302,6.000){2}{\rule{0.650pt}{0.400pt}}
\multiput(210.00,139.60)(2.528,0.468){5}{\rule{1.900pt}{0.113pt}}
\multiput(210.00,138.17)(14.056,4.000){2}{\rule{0.950pt}{0.400pt}}
\multiput(228.00,143.61)(3.811,0.447){3}{\rule{2.500pt}{0.108pt}}
\multiput(228.00,142.17)(12.811,3.000){2}{\rule{1.250pt}{0.400pt}}
\multiput(246.00,146.58)(0.912,0.491){17}{\rule{0.820pt}{0.118pt}}
\multiput(246.00,145.17)(16.298,10.000){2}{\rule{0.410pt}{0.400pt}}
\multiput(264.00,156.58)(0.644,0.494){25}{\rule{0.614pt}{0.119pt}}
\multiput(264.00,155.17)(16.725,14.000){2}{\rule{0.307pt}{0.400pt}}
\multiput(282.00,170.59)(1.154,0.488){13}{\rule{1.000pt}{0.117pt}}
\multiput(282.00,169.17)(15.924,8.000){2}{\rule{0.500pt}{0.400pt}}
\multiput(300.00,176.95)(3.811,-0.447){3}{\rule{2.500pt}{0.108pt}}
\multiput(300.00,177.17)(12.811,-3.000){2}{\rule{1.250pt}{0.400pt}}
\multiput(318.00,175.58)(0.826,0.492){19}{\rule{0.755pt}{0.118pt}}
\multiput(318.00,174.17)(16.434,11.000){2}{\rule{0.377pt}{0.400pt}}
\multiput(336.58,186.00)(0.495,0.895){33}{\rule{0.119pt}{0.811pt}}
\multiput(335.17,186.00)(18.000,30.316){2}{\rule{0.400pt}{0.406pt}}
\multiput(354.58,218.00)(0.495,0.895){33}{\rule{0.119pt}{0.811pt}}
\multiput(353.17,218.00)(18.000,30.316){2}{\rule{0.400pt}{0.406pt}}
\multiput(372.00,250.59)(1.575,0.482){9}{\rule{1.300pt}{0.116pt}}
\multiput(372.00,249.17)(15.302,6.000){2}{\rule{0.650pt}{0.400pt}}
\multiput(390.00,254.94)(2.528,-0.468){5}{\rule{1.900pt}{0.113pt}}
\multiput(390.00,255.17)(14.056,-4.000){2}{\rule{0.950pt}{0.400pt}}
\multiput(408.00,250.92)(0.528,-0.495){31}{\rule{0.524pt}{0.119pt}}
\multiput(408.00,251.17)(16.913,-17.000){2}{\rule{0.262pt}{0.400pt}}
\multiput(426.00,235.58)(0.600,0.494){27}{\rule{0.580pt}{0.119pt}}
\multiput(426.00,234.17)(16.796,15.000){2}{\rule{0.290pt}{0.400pt}}
\multiput(444.00,250.58)(0.561,0.494){29}{\rule{0.550pt}{0.119pt}}
\multiput(444.00,249.17)(16.858,16.000){2}{\rule{0.275pt}{0.400pt}}
\multiput(462.58,266.00)(0.495,0.810){33}{\rule{0.119pt}{0.744pt}}
\multiput(461.17,266.00)(18.000,27.455){2}{\rule{0.400pt}{0.372pt}}
\multiput(480.58,295.00)(0.495,1.039){31}{\rule{0.119pt}{0.924pt}}
\multiput(479.17,295.00)(17.000,33.083){2}{\rule{0.400pt}{0.462pt}}
\multiput(497.00,330.58)(0.644,0.494){25}{\rule{0.614pt}{0.119pt}}
\multiput(497.00,329.17)(16.725,14.000){2}{\rule{0.307pt}{0.400pt}}
\multiput(515.00,344.58)(0.498,0.495){33}{\rule{0.500pt}{0.119pt}}
\multiput(515.00,343.17)(16.962,18.000){2}{\rule{0.250pt}{0.400pt}}
\multiput(533.00,362.59)(1.575,0.482){9}{\rule{1.300pt}{0.116pt}}
\multiput(533.00,361.17)(15.302,6.000){2}{\rule{0.650pt}{0.400pt}}
\multiput(551.00,368.61)(3.811,0.447){3}{\rule{2.500pt}{0.108pt}}
\multiput(551.00,367.17)(12.811,3.000){2}{\rule{1.250pt}{0.400pt}}
\multiput(569.00,371.61)(3.811,0.447){3}{\rule{2.500pt}{0.108pt}}
\multiput(569.00,370.17)(12.811,3.000){2}{\rule{1.250pt}{0.400pt}}
\put(587,372.17){\rule{3.700pt}{0.400pt}}
\multiput(587.00,373.17)(10.320,-2.000){2}{\rule{1.850pt}{0.400pt}}
\multiput(605.00,370.92)(0.498,-0.495){33}{\rule{0.500pt}{0.119pt}}
\multiput(605.00,371.17)(16.962,-18.000){2}{\rule{0.250pt}{0.400pt}}
\multiput(623.58,349.53)(0.495,-1.235){33}{\rule{0.119pt}{1.078pt}}
\multiput(622.17,351.76)(18.000,-41.763){2}{\rule{0.400pt}{0.539pt}}
\multiput(641.58,304.14)(0.495,-1.661){33}{\rule{0.119pt}{1.411pt}}
\multiput(640.17,307.07)(18.000,-56.071){2}{\rule{0.400pt}{0.706pt}}
\multiput(659.58,247.26)(0.495,-1.008){33}{\rule{0.119pt}{0.900pt}}
\multiput(658.17,249.13)(18.000,-34.132){2}{\rule{0.400pt}{0.450pt}}
\multiput(677.58,210.62)(0.495,-1.207){33}{\rule{0.119pt}{1.056pt}}
\multiput(676.17,212.81)(18.000,-40.809){2}{\rule{0.400pt}{0.528pt}}
\multiput(695.58,169.19)(0.495,-0.725){33}{\rule{0.119pt}{0.678pt}}
\multiput(694.17,170.59)(18.000,-24.593){2}{\rule{0.400pt}{0.339pt}}
\multiput(713.00,144.93)(1.935,-0.477){7}{\rule{1.540pt}{0.115pt}}
\multiput(713.00,145.17)(14.804,-5.000){2}{\rule{0.770pt}{0.400pt}}
\multiput(749.00,139.95)(3.811,-0.447){3}{\rule{2.500pt}{0.108pt}}
\multiput(749.00,140.17)(12.811,-3.000){2}{\rule{1.250pt}{0.400pt}}
\put(767,137.67){\rule{4.336pt}{0.400pt}}
\multiput(767.00,137.17)(9.000,1.000){2}{\rule{2.168pt}{0.400pt}}
\multiput(785.00,137.95)(3.811,-0.447){3}{\rule{2.500pt}{0.108pt}}
\multiput(785.00,138.17)(12.811,-3.000){2}{\rule{1.250pt}{0.400pt}}
\multiput(803.00,134.93)(1.575,-0.482){9}{\rule{1.300pt}{0.116pt}}
\multiput(803.00,135.17)(15.302,-6.000){2}{\rule{0.650pt}{0.400pt}}
\multiput(821.00,128.93)(1.935,-0.477){7}{\rule{1.540pt}{0.115pt}}
\multiput(821.00,129.17)(14.804,-5.000){2}{\rule{0.770pt}{0.400pt}}
\put(731.0,141.0){\rule[-0.200pt]{4.336pt}{0.400pt}}
\end{picture}


% GNUPLOT: LaTeX picture
\setlength{\unitlength}{0.240900pt}
\ifx\plotpoint\undefined\newsavebox{\plotpoint}\fi
\sbox{\plotpoint}{\rule[-0.200pt]{0.400pt}{0.400pt}}%
\begin{picture}(900,450)(0,0)
\font\gnuplot=cmr10 at 10pt
\gnuplot
\sbox{\plotpoint}{\rule[-0.200pt]{0.400pt}{0.400pt}}%
\put(120.0,123.0){\rule[-0.200pt]{4.818pt}{0.400pt}}
\put(100,123){\makebox(0,0)[r]{0}}
\put(819.0,123.0){\rule[-0.200pt]{4.818pt}{0.400pt}}
\put(120.0,180.0){\rule[-0.200pt]{4.818pt}{0.400pt}}
\put(100,180){\makebox(0,0)[r]{0.2}}
\put(819.0,180.0){\rule[-0.200pt]{4.818pt}{0.400pt}}
\put(120.0,238.0){\rule[-0.200pt]{4.818pt}{0.400pt}}
\put(100,238){\makebox(0,0)[r]{0.4}}
\put(819.0,238.0){\rule[-0.200pt]{4.818pt}{0.400pt}}
\put(120.0,295.0){\rule[-0.200pt]{4.818pt}{0.400pt}}
\put(100,295){\makebox(0,0)[r]{0.6}}
\put(819.0,295.0){\rule[-0.200pt]{4.818pt}{0.400pt}}
\put(120.0,353.0){\rule[-0.200pt]{4.818pt}{0.400pt}}
\put(100,353){\makebox(0,0)[r]{0.8}}
\put(819.0,353.0){\rule[-0.200pt]{4.818pt}{0.400pt}}
\put(120.0,410.0){\rule[-0.200pt]{4.818pt}{0.400pt}}
\put(100,410){\makebox(0,0)[r]{1}}
\put(819.0,410.0){\rule[-0.200pt]{4.818pt}{0.400pt}}
\put(120.0,123.0){\rule[-0.200pt]{0.400pt}{4.818pt}}
\put(120,82){\makebox(0,0){0}}
\put(120.0,390.0){\rule[-0.200pt]{0.400pt}{4.818pt}}
\put(210.0,123.0){\rule[-0.200pt]{0.400pt}{4.818pt}}
\put(210,82){\makebox(0,0){5}}
\put(210.0,390.0){\rule[-0.200pt]{0.400pt}{4.818pt}}
\put(300.0,123.0){\rule[-0.200pt]{0.400pt}{4.818pt}}
\put(300,82){\makebox(0,0){10}}
\put(300.0,390.0){\rule[-0.200pt]{0.400pt}{4.818pt}}
\put(390.0,123.0){\rule[-0.200pt]{0.400pt}{4.818pt}}
\put(390,82){\makebox(0,0){15}}
\put(390.0,390.0){\rule[-0.200pt]{0.400pt}{4.818pt}}
\put(480.0,123.0){\rule[-0.200pt]{0.400pt}{4.818pt}}
\put(480,82){\makebox(0,0){20}}
\put(480.0,390.0){\rule[-0.200pt]{0.400pt}{4.818pt}}
\put(569.0,123.0){\rule[-0.200pt]{0.400pt}{4.818pt}}
\put(569,82){\makebox(0,0){25}}
\put(569.0,390.0){\rule[-0.200pt]{0.400pt}{4.818pt}}
\put(659.0,123.0){\rule[-0.200pt]{0.400pt}{4.818pt}}
\put(659,82){\makebox(0,0){30}}
\put(659.0,390.0){\rule[-0.200pt]{0.400pt}{4.818pt}}
\put(749.0,123.0){\rule[-0.200pt]{0.400pt}{4.818pt}}
\put(749,82){\makebox(0,0){35}}
\put(749.0,390.0){\rule[-0.200pt]{0.400pt}{4.818pt}}
\put(839.0,123.0){\rule[-0.200pt]{0.400pt}{4.818pt}}
\put(839,82){\makebox(0,0){40}}
\put(839.0,390.0){\rule[-0.200pt]{0.400pt}{4.818pt}}
\put(120.0,123.0){\rule[-0.200pt]{173.207pt}{0.400pt}}
\put(839.0,123.0){\rule[-0.200pt]{0.400pt}{69.138pt}}
\put(120.0,410.0){\rule[-0.200pt]{173.207pt}{0.400pt}}
\put(479,21){\makebox(0,0){Search depth}}
\put(120.0,123.0){\rule[-0.200pt]{0.400pt}{69.138pt}}
\put(679,205){\makebox(0,0)[r]{Actual shaving ratio}}
\put(699.0,205.0){\rule[-0.200pt]{24.090pt}{0.400pt}}
\put(120,123){\usebox{\plotpoint}}
\multiput(156.00,123.58)(0.498,0.495){33}{\rule{0.500pt}{0.119pt}}
\multiput(156.00,122.17)(16.962,18.000){2}{\rule{0.250pt}{0.400pt}}
\multiput(174.58,141.00)(0.495,1.576){33}{\rule{0.119pt}{1.344pt}}
\multiput(173.17,141.00)(18.000,53.210){2}{\rule{0.400pt}{0.672pt}}
\multiput(192.00,197.59)(1.332,0.485){11}{\rule{1.129pt}{0.117pt}}
\multiput(192.00,196.17)(15.658,7.000){2}{\rule{0.564pt}{0.400pt}}
\multiput(210.58,204.00)(0.495,2.001){33}{\rule{0.119pt}{1.678pt}}
\multiput(209.17,204.00)(18.000,67.518){2}{\rule{0.400pt}{0.839pt}}
\multiput(228.00,273.92)(0.695,-0.493){23}{\rule{0.654pt}{0.119pt}}
\multiput(228.00,274.17)(16.643,-13.000){2}{\rule{0.327pt}{0.400pt}}
\multiput(246.00,262.59)(1.332,0.485){11}{\rule{1.129pt}{0.117pt}}
\multiput(246.00,261.17)(15.658,7.000){2}{\rule{0.564pt}{0.400pt}}
\multiput(264.00,269.58)(0.912,0.491){17}{\rule{0.820pt}{0.118pt}}
\multiput(264.00,268.17)(16.298,10.000){2}{\rule{0.410pt}{0.400pt}}
\multiput(282.58,279.00)(0.495,1.037){33}{\rule{0.119pt}{0.922pt}}
\multiput(281.17,279.00)(18.000,35.086){2}{\rule{0.400pt}{0.461pt}}
\multiput(300.58,313.83)(0.495,-0.526){33}{\rule{0.119pt}{0.522pt}}
\multiput(299.17,314.92)(18.000,-17.916){2}{\rule{0.400pt}{0.261pt}}
\multiput(318.00,297.60)(2.528,0.468){5}{\rule{1.900pt}{0.113pt}}
\multiput(318.00,296.17)(14.056,4.000){2}{\rule{0.950pt}{0.400pt}}
\multiput(336.00,301.58)(0.600,0.494){27}{\rule{0.580pt}{0.119pt}}
\multiput(336.00,300.17)(16.796,15.000){2}{\rule{0.290pt}{0.400pt}}
\multiput(354.00,316.59)(1.019,0.489){15}{\rule{0.900pt}{0.118pt}}
\multiput(354.00,315.17)(16.132,9.000){2}{\rule{0.450pt}{0.400pt}}
\put(372,324.67){\rule{4.336pt}{0.400pt}}
\multiput(372.00,324.17)(9.000,1.000){2}{\rule{2.168pt}{0.400pt}}
\multiput(390.58,326.00)(0.495,0.583){33}{\rule{0.119pt}{0.567pt}}
\multiput(389.17,326.00)(18.000,19.824){2}{\rule{0.400pt}{0.283pt}}
\multiput(408.58,344.74)(0.495,-0.554){33}{\rule{0.119pt}{0.544pt}}
\multiput(407.17,345.87)(18.000,-18.870){2}{\rule{0.400pt}{0.272pt}}
\put(426,326.67){\rule{4.336pt}{0.400pt}}
\multiput(426.00,326.17)(9.000,1.000){2}{\rule{2.168pt}{0.400pt}}
\multiput(444.00,326.93)(1.154,-0.488){13}{\rule{1.000pt}{0.117pt}}
\multiput(444.00,327.17)(15.924,-8.000){2}{\rule{0.500pt}{0.400pt}}
\multiput(462.00,318.92)(0.498,-0.495){33}{\rule{0.500pt}{0.119pt}}
\multiput(462.00,319.17)(16.962,-18.000){2}{\rule{0.250pt}{0.400pt}}
\multiput(480.00,302.59)(1.088,0.488){13}{\rule{0.950pt}{0.117pt}}
\multiput(480.00,301.17)(15.028,8.000){2}{\rule{0.475pt}{0.400pt}}
\multiput(497.00,308.95)(3.811,-0.447){3}{\rule{2.500pt}{0.108pt}}
\multiput(497.00,309.17)(12.811,-3.000){2}{\rule{1.250pt}{0.400pt}}
\multiput(515.58,304.65)(0.495,-0.583){33}{\rule{0.119pt}{0.567pt}}
\multiput(514.17,305.82)(18.000,-19.824){2}{\rule{0.400pt}{0.283pt}}
\multiput(533.58,286.00)(0.495,1.122){33}{\rule{0.119pt}{0.989pt}}
\multiput(532.17,286.00)(18.000,37.948){2}{\rule{0.400pt}{0.494pt}}
\multiput(551.00,324.93)(1.935,-0.477){7}{\rule{1.540pt}{0.115pt}}
\multiput(551.00,325.17)(14.804,-5.000){2}{\rule{0.770pt}{0.400pt}}
\multiput(569.00,321.58)(0.600,0.494){27}{\rule{0.580pt}{0.119pt}}
\multiput(569.00,320.17)(16.796,15.000){2}{\rule{0.290pt}{0.400pt}}
\multiput(587.00,334.92)(0.826,-0.492){19}{\rule{0.755pt}{0.118pt}}
\multiput(587.00,335.17)(16.434,-11.000){2}{\rule{0.377pt}{0.400pt}}
\multiput(605.58,325.00)(0.495,0.611){33}{\rule{0.119pt}{0.589pt}}
\multiput(604.17,325.00)(18.000,20.778){2}{\rule{0.400pt}{0.294pt}}
\multiput(623.58,347.00)(0.495,0.583){33}{\rule{0.119pt}{0.567pt}}
\multiput(622.17,347.00)(18.000,19.824){2}{\rule{0.400pt}{0.283pt}}
\multiput(641.00,366.92)(0.644,-0.494){25}{\rule{0.614pt}{0.119pt}}
\multiput(641.00,367.17)(16.725,-14.000){2}{\rule{0.307pt}{0.400pt}}
\multiput(659.58,354.00)(0.495,0.952){33}{\rule{0.119pt}{0.856pt}}
\multiput(658.17,354.00)(18.000,32.224){2}{\rule{0.400pt}{0.428pt}}
\multiput(677.00,388.59)(1.154,0.488){13}{\rule{1.000pt}{0.117pt}}
\multiput(677.00,387.17)(15.924,8.000){2}{\rule{0.500pt}{0.400pt}}
\multiput(695.00,396.58)(0.644,0.494){25}{\rule{0.614pt}{0.119pt}}
\multiput(695.00,395.17)(16.725,14.000){2}{\rule{0.307pt}{0.400pt}}
\put(120.0,123.0){\rule[-0.200pt]{8.672pt}{0.400pt}}
\multiput(731.58,406.63)(0.495,-0.895){33}{\rule{0.119pt}{0.811pt}}
\multiput(730.17,408.32)(18.000,-30.316){2}{\rule{0.400pt}{0.406pt}}
\multiput(749.58,371.68)(0.495,-1.803){33}{\rule{0.119pt}{1.522pt}}
\multiput(748.17,374.84)(18.000,-60.841){2}{\rule{0.400pt}{0.761pt}}
\multiput(767.58,314.00)(0.495,1.547){33}{\rule{0.119pt}{1.322pt}}
\multiput(766.17,314.00)(18.000,52.256){2}{\rule{0.400pt}{0.661pt}}
\multiput(785.58,369.00)(0.495,1.150){33}{\rule{0.119pt}{1.011pt}}
\multiput(784.17,369.00)(18.000,38.901){2}{\rule{0.400pt}{0.506pt}}
\put(713.0,410.0){\rule[-0.200pt]{4.336pt}{0.400pt}}
\put(803.0,410.0){\rule[-0.200pt]{8.672pt}{0.400pt}}
\put(679,164){\makebox(0,0)[r]{Target shaving ratio}}
\multiput(699,164)(20.756,0.000){5}{\usebox{\plotpoint}}
\put(799,164){\usebox{\plotpoint}}
\put(120,338){\usebox{\plotpoint}}
\put(120.00,338.00){\usebox{\plotpoint}}
\put(140.76,338.00){\usebox{\plotpoint}}
\put(161.51,338.00){\usebox{\plotpoint}}
\put(182.27,338.00){\usebox{\plotpoint}}
\put(203.02,338.00){\usebox{\plotpoint}}
\put(223.78,338.00){\usebox{\plotpoint}}
\put(244.53,338.00){\usebox{\plotpoint}}
\put(265.29,338.00){\usebox{\plotpoint}}
\put(286.04,338.00){\usebox{\plotpoint}}
\put(306.80,338.00){\usebox{\plotpoint}}
\put(327.55,338.00){\usebox{\plotpoint}}
\put(348.31,338.00){\usebox{\plotpoint}}
\put(369.07,338.00){\usebox{\plotpoint}}
\put(389.82,338.00){\usebox{\plotpoint}}
\put(410.58,338.00){\usebox{\plotpoint}}
\put(431.33,338.00){\usebox{\plotpoint}}
\put(452.09,338.00){\usebox{\plotpoint}}
\put(472.84,338.00){\usebox{\plotpoint}}
\put(493.60,338.00){\usebox{\plotpoint}}
\put(514.35,338.00){\usebox{\plotpoint}}
\put(535.11,338.00){\usebox{\plotpoint}}
\put(555.87,338.00){\usebox{\plotpoint}}
\put(576.62,338.00){\usebox{\plotpoint}}
\put(597.38,338.00){\usebox{\plotpoint}}
\put(618.13,338.00){\usebox{\plotpoint}}
\put(638.89,338.00){\usebox{\plotpoint}}
\put(659.64,338.00){\usebox{\plotpoint}}
\put(680.40,338.00){\usebox{\plotpoint}}
\put(701.15,338.00){\usebox{\plotpoint}}
\put(721.91,338.00){\usebox{\plotpoint}}
\put(742.66,338.00){\usebox{\plotpoint}}
\put(763.42,338.00){\usebox{\plotpoint}}
\put(784.18,338.00){\usebox{\plotpoint}}
\put(804.93,338.00){\usebox{\plotpoint}}
\put(825.69,338.00){\usebox{\plotpoint}}
\put(839,338){\usebox{\plotpoint}}
\end{picture}

Given that self-adapting shaving slightly improves the performances within the same time limit of 300s, we used this configuration with an extended time-limit of 1800s. The results are summarized on Table \ref{tab:Results4}. Out of the $617$ previously open instances, we improve $193$ lower-bounds  (that is more than $31\%$ of the previously open instances) with an average gap reduction of $61.1\%$ and close $96$ instances (that is more than 15\% of the previously open instances). 

\begin{table}
	\centering
    \scriptsize
		\begin{tabular}{|l|r|rr|r|rr|} \hline
			Inst. & \#O   & \#I &(\%I)    & AGR    & \#C &(\%C)   \\ \hline \hline
			J60		&  98		& 44  &(44.9\%) & 72.4\% & 26  &(26.5\%) \\ \hline
			J90		& 129		& 54  &(41.9\%) & 66.0\% & 30  &(23.3\%) \\ \hline
			J120	& 390		& 95  &(24.4\%) & 48.1\% & 40  &(10.3\%) \\ \hline
			ALL   & 617   & 193 &(31.3\%) & 61.1\% & 96  &(15.6\%) \\ \hline
		\end{tabular}
	\caption{Results on RCPSP with a self-adapting shaving and a time-limit of 1800s}
	\label{tab:Results4}
\end{table}
\normalsize

\subsection{Results on open-shop problems}

Open-shop problems can be represented as a special cases of RCPSP where all resources have a unit capacity and additional unary resources are used to model the fact that activities of the same job cannot not overlap. We tested our approach, with the same settings as in previous section on the open-shop problems instances proposed in \cite{guere-prins-99}. Those instances are considered to be very hard instances of open-shop problems and serve as classical benchmark in open-shop scheduling (see for instance recent work in \cite{blum-05}). The benchmark consists of 80 instances ranging from $\textrm{3 jobs}\times\textrm{3 machines}$ problems until $\textrm{10 jobs}\times\textrm{10 machines}$ problems. Out of these 80 problems, 34 instances are still open. Using our approach, we were able to close all those instances in less than 5s CPU time. The optimal makespan for the 34 previously open instances is summarized on Table \ref{tab:ossp} where the column $UB$ corresponds to the currently best known upper-bound for which no optimality proof did exist.

\begin{table}
	\centering
    \scriptsize
		\begin{tabular}{|l|r|r||l|r|r|} \hline
			Instance & UB   & Optim.      & Instance & UB   & Optim.      \\ \hline \hline
			gp06-03  & 1255 & {\bf 1255*} & gp09-02  & 1112 & {\bf 1110*} \\ \hline
			gp06-07  & 1290 & {\bf 1290*} & gp09-03  & 1117 & {\bf 1115*} \\ \hline
			gp06-09  & 1243 & {\bf 1243*} & gp09-06  & 1093 & {\bf 1093*} \\ \hline
			gp06-10  & 1254 & {\bf 1254*} & gp09-07  & 1097 & {\bf 1090*} \\ \hline
			gp07-01  & 1159 & {\bf 1159*} & gp09-08  & 1106 & {\bf 1105*} \\ \hline
			gp07-02  & 1185 & {\bf 1185*} & gp09-09  & 1126 & {\bf 1123*} \\ \hline
			gp07-04  & 1167 & {\bf 1167*} & gp09-10  & 1120 & {\bf 1110*} \\ \hline
			gp07-06  & 1193 & {\bf 1193*} & gp10-01  & 1099 & {\bf 1093*} \\ \hline
			gp07-08  & 1180 & {\bf 1180*} & gp10-02  & 1099 & {\bf 1097*} \\ \hline
			gp07-09  & 1220 & {\bf 1220*} & gp10-03  & 1081 & {\bf 1081*} \\ \hline
			gp08-02  & 1135 & {\bf 1135*} & gp10-04  & 1089 & {\bf 1077*} \\ \hline
			gp08-04  & 1154 & {\bf 1153*} & gp10-05  & 1080 & {\bf 1071*} \\ \hline
			gp08-06  & 1116 & {\bf 1115*} & gp10-06  & 1072 & {\bf 1071*} \\ \hline
			gp08-07  & 1126 & {\bf 1126*} & gp10-07  & 1081 & {\bf 1079*} \\ \hline
			gp08-08  & 1148 & {\bf 1148*} & gp10-08  & 1098 & {\bf 1093*} \\ \hline
			gp08-10  & 1161 & {\bf 1161*} & gp10-09  & 1120 & {\bf 1112*} \\ \hline
			gp09-01  & 1135 & {\bf 1129*} & gp10-10  & 1092 & {\bf 1092*} \\ \hline
		\end{tabular}
	\caption{Results on open-shop with a self-adapting shaving and a time-limit of 5s}
	\label{tab:ossp}
\end{table}
\normalsize

We also experimented with the open instances of the benchmark of \cite{brucker-97}. We closed 3 of these 6 open instances: namely {\tt j8-per0-2} (optimal makespan: 1052), {\tt j8-per10-0} (optimal makespan 1017) and {\tt j8-per10-1} (optimal makespan 1000).

\subsection{Results on cumulative jobshop problems}

We tested our approach, with the same settings, on the cumulative job-shop problem benchmark described in \cite{Nuijten1996}. These instances are derived from classical jobshop scheduling problems by multiplying the number of jobs (and thus the number of activities) and the capacity of the resources by a given factor ($\times 2$ or $\times 3$). Our results are summarized on Table \ref{tab:cumulative} where $LB$ is the lower bound using the consistency checking described in \cite{Nuijten1996} and $New LB$ the new lower bound of our approach. We were able to close the {\tt ft06$\times$2} and {\tt ft06$\times$3} instances as well as to improve 12 lower bounds out of these 38 open instances.

\begin{table}
	\centering
    \scriptsize
		\begin{tabular}{|l|r|r||l|r|r|} \hline
			Instance      & LB   & New LB    & Instance      & LB   & New LB     \\ \hline \hline
			ft06$\times$2 & 53   & {\bf 55*} & ft06$\times$3 & 53   & {\bf 55*}  \\ \hline
			ft10$\times$2 & 835  & {\bf 837} & ft10$\times$3 & 828  & 828        \\ \hline
			la03$\times$2 & 593  & 593       & la03$\times$3 & 590  & 590        \\ \hline
			la04$\times$2 & 572  & 572       & la04$\times$3 & 570  & 570        \\ \hline
			la16$\times$2 & 888  & {\bf 892} & la16$\times$3 & 884  & {\bf 887}  \\ \hline
			la17$\times$2 & 754  & 754       & la17$\times$3 & 753  & 753        \\ \hline
			la18$\times$2 & 783  & {\bf 803} & la18$\times$3 & 776  & {\bf 783}  \\ \hline
			la19$\times$2 & 731  & {\bf 756} & la19$\times$3 & 724  & {\bf 740}  \\ \hline
			la20$\times$2 & 830  & {\bf 849} & la20$\times$3 & 829  & {\bf 842}  \\ \hline
			la21$\times$2 & 1017 & 1017      & la21$\times$3 & 1010 & {\bf 1012} \\ \hline
			la22$\times$2 & 913  & 913       & la22$\times$3 & 913  &  913       \\ \hline
			la24$\times$2 & 885  & 885       & la24$\times$3 & 884  & 884        \\ \hline
			la25$\times$2 & 907  & 907       & la25$\times$3 & 903  & 903        \\ \hline
			la29$\times$2 & 1117 & 1117      & la29$\times$3 & 1116 & 1116       \\ \hline
			la36$\times$2 & 1229 & 1229      & la36$\times$3 & 1227 & 1227       \\ \hline
			la37$\times$2 & 1378 & 1378      & la37$\times$3 & 1370 & 1370       \\ \hline
			la38$\times$2 & 1092 & 1092      & la38$\times$3 & 1087 & 1087       \\ \hline
			la39$\times$2 & 1221 & 1221      & la39$\times$3 & 1221 & 1221       \\ \hline
			la40$\times$2 & 1180 & 1180      & la40$\times$3 & 1176 & 1176       \\ \hline
		\end{tabular}
	\caption{Results on cumulative job-shop with a self-adapting shaving and a time-limit of 1800s}
	\label{tab:cumulative}
\end{table}
\normalsize

\section{Conclusions}

We presented a simple complete search
procedure implemented   on  top  of classical  constraint  propagation
algorithms and applied it to resource constrained project
scheduling  problems. In average, this approach outperforms the best algorithms for 
finding lower bounds on those scheduling problems, even with a time limit of 
300s per optimization step\footnote{When this time limit is exceeded at 
an optimization step, usually, the previous steps where fairly quick so 
that the overall time for computing the lower bound is close to 300s.}.  
Using this  approach in conjunction with a self-adapting shaving procedure,  we were able  
to close more than 15\%    of   the  previously open    problems    
of the  PSPLIB and improve more than 31\% of the best known lower
bounds. What is even more remarkable is that this very same approach allows closing all the hard open-shop instances of \cite{guere-prins-99} in less than 5s CPU time although the approach was not particularly designed to tackle disjunctive scheduling and does not exploit the open-shop nature of the problems.

The understanding of why our method works so well on the instances 
of the PSPLIB and on many open-shop problems would deserve a deeper study. From one hand, if the problems are highly cumulative, our approach 
is clearly limited by the explosion of the number of MCSs to consider. From 
the other hand, when problems are highly disjunctive, we could expect 
other approaches dedicated to disjunctive scheduling to work better. A 
first possible explanation could be a good fit between our approach and 
the "disjunctivity" degree of the hard instances of the PSPLIB as suggested by some recent work \cite{garaix-05}. A result of this study could be some kind of hybridizing of MCS-based search with techniques more adapted to highly cumulative problems, MCS-based search being restricted to the resolution of MCSs with small preserved search space (thus small MCSs) at the top of the search tree. A second direction for future work is the generalization of the notion  of {\em self-adapting} shaving introduced in this paper to other shaving techniques in scheduling.

\bibliography{ijcai05}
\bibliographystyle{named}

\end{document}
