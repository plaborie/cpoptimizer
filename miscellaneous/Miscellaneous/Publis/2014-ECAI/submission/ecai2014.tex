\documentclass{ecai2014}
\usepackage{times}
%\usepackage{graphicx}
\usepackage{latexsym}
\usepackage{nicefrac}
\usepackage{helvet}
\usepackage{courier}
\usepackage{amsmath,amssymb}
\usepackage[pdftex]{graphicx}
\usepackage{algorithm}
%\usepackage[noend]{algpseudocode}
\usepackage{algpseudocode}
\usepackage[usenames]{color}
\usepackage{theomac}

\newcommand{\Z}{\mathbb{Z}}

\newtheorem{definition}{Definition}
\newtheoremWithMacro{property}{Property}
\def\qed{\hfill \quad \vrule height4.17pt width4.17pt depth0pt} 
\newenvironment{proof}[1][Proof]{\noindent \textbf{#1:} }{\qed}

% Macro for colored "todo" notes:
\definecolor{NoteColor}{rgb}{1, 0.2, 0.2}
\newcommand{\note}[1]{\textcolor{NoteColor}{#1}}

% Macros for math functions:
\DeclareMathOperator{\pos}{pos}
\DeclareMathOperator{\idx}{index}

\newcommand{\algn}{\text{\it algn}}

%%\ecaisubmission   % inserts page numbers. Use only for submission of paper.
                  % Do NOT use for camera-ready version of paper.

\begin{document}

\title{An Optimal Iterative Algorithm for Extracting MUCs\\ in a Black-box Constraint Network}

\author{Philippe Laborie\institute{IBM Software Group,
France, email: laborie@fr.ibm.com} }

\maketitle
\bibliographystyle{ecai2014}

\begin{abstract}
We present a non-intrusive iterative algorithm for extracting Minimal Unsatisfiable Cores in black-box constraint networks. The problem can be generalized as the one of finding a minimal subset satisfying an upward-closed property ${\cal P}$. The proposed algorithm, ADEL, integrates several ingredients (Acceleration, Dichotomy, Estimation of distance between consecutive elements, Lazy checks) that are motivated by a probabilistic study on the position of elements in the selected minimal subset. If performance is measured as the number of infeasibility property checks, we show that the proposed approach is optimal both for small and for large MUCs and that it consistently outperforms existing approaches.
\end{abstract}

\section{Introduction}

When a Constraint Satisfaction Problem is infeasible, providing a minimal explanation for infeasibility in the form of a minimal subset of constraints that are mutually contradictory (known as Minimal Unsatisfiable Core or MUC) helps identifying the causes of the infeasibility \cite{Chinneck2007}. Many existing approaches for MUC extraction exploit the particular features of the problem or of the resolution engine. For instance techniques such as {\em elastic filters} can be used for Linear Programming models \cite{Chinneck1997}, {\em explanations} recording \cite{Jussien2002} for Constraint Programming models or {\em no-goods} learning for Boolean Satisfiability models \cite{MarquesSilva1996}. 

In this paper, we tackle the problem of MUC extraction in a {\em non-intrusive} way with respect to the resolution engine by considering that checking for the (in)feasibility of a subset of constraints is a {\em black-box} operation. Advances of the state-of-the-art in Optimization result in increasingly sophisticated and efficient engines. Engines sophistication makes it harder to implement intrusive methods to compute MUCs whereas increase of engines efficiency makes infeasibility property checks faster. Both aspects tend to make the black-box approach attractive.

The problem studied in this paper is thus more general than MUC extraction and can be defined as follows: given a finite set $U$ and a property ${\cal P}$ on the subsets of $U$ that is upward-closed (that is, whenever it holds for a subset $X$ it also holds for any superset of $X$), find a {\em minimal subset} of $U$ that satisfies property ${\cal P}$. For MUC extraction the property ${\cal P}(X)$ is the {\em infeasibility} of a subset of constraints $X$, it clearly is upward-closed as the superset of any infeasible subset of constraints is infeasible too. Beside MUC extraction, this problem occurs in several other fields for instance in diagnosis (find a minimal subset of faulty components of a system that explains the current observations) or in non-monotonic logics (find a minimal subset of abnormalities in the clauses that restore admissibility). In general checking property ${\cal P}(X)$ for a particular subset is an expensive operation: it is NP-complete in the case of MUC extraction, for other applications it could for instance be the outcome of a complex simulation process based on a set of input events $X$. The complexity of the approach is measured by the number of property checks performed by the algorithm in order to find a minimal subset.

The works closest to ours are the {\tt QuickXplain} recursive algorithm \cite{Junker2004} developed in the context of the extraction of MUCs for constraint programming using a black-box approach and the iterative dichotomy algorithm {\tt DC} proposed in \cite{Hemery2006}.

After introducing some notations and formally defining the problem, we present some probabilistic results about the relative positions of elements of the selected minimal subset. This study highlights a couple of interesting properties that are exploited in the sequel of the paper to introduce step by step the proposed algorithm {\tt ADEL} (standing for the four ingredients of the approach: Dichotomy, Acceleration, Estimation and Lazy checks). We show in the last section that the complexity of this algorithm is optimal both for small and large minimal subsets and that it outperforms both the {\tt QuickXplain} and the {\tt DC} algorithms. Proofs of properties are not provided in the article for space reason.

% ===============================================================
% PROBLEM DEFINITION AND NOTATIONS
% ===============================================================

\section{Problem Definition and Notations}

Let $U$ be a finite set of cardinality $n$ and ${\cal P}$ an {\em upward-closed} property on its powerset $2^U$ that is, a property such that:
\[ \big( X \subseteq Y \subseteq U \big) \land {\cal P}(X) \Rightarrow {\cal P}(Y) \]

\begin{definition}[Minimal Subset]
\label{minsubset}
A subset $X \subseteq U$ is said to be {\bf minimal} if and only if:
${\cal P}(X) \land \forall Y \subset X: \neg {\cal P}(Y)$. Let ${\cal M}$ denote the set of all minimal subsets of $U$.
\end{definition}

In the sequel of this paper we assume ${\cal P}(U)$, that is there exist at least one minimal subset. Of course, in general there is not a unique minimal subset: there can be an exponential number of such subsets, for instance the set of minimal subsets could be all subsets of cardinality $n/2$ which is lower bounded by \smash{$2^{n/2}$}.

Without loss of generality we assume a total order $\prec$ over the elements in $U$ so that the elements can be indexed: $U=(u_1,...,u_n)$ with $u_i \prec u_{i+1}$. 

The total order $\prec$ over $U$ implies a lexicographic total order $\prec$ over the powerset \smash{$2^U$}. Let $X,Y \subseteq U, X \neq Y$:
\begin{eqnarray*}
X \prec Y \Leftrightarrow \exists j \in [1,n]: \left \lbrace
\begin{array}{l}
\forall i<j, (u_i \in X) \Leftrightarrow (u_i \in Y) \\
u_j \in X \\
u_j \notin Y
\end{array}
\right.
\end{eqnarray*}

We denote $X^*$ the unique minimal subset $X^* \in {\cal M}$ that is maximal with respect to the total order $\prec$. 

The problem studied in this paper is the design of efficient algorithms to compute minimal subset $X^*$ without any knowledge about property ${\cal P}$ beside the assumption that it is upward-closed. We estimate the complexity of an algorithm to compute a minimal subset as the number of property checks it performs. Note that for comparing the different algorithms, we sometimes use in this paper a more fine-grain measure than the traditional {\em big O} comparison because we count the number of property checks which is an homogeneous measure for all the approaches. That's why we use a comparison {\em on the order of}: $f(n) \sim g(n)$ meaning $\lim_{n\rightarrow+\infty} \big( f(n)/g(n) \big) = 1$. This allows constant factors to be taken into account.

Note that in practice, relation $\prec$ can be used to express preferences over minimal subsets, in this case, the algorithms presented in this paper will compute a preferred minimal subset. The definition of relation $\prec$ is the same as in \cite{Junker2004}.

Let $X\subseteq U$ and $m=|X|$. Without loss of generality, we can sort the $m$ elements of $X$ according to their index in $U$: \smash{$X = \{ u_{\pi(X,j)} \}_{j \in [1,m]}$} with \smash{$1 \leq \pi(X,j) < \pi(X,j+1) \leq n$}. Integer \smash{$\pi(X,j)$} denotes the position in $U$ of the \smash{$j^{th}$} element of subset $X$.

Figure \ref{fig:notations} illustrates a set $U$, its minimal subsets for a property ${\cal P}$ ordered by relation $\prec$, subset $X^*$ and the distances between consecutive elements in $X^*$. 

\begin{figure}[ht]
\centering
\includegraphics[width=6cm,keepaspectratio,clip]{notations.pdf}
\caption{Minimal subsets}
\label{fig:notations}
\end{figure}

For $1 \leq i \leq n$, \smash{$U_{i\rightarrow}$} denotes the subset \smash{$\{u_j | i \leq j \leq n \}$} of all $u_j$ with index $j$ greater than or equal to $i$.

% ===============================================================
% PROBABILISTIC STUDY
% ===============================================================

\section{Probabilistic study}

In this section, we provide some analytical and experimental results about the distribution of the distances \smash{$\pi(X^*,j+1)-\pi(X^*,j)$} between consecutive elements of minimal subset $X^*$ in $U$ under some assumptions. The main conclusions of this study will be used to design efficient algorithms for computing $X^*$.

\subsection{Unique minimal subset}

We assume $U$ contains a unique minimal subset of size $m$ with $0 < m \leq n$ uniformly distributed in $U$. Property \ref{prop:unique_subset_distance} gives the probability distributions of the distances between consecutive elements of the unique minimal subset in $U$. 

\begin{property}[\propertyUniqueSubsetDistance]
\label{prop:unique_subset_distance} In the case of a unique minimal subset of size $m$, the probability that the distance $\pi(X^*,j+1)-\pi(X^*,j)$ between two consecutive elements is $k$ does not depend on $j$ and is equal to 
\[p(k)=\binom{n-k}{m-1}/\binom{n}{m}\]
\end{property}

From the formula, probability $p(k)$ can be computed recursively as follows:
\begin{itemize}
\item $p(1)=m/n$
\item $\forall k \in [2,n-m+1], p(k+1)= \big( 1 - \frac{m-1}{n-k} \big) \  p(k)$
\item $\forall k > n-m+1, p(k) = 0$
\end{itemize}

The probability distribution $p(k)$ is illustrated on figure \ref{fig:unique_subset_distance} for $n=20$ and $m=4$. This probability exponentially decreases with $k$. This result can be seen as a discrete version of the probability density of the distance $x$ between two neighbor points from a set of points randomly and uniformly spread on a line which is \smash{$p(x)=\frac{1}{d}\ e^{-x / d}$} where $d$ is the mean distance between two neighbor points \cite{demaret1977}.

\begin{figure}[ht]
\centering
\includegraphics[width=7.5cm,keepaspectratio,clip]{unique_subset_distance.pdf}
\caption{Distance probability between two consecutive elements}
\label{fig:unique_subset_distance}
\end{figure}

\subsection{Disjoint minimal subsets of random size}

We assume $U$ contains $c$ uniformly distributed disjoint minimal subsets of random sizes $m_{i}$, $i \in [1,c]$ with $0 < \sum_{i \in [1,c] } m_i \leq n$. The following property holds, independently from the probability distribution of the minimal subset sizes $m_i$:

\begin{property}[\propertyDisjointSubsetDistance]
\label{prop:disjoint_subset_distance} In the case of disjoint minimal subsets, the probability that the distance $\pi(X^*,j+1)-\pi(X^*,j)$ between two consecutive elements is $k$ does not depend on $j$ and is a non-increasing function of $k$.
\end{property}

\subsection{Random minimal subsets of random size}

In this section, we experimentally study the distributions of random minimal subsets of random size. For each sample, a set of $c$ subsets of size uniformly selected in $[1,n]$ is generated and the non-minimal subsets are eliminated. 

For $n=20$ and $c=15$, Figure \ref{fig:random_subset_size} shows the probability distribution of (1) the size of all generated minimal subsets and (2) the size of the selected minimal subset $X^*$. As we could expect, selected subsets $X^*$ are usually smaller than the average size of all minimal subsets. This is because large minimal subsets are in general lower than small minimal subsets in the sense of total order $\prec$.

\begin{figure}[ht]
\centering
\includegraphics[width=8cm,keepaspectratio,clip]{random_subset_size.pdf}
\caption{Minimal subset sizes}
\label{fig:random_subset_size}
\end{figure}

For the same configuration and only considering the samples for which the selected minimal subset $X^*$ is of size 4, Figure \ref{fig:random_subset_distance} displays the probability distributions of the distances between consecutive elements in $X^*$. Here, unlike for disjoint subsets, we see that these distributions slightly depend on which consecutive elements of $X^*$ are considered. Elements of $X^*$ tend to be slightly more concentrated in the end. It is to be noted that, here again, all those probability distributions are non-increasing functions of the distance.

\begin{figure}[ht]
\centering
\includegraphics[width=7.5cm,keepaspectratio,clip]{random_subset_distance.pdf}
\caption{Distance probability between two consecutive elements}
\label{fig:random_subset_distance}
\end{figure}

\subsection{Conclusion of the probabilistic study}

We can draw two conclusions from this short study on uniformly distributed minimal subsets:
\begin{enumerate}
\item The probability distribution of the distance between consecutive elements of the selected minimal subset $X^*$ is in general a non-increasing function. This was formally shown in the case of disjoint minimal sets. Stated otherwise, the $j+1^{th}$ element of the selected minimal subset is likely to be close to the $j^{th}$ element.
\item The probability distributions of the distance between consecutive elements of the selected minimal subset $X^*$ do not heavily depend on which element is considered. In case of disjoint minimal subsets it can even be shown that these distributions are the same.
\end{enumerate}

% ===============================================================
% ALGORITHMS
% ===============================================================

\section{Algorithms}

We now describe a family of iterative algorithms for computing a minimal subset. The input problem is given by:
\begin{itemize}
\item The finite set $U=(u_1,...,u_n)$
\item The upward-closed property ${\cal P}$
\end{itemize}

All the iterative algorithms introduced in this section share the same framework presented on Algorithm \ref{alg:alg1}. The input set $U$ is stored as an array of size $n$ where $U[i]=u_i, i \in \{1,...,n\}$.

\begin{algorithm}
	\caption{FindMinimalSubset($U$, ${\cal P}$)}
	\label{alg:alg1}
	\begin{algorithmic}[1]
  %\Procedure{FindMinimalSubset}{$U$, ${\cal P}$}
    \Require ${\cal P}(U)$
		\State $\mathrm{Shuffle}(U)$ \Comment{Called once: $O(n)$}
		\State $X \gets \emptyset$ \Comment{$X$: minimal subset under construction}
		\State $i \gets 0$ \Comment{$i$: index of last element added to $X$}
		\Repeat
			\State $i \gets \mathrm{FindNext}(X,U,i+1,{\cal P})$
			\State $X \gets X \cup \{ U[i] \}$
		\Until{$(i=n) \lor {\cal P}(X)$}
		\State \textbf{return} $X$
		%\EndProcedure
	\end{algorithmic}
\end{algorithm}

At line 1, the array $U$ is shuffled. This will rule out any particular structure in the set $U$ and, informally speaking, will ensure that minimal subsets are uniformly distributed in $U$ so that the main conclusions of section 3 can be exploited. This shuffling defines a total order $\prec$ among the elements of $U$. Procedure $\mathrm{FindNext}(X,U,i,{\cal P})$ is in charge of finding and returning the largest index $j$ ($i \leq j \leq n$) such that ${\cal P}(X \cup U_{j \rightarrow})$. The algorithms differ according to their implementation of this procedure.

Figure \ref{fig:subsets} illustrates a set $U$, its minimal subsets for property ${\cal P}$ ordered by relation $\prec$ and the subsets $X$ and $U_{i \rightarrow}$ at the iteration of algorithm \ref{alg:alg1} when $U[i]$ is added to the current minimal subset $X$ (line 6 of Algorithm \ref{alg:alg1}).

\begin{figure}[ht]
\centering
\includegraphics[width=6cm,keepaspectratio,clip]{subsets.pdf}
\caption{Minimal subsets}
\label{fig:subsets}
\end{figure}

\begin{property}[\propertyAlgoCorrectness]
\label{correctness}
Algorithm \ref{alg:alg1} is correct and returns the unique minimal subset $X^* \in {\cal M}$ that is maximal with respect to the total order $\prec$. 
\end{property}


\subsection{Naive Algorithm}

The naive algorithm simply iterates over all elements $u_j$, $i<j$ until the first $j$ such that $\neg {\cal P}(X \cup U_{j \rightarrow})$ in order to return index $j-1$ (See Algorithm \ref{alg:naive}). This algorithm is clearly a correct implementation for procedure $\mathrm{FindNext}$.

\begin{algorithm}
	\caption{FindNext-Naive($X$, $U$, $i$, ${\cal P}$)}
	\label{alg:naive}
	\begin{algorithmic}[1]
	\Require{${\cal P}(X \cup U_{i \rightarrow})$}
	\State $j \gets i$
	  \Repeat
	    \State $j \gets j+1$
	  \Until{$(j=n+1) \lor \neg {\cal P}(X \cup U_{j \rightarrow})$}
		\State \textbf{return} $j-1$
	\end{algorithmic}
\end{algorithm}

\begin{property}[\propertyComplexityNaiveAlgo]
\label{complexity-naive}
If $\pi(X^*,|X^*|)<n$, the naive algorithm performs \smash{$\pi(X^*,|X^*|) +|X^*|$} property checks. Otherwise, if $\pi(X^*,|X^*|)=n$, the naive algorithm performs \smash{$n +|X^*|-2$} property checks.
\end{property}

\subsection{Dichotomy Algorithm}

The dichotomy algorithm corresponds to the DC algorithm proposed in \cite{Hemery2006}. It performs a dichotomic search presented in Algorithm \ref{alg:dichotomy}.

\begin{algorithm}
	\caption{FindNext-D($X$, $U$, $i$, ${\cal P}$)}
	\label{alg:dichotomy}
	\begin{algorithmic}[1]
	\Require{${\cal P}(X \cup U_{i \rightarrow})$}
	\State $l \gets i$, $r \gets n$
	  \While {$l \neq r$}
	    \State $m \gets \lceil (l+r)/2 \rceil$
	    \If {${\cal P}(X \cup U_{m \rightarrow})$} 
	      \State $l \gets m$
	    \Else
	      \State $r \gets m-1$
	    \EndIf
	  \EndWhile
		\State \textbf{return} $l$
	\end{algorithmic}
\end{algorithm}

The dichotomy algorithm is clearly a correct implementation for procedure $\mathrm{FindNext}$.

\begin{property}[\propertyCDichotomy]
\label{complexity-dichotomy}
In average, the dichotomy algorithm performs $O(|X^*|\times \log_2(n))$ property checks.
\end{property}

If $|X^*|=1$, the dichotomy algorithm performs $O(\log_2(n))$ checks which is clearly better than the $O(n)$ checks of the naive algorithm. At the other side of the spectrum if $|X^*|=n$, the dichotomy algorithm performs worse ($O(n\ \log_2(n))$) than the naive one ($O(n)$). The purpose of the algorithms described in the following sections is to take the best of both worlds.

\subsection{AD (Accelerate \& Dichotomize) Algorithm}

The idea of the basic {\em accelerate \& dichotomize} algorithm is to exploit the second conclusion of the probabilistic study that shows that the index $j$ returned by procedure $\mathrm{FindNext}(X,U,i,{\cal P})$ is likely to be quite close to index $i$. This is of course especially true if the size of the selected minimal subset is large. So it may pay off to search for such an index $j$ starting from index $i$ with an acceleration phase (trying $i+1$, $i+2$, $i+4$, ..., $i+2^k$) until the first index such that $\neg {\cal P}(X \cup U_{{i+2^k} \rightarrow})$ and then applying a dichotomic search on the index segment $[i+2^{k-1},i+2^k)$. 

\subsection{ADE (Accelerate, Dichotomize \& Estimate) Algorithm}

We can elaborate further on the previous algorithm by learning the initial step $s$ of the acceleration phase from the previous calls to the $\mathrm{FindNext}$ procedure. This is supported by the second conclusion of the probabilistic study that shows that the probability distribution of the distances between consecutive elements of $X^*$ does not depend much on which element is considered, thus this information can be estimated from the previous elements. In this context, the initial value of $s$ represents the expected average distance between two successive elements of the minimal set. For the first call to $\mathrm{FindNext}$, we take $s=n$ so this first call boils down to the pure dichotomy algorithm. For the later calls, the initial $s$ is computed as the average of the distance between successive elements already added to the minimal set $X$ under construction. 

\subsection{ADEL (Accelerate, Dichotomize, Estimate \& Lazy checks) Algorithm}

We finally propose a last improvement to the previous algorithm. Property checks on the subset $X$ under construction in Algorithm \ref{alg:alg1}, line $7$ will be called $m$ times for a minimal set of size $m$. If $m$ is large and typically getting close to $n$, this may represent a large proportion of the property checks of the algorithm. On the other side, it is not necessary to stop the algorithm as soon as one has proved the current subset $X$ satisfies the property: one can let the function $\mathrm{FindNext}$  show that the current subset does not have to be extended. It will show it with approximately $\log_2(n)$ property checks in the acceleration step. So the idea is to consider that once the size of the current subset $X$ is larger than $\log_2(n)$, as the effort to be spent for proving the property for $X^*$ with function $\mathrm{FindNext}$ won't exceed the effort already spent checking the property for each elements added to $X$ so far, we can stop checking the property in Algorithm \ref{alg:alg1}, line $7$ . This idea results in a {\em lazy} version of the algorithm denoted {\tt ADEL}\footnote{Note that the same idea can apply to algorithm {\tt DC}.} and shown in Algorithms \ref{alg:ADEL1} and \ref{alg:ADEL2}.

\begin{algorithm}
	\caption{FindMinimalSubset($U$, ${\cal P}$)}
	\label{alg:ADEL1}
	\begin{algorithmic}[1]
    \Require ${\cal P}(U)$
		\State $\mathrm{Shuffle}(U)$ \Comment{Called once: $O(n)$}
		\State $X \gets \emptyset$ \Comment{$X$: minimal subset under construction}
		\State $i \gets 0$ \Comment{$i$: index of last element added to $X$}
		\State $s \gets n$, $d_1 \gets 0$, $d_0 \gets 0$
		\Repeat
			\State $j \gets$ FindNext$(X,U,i+1,{\cal P},s)$
			\State $d_0 \gets d_0 + 1$
			%\State $d_1 \gets d_1 + j - i$, $s_0 = \lfloor d_1 / d_0 \rfloor - 1$
			% I DON'T THINK THE -1 ABOVE IS RIGHT, WITH IT WE CAN HAVE s0=0
			\State $d_1 \gets d_1 + j - i$, $s = \lfloor d_1 / d_0 \rfloor$ \Comment{Distance estimation}
			\State $i \gets j$
			\State $X \gets X \cup \{ U[i] \}$
		\Until{$(i=n) \lor (i \leq \log_2(n) \land {\cal P}(X)$)} \Comment{Lazy checks}
		\State \textbf{return} $X$
		%\EndProcedure
	\end{algorithmic}
\end{algorithm}

\begin{algorithm}
	\caption{FindNext($X$, $U$, $i$, ${\cal P}$,$s$)}
	\label{alg:ADEL2}
	\begin{algorithmic}[1]
	\Require{${\cal P}(X \cup U_{i \rightarrow})$}
	\State $l \gets i$, $r \gets n$
	\State $s_{max} \gets \lfloor (n-i)/2 \rfloor$ \Comment{Maximal step}
	\While {$(s \leq s_{max}) \land {\cal P}(X \cup U_{{i+s} \rightarrow})$} \Comment{Accelerate}
	\State $l \gets i+s$, $s \gets s*2$
	\EndWhile
	\If {$s \leq s_{max}$} 
	\State $r \gets i+s-1$
	\EndIf
	  \While {$l \neq r$} \Comment{Dichotomize}
	    \State $m \gets \lceil (l+r)/2 \rceil$
	    \If {${\cal P}(X \cup U_{m \rightarrow})$} 
	      \State $l \gets m$
	    \Else
	      \State $r \gets m-1$
	    \EndIf
	  \EndWhile
		\State \textbf{return} $l$
	\end{algorithmic}
\end{algorithm}
% ===============================================================
% EXPERIMENTAL STUDY
% ===============================================================

\section{Performance Study}

In this section, we compare the performances of the algorithms described in previous section together with the {\tt QuickXPlain} algorithm.

\subsection{Unique minimal subset}

We assume $U$ contains a unique minimal subset of size $m$ uniformly distributed in $U$ and $n=|U|$. The curves on Figures \ref{fig:experiments_n2048} and \ref{fig:experiments_m8} show the performance of the different algorithms (number of property checks) as a function of the size of the minimal subset $m$ and the size $n$ of set $U$. Each point is computed as the average over $100$ samples for the position of the minimal set in $U$. Note that both axis use a logarithmic scale. For the x-axis this is to give a special importance to small values of $m$.

Figure \ref{fig:experiments_n2048} shows for a fixed value of $n$ ($n=2048$) the average number of property checks performed by the different algorithms as a function of the size $m$ of the unique minimal subset of $U$. 

For the naive algorithm, for minimal subsets of unit size ($m=1$), the average number of checks is $n/2$ which is the average position of the element of the minimal set in $U$. For a minimal subset of size $n$, the naive algorithm has to perform $2\,n$ checks: $n$ checks to verify that removing any element of $U$ leads to a subset that do not check the property and $n$ checks to verify whether the current subset under construction satisfies the property.

The dichotomy algorithm {\tt D} performs $m\,(1+\log_2(n))$ checks which is linear with $m$ for a fixed $n$. For $m=1$ it performs $1+\log_2(n)$ checks and for $m=n$, it requires about $n\,(1+\log_2(n))$ checks.

Algorithm {\tt AD} performs worse than {\tt D} for small values of $m$. This is because when the elements of the selected minimal subset are sparse in $U$, the acceleration phase that starts with a unit step need to perform about $\log_2(n)$ iterations before to give the hand to the dichotomy phase that also will run in about $\log_2(n)$. For instance, for $m=1$, it requires about $1+2\,\log_2(n)$ steps and {\tt AD} is about twice worse as {\tt D}. The situation is the opposite when $m$ is getting closer to $n$: starting the acceleration phase with small values pays off and when $m=n$, the number of check of {\tt AD} is $2\,n$ (same as the naive algorithm) which is a $n/\log_2(n)$ improvement factor over {\tt D}.

As expected, algorithm {\tt ADE} takes the best of both algorithms {\tt D} and {\tt AD}: by learning the initial acceleration step, it can achieve the same performance as {\tt D} for small $m$ and as {\tt AD} for large $m$ values.

The last improvement in algorithm {\tt ADEL} pays off for large values of $m$ by replacing the $m$ checks in the FindNext functions by $\log_2(n)$ checks. For $m=n$, the number of checks is about $n+\log_2(n)$ where $\log_2(n)$ is the initial dichotomy to find the first element of the subset. So the improvement factor compared to {\tt ADE} is about $2$. 

In fact, {\tt ADEL} has {\em optimal complexity}\footnote{In terms of the {\em order of} comparison: $f(n) \sim g(n)$.} with $n$ both for {\em small} minimal sets ($m=1$) and {\em large} ones ($m=n$). When $m=1$ it boils down to a binary search in $\log_2(n)$. When $m=n$, complexity is in $n + 2\ \log_2(n)$ and any algorithm need indeed to at least check that each of the $n$ elements is member of the minimal subset by performing a check without it. The number of useless checks, due to the fact we do not know {\em a priori} that $m=n$, is only $2\,\log_2(n)$ and is negligible compared to $n$: $\log_2(n)$checks for identifying the first element with the initial binary search and the $\log_2(n)$ initial checks in Algorithm \ref{alg:ADEL1}, line $11$.

\begin{figure}[ht]
\centering
\includegraphics[width=8.5cm,keepaspectratio,clip]{experiments_n2048.pdf}
\caption{Number of property checks for $n=2048$}
\label{fig:experiments_n2048}
\end{figure}

Figure \ref{fig:comparison_n2048_quickxplain} compares the number of property checks performed by algorithms {\tt ADEL} and {\tt QuickXplain} \cite{Junker2004}. We see that {\tt ADEL} consistently performs less checks than {\tt QuickXplain} (between 10\% and 50\% less checks).

\begin{figure}[ht]
\centering
\includegraphics[width=8cm,keepaspectratio,clip]{comparison_n2048_quickxplain.pdf}
\caption{Number of property checks for $n=2048$}
\label{fig:comparison_n2048_quickxplain}
\end{figure}

Property \ref{prop:average_quickxplain} shows the average complexity for {\tt QuickXplain} in case of a unique minimal subset of unit size. Note that this average complexity turns out to be the average value of the best and worst case complexities (resp. $\log_2(n)$ and 2\ $\log_2(n)$) proved in \cite{Junker2004}. Compared to the $\log_2(n)$ complexity of algorithm {\tt ADEL}, this gives a \nicefrac{3}{2} factor in favor of {\tt ADEL}.

\begin{property}[\propertyAverageQuickXplain]
\label{prop:average_quickxplain} In case of a unique minimal set of size $1$, the average complexity of the {\tt QuickXplain} algorithm is in the order of $\nicefrac{3}{2}\ \log_2(n)$. 
\end{property}

In case of a minimal subset of size $n$, {\tt QuickXplain} need to perform $2\ n -2$ property checks that is, the number of nodes of a binary tree with $n$ leaves minus one because the top level node is not checked. Compared with the $n + 2\ \log_2(n)$ complexity of {\tt ADEL} in this case this gives, for large values of $n$ a factor $2$ in favor of {\tt ADEL}.

Figure \ref{fig:experiments_m8} shows, for a fixed value of $m$ ($m=8$) the average number of property checks performed by the different algorithms as a function of the size $n$ of $U$. 

\begin{figure}[ht]
\centering
\includegraphics[width=7cm,keepaspectratio,clip]{experiments_m8.pdf}
\caption{Number of property checks for $m=8$}
\label{fig:experiments_m8}
\end{figure}

\subsection{Random minimal subsets of random size}

For each sample, a set of $c$ subsets of size uniformly selected in $[1,n]$ is generated and the non-minimal subset are eliminated. Curves on Figure \ref{fig:experiments_random_n2048} shows the performance of the different algorithms (number of property checks) for $n=2048$ as a function of the number of subsets $c$ generated. When $c$ grows, the size of the selected minimal subset $X^*$ tends to decrease. The average and standard deviation of the size of the selected subset is also shown on the figure as an indicator. We can make similar observations as in the previous section: algorithm {\tt ADEL} outperforms all other iterative algorithms.

\begin{figure}[ht]
\centering
\includegraphics[width=8.5cm,keepaspectratio,clip]{experiments_random_n2048.pdf}
\caption{Number of property checks for $n=2048$}
\label{fig:experiments_random_n2048}
\end{figure}

The comparison with algorithm {\tt QuickXplain} is shown on Figure \ref{fig:comparison_random_n2048_quickxplain}. We see that {\tt ADEL} consistently performs less checks than {\tt QuickXplain} (between 10\% and 30\% less checks).

\begin{figure}[ht]
\centering
\includegraphics[width=8.5cm,keepaspectratio,clip]{comparison_random_n2048_quickxplain.pdf}
\caption{Number of property checks for $n=2048$}
\label{fig:comparison_random_n2048_quickxplain}
\end{figure}

\section{Conclusion}
This paper studies the problem of finding a minimal subset satisfying an upward-closed property. No assumption is made on the property being checked and the objective is to minimize the number of property checks. The approach can be applied to the extraction of MUCs in Constraint Networks. The proposed {\tt ADEL} algorithm initially shuffles the set of elements and then exploits the probabilistic properties of the position of consecutive elements in the selected subset. We show that:
\begin{itemize}
\item it is {\em optimal} for small subsets, with a complexity in the order of $\log_2(n)$ for a unique minimal subset of size $1$.
\item it is {\em optimal} for large subsets, with a complexity in the order of $n$ for a minimal subset of size $n$.
\item the average number of checks behaves continuously in between those two extremal cases and outperforms all variants studied in this paper.
\item in the case of a unique minimal set of size one, it performs in average \nicefrac{3}{2} times less checks than the {\tt QuickXplain} algorithm (33\% less checks).
\item in the case of a unique minimal set of size $n$, for large values of $n$, it performs about twice less checks than the {\tt QuickXplain} algorithm (50\% less checks).
\item in the case of a unique minimal set of size $n$, for large values of $n$,  it performs about $\log_2(n)$ less checks than the {\tt DC} algorithm.
\item in between those two extremal cases it consistently performs less property checks than the {\tt QuickXplain} algorithm (between 10\% and 50\% less checks in the instances generated for our experiments).
\item the algorithm is by nature iterative and is thus easier to implement than a recursive algorithm like {\tt QuickXplain} if recursion is to be avoided.
\end{itemize} 
Algorithm {\tt ADEL} is used as the implementation of the MUC extraction functionality (Conflict Refiner) in CP Optimizer since version 12.5.

\bibliography{c:/MyOffice/Biblio/biblio}
\end{document}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
