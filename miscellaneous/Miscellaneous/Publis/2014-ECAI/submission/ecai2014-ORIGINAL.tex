\documentclass{ecai2014}
\usepackage{times}
%\usepackage{graphicx}
\usepackage{latexsym}
\usepackage{nicefrac}
\usepackage{helvet}
\usepackage{courier}
\usepackage{amsmath,amssymb}
\usepackage[pdftex]{graphicx}
\usepackage{algorithm}
%\usepackage[noend]{algpseudocode}
\usepackage{algpseudocode}
\usepackage[usenames]{color}
\usepackage{theomac}

\newcommand{\Z}{\mathbb{Z}}

\newtheorem{definition}{Definition}
\newtheoremWithMacro{property}{Property}
\def\qed{\hfill \quad \vrule height4.17pt width4.17pt depth0pt} 
\newenvironment{proof}[1][Proof]{\noindent \textbf{#1:} }{\qed}

% Macro for colored "todo" notes:
\definecolor{NoteColor}{rgb}{1, 0.2, 0.2}
\newcommand{\note}[1]{\textcolor{NoteColor}{#1}}

% Macros for math functions:
\DeclareMathOperator{\pos}{pos}
\DeclareMathOperator{\idx}{index}

\newcommand{\algn}{\text{\it algn}}

%%\ecaisubmission   % inserts page numbers. Use only for submission of paper.
                  % Do NOT use for camera-ready version of paper.

\begin{document}

\title{An Optimal Algorithm for extracting Minimal Conflicts\\ in a Black-box Constraint Network}

\author{Philippe Laborie\institute{IBM Software Group,
France, email: laborie@fr.ibm.com} }

\maketitle
\bibliographystyle{ecai2014}

\begin{abstract}
  TODO
\end{abstract}

\section{Introduction}

Given a finite set $U$, finding a minimal subset of $U$ satisfying a given property ${\cal P}$ is a frequent problem in computer science. For instance, this problem can be found in diagnosis ({\em find a minimal subset of faulty components of a system that explains the current observations}) or in non-monotonic logics ({\em find a minimal subset of abnormalities in the clauses that restore admissibility}).

In several important applications the property ${\cal P}$ is upward-closed that is, whenever it holds for a subset $X$ it also holds for any superset of $X$. This is typically the case of the identification of minimal infeasible subsets (conflicts) of constraints in optimization models \cite{Chinneck2007}: if a subset $X$ of constraints is infeasible, any superset of $X$ clearly is infeasible too.

In general, checking property ${\cal P}$ for a particular subset is an expensive operation (it can be NP-complete) and often the approaches for solving this minimal subset problem rely on the particular features of the property ${\cal P}$ being considered. For instance in the optimization context mentioned above techniques such as {\em elastic filters} can be used for Linear Programming models \cite{Chinneck1997}, {\em explanations} recording \cite{Jussien2002} for Constraint Programming models or {\em no-goods} learning for Boolean Satisfiability models \cite{MarquesSilva1996}.

But the logic behind property check ${\cal P}(X)$ can be so complex that it may turn out to be necessary or advantageous to consider it as a black box operation. Property ${\cal P}(X)$ could for instance be the outcome of a complex simulation process based on a set of input events $X$. In the context of minimal conflicts identification, the advances in the optimization state-of-the-art result in increasingly sophisticated and efficient engines. Engines sophistication makes it harder to implement intrusive methods to compute minimal conflicts whereas increase of engines efficiency makes infeasibility property check faster. Both aspects tend to make the black box approach more attractive.

In this paper we investigate the problem of finding a minimal subset satisfying an upward-closed property {\em without any knowledge on the property} except for the assumption that it is upward-closed. The complexity measure we consider is therefore the number of property checks performed. The work closest to ours is probably the {\tt QuickXplain} recursive algorithm \cite{Junker2004} developed in the context of the computation of minimal conflicts for constraint programming using a black box approach. An iterative algorithm to solve this problem has also been proposed in \cite{Hemery2006}.

After introducing some notations and formally defining the problem, the paper presents a statistical study of the relative positions of elements of the selected minimal set. This study highlights a couple of interesting properties that are exploited in the following section of the paper that gradually introduces the proposed algorithm  {\tt ADEL} (standing for the four ingredients of the approach: Dichotomy, Acceleration, Estimation and Lazy checks). We show in the last section of the paper that the complexity of this algorithm is optimal both for small and large minimal subsets and that it performs in average between 10\% and 50\% less property checks than the {\tt QuickXplain} algorithm. Proofs of properties are given in a separate section at the end.

% ===============================================================
% PROBLEM DEFINITION AND NOTATIONS
% ===============================================================

\section{Problem Definition and Notations}

Let $U$ be a finite set of cardinality $n$ and ${\cal P}$ an {\em upward-closed} property on its powerset $2^U$ that is, a property such that:
\[ \big( X \subseteq Y \subseteq U \big) \land {\cal P}(X) \Rightarrow {\cal P}(Y) \]

\begin{definition}[Minimal Subset]
\label{minsubset}
A subset $X \subseteq U$ is said to be {\bf minimal} if and only if:
${\cal P}(X) \land \forall Y \subset X: \neg {\cal P}(Y)$. Let ${\cal M}$ denote the set of all minimal subsets of $U$.
\end{definition}

In the sequel of this paper we assume ${\cal P}(U)$, that is there exist at least one minimal subset. Of course, in general there is not a unique minimal subset: there can be an exponential number of such subsets, for instance the set of minimal subsets could be all subsets of cardinality $n/2$ which is lower bounded by \smash{$2^{n/2}$}.

Without loss of generality we assume a total order $\prec$ over the elements in $U$ so that the elements can be indexed: $U=(u_1,...,u_n)$ with $u_i \prec u_{i+1}$. 

The total order $\prec$ over $U$ implies a lexicographic total order $\prec$ over the powerset \smash{$2^U$}. Let $X,Y \subseteq U, X \neq Y$:
\begin{eqnarray*}
X \prec Y \Leftrightarrow \exists j \in [1,n]: \left \lbrace
\begin{array}{l}
\forall i<j, (u_i \in X) \Leftrightarrow (u_i \in Y) \\
u_j \in X \\
u_j \notin Y
\end{array}
\right.
\end{eqnarray*}

We denote $X^*$ the unique minimal subset $X^* \in {\cal M}$ that is maximal with respect to the total order $\prec$. 

The problem studied in this paper is the design of efficient algorithms to compute minimal subset $X^*$ without any knowledge about property ${\cal P}$ beside the assumption that it is upward-closed. We estimate the complexity of an algorithm to compute a minimal subset as the number of property checks it performs.

Note that in practice, relation $\prec$ can be used to express preferences over minimal subsets, in this case, the algorithms presented in this paper will compute a preferred minimal subset. The definition of relation $\prec$ is the same as in \cite{Junker2004}.

Let $X\subseteq U$ and $m=|X|$. Without loss of generality, we can sort the $m$ elements of $X$ by their index in $U$: \smash{$X = \{ u_{\pi(X,j)} \}_{j \in [1,m]}$} with \smash{$1 \leq \pi(X,j) < \pi(X,j+1) \leq n$}. Integer \smash{$\pi(X,j)$} denotes the position in $U$ of the \smash{$j^{th}$} element of subset $X$.

Figure \ref{fig:notations} illustrates a set $U$, the set of all minimal subsets ordered by relation $\prec$, subset $X^*$ and the distances between consecutive elements in $X^*$. 

\begin{figure}[ht]
\centering
\includegraphics[width=7cm,keepaspectratio,clip]{notations.pdf}
\caption{Minimal subsets}
\label{fig:notations}
\end{figure}

For $1 \leq i \leq n$, the subset \smash{$\{u_j | i \leq j \leq n \}$} of all $u_j$ with index greater than $i$ is denoted \smash{$U_{i\rightarrow}$}.

% ===============================================================
% STATISTICAL STUDY
% ===============================================================

\section{Statistical study}

In this section, we study (either analytically or experimentally) the distribution of the positions \smash{$\pi(X^*,j)$} and distances \smash{$\pi(X^*,j+1)-\pi(X^*,j)$} between the elements of minimal subset $X^*$ in $U$ under some assumptions. The main conclusions of this study will be used to design efficient algorithms for computing $X^*$.

\subsection{Unique minimal subset}

In this section we assume $U$ contains a unique minimal subset of size $m$ with $0 < m \leq n$ uniformly distributed in $U$. 

Property \ref{prop:unique_subset_position} gives the probability distributions of the $j^{th}$ element of the minimal subset. 

\begin{property}[\propertyUniqueSubsetPosition]
\label{prop:unique_subset_position} In the case of a unique minimal subset of size $m$, the probability $P\big(\pi(X^*,j)=k\big)$ that the position of the $j^{th}$ element of the minimal subset is $k$, denoted $p_j(k)$ is given by:\\
\[ p_j(k) = \binom{k-1}{j-1}\binom{n-k}{m-j}/\binom{n}{m} \]
\end{property}

These distributions are illustrated on figure \ref{fig:unique_subset_position} for $n=20$ and $m=4$.

\begin{figure}[ht]
\centering
\includegraphics[width=8.5cm,keepaspectratio,clip]{unique_subset_position.pdf}
\caption{Position probabilities}
\label{fig:unique_subset_position}
\end{figure}

Property \ref{prop:unique_subset_distance} gives the probability distributions of the distances between consecutive elements of the unique minimal subset in $U$. 

\begin{property}[\propertyUniqueSubsetDistance]
\label{prop:unique_subset_distance} In the case of a unique minimal subset of size $m$, the probability that the distance $\pi(X^*,j+1)-\pi(X^*,j)$ between two consecutive elements is $k$ does not depend on $j$ and is the same as the probability that the first element of the minimal subset is at position $k$ ($p_1(k)$): \\
$\forall j \in [1,m), P\big(\pi(X^*,j+1)-\pi(X^*,j)=k\big) = p_1(k) $\\
With:\\
\[ p_1(k) = \binom{n-k}{m-1}/\binom{n}{m} \]
\end{property}

From the formula, probability $p_1(k)$ can be computed recursively as follows:
\begin{itemize}
\item $p_1(1)=m/n$
\item $\forall k \in [2,n-m+1], p_1(k+1)= \big( 1 - \frac{m-1}{n-k} \big) \  p_1(k)$
\item $\forall k > n-m+1, p_1(k) = 0$
\end{itemize}

The probability distribution $p_1(k)$ is illustrated on figure \ref{fig:unique_subset_distance} for $n=20$ and $m=4$. This probability exponentially decreases with $k$. This result can be seen as a discrete version of the probability density of the distance $x$ between two neighbor points from a set of points randomly and uniformly spread on a line which is \smash{$p(x)=\frac{1}{d}\ e^{-x / d}$} where $d$ is the mean distance between two neighbor points \cite{demaret1977}.

\begin{figure}[ht]
\centering
\includegraphics[width=8.5cm,keepaspectratio,clip]{unique_subset_distance.pdf}
\caption{Distance probability between two consecutive elements}
\label{fig:unique_subset_distance}
\end{figure}

\subsection{Disjoint minimal subsets of random size}

In this section we assume $U$ contains $c$ uniformly distributed disjoint minimal subsets of random sizes $m_{i}$, $i \in [1,c]$ with $0 < \sum_{i \in [1,c] } m_i \leq n$. The following property holds, independently from the probability distribution of the minimal subset sizes $m_i$:

\begin{property}[\propertyDisjointSubsetDistance]
\label{prop:disjoint_subset_distance} In the case of disjoint minimal subsets, the probability that the distance $\pi(X^*,j+1)-\pi(X^*,j)$ between two consecutive elements is $k$ does not depend on $j$ and is a non-increasing function of $k$.
\end{property}

Figure \ref{fig:samesize_disjoint_subset_position} gives the distributions of the positions of the elements of minimal subset $X^*$ in $U$ for the particular case of a set of $5$ independent minimal subsets of size $4$. It is to be noted that in this case, all the elements of $U$ belong to a unique minimal subset. Figure \ref{fig:samesize_disjoint_subset_distance} shows the distribution of the distance between consecutive elements of $X^*$ in this case. 

\begin{figure}[ht]
\centering
\includegraphics[width=8.5cm,keepaspectratio,clip]{samesize_disjoint_subset_position.pdf}
\caption{Position probabilities}
\label{fig:samesize_disjoint_subset_position}
\end{figure}

\begin{figure}[ht]
\centering
\includegraphics[width=8.5cm,keepaspectratio,clip]{samesize_disjoint_subset_distance.pdf}
\caption{Distance probability between two consecutive elements}
\label{fig:samesize_disjoint_subset_distance}
\end{figure}

\subsection{Random minimal subsets of random size}

In this section, we experimentally study the distributions of random minimal subsets of random size. For each sample, a set of $c$ subsets of size uniformly selected in $[1,n]$ is generated and the non-minimal subsets are eliminated. 

For $n=20$ and $c=15$, Figure \ref{fig:random_subset_size} shows the probability distribution of (1) the size of all generated minimal subsets and (2) the size of the selected minimal subset $X^*$. As we could expect, selected subsets $X^*$ are usually smaller than the average size of all minimal subsets. This is because large minimal subsets are in general lower than small minimal subsets in the sense of total order $\prec$.

\begin{figure}[ht]
\centering
\includegraphics[width=8cm,keepaspectratio,clip]{random_subset_size.pdf}
\caption{Minimal subset sizes}
\label{fig:random_subset_size}
\end{figure}

For the same configuration and only considering the samples for which the selected minimal subset $X^*$ is of size 4, Figure \ref{fig:random_subset_distance} displays the probability distributions of the distances between consecutive elements in $X^*$. Here, unlike for disjoint subsets, we see that these distributions slightly depend on which consecutive elements of $X^*$ are considered. Elements of $X^*$ tend to be slightly more concentrated in the end. It is to be noted that, here again, all those probability distributions are non-increasing functions of the distance.

\begin{figure}[ht]
\centering
\includegraphics[width=7.5cm,keepaspectratio,clip]{random_subset_distance.pdf}
\caption{Distance probability between two consecutive elements}
\label{fig:random_subset_distance}
\end{figure}

\subsection{Conclusion of the statistical study}

We can draw three conclusions from this short study on uniformly distributed minimal subsets:
\begin{enumerate}
\item When there are several minimal subsets, the selected one $X^*$ tend to be smaller than the average. This is a good property as in most of the applications one is in general interested in minimal subsets of small cardinality.
\item The probability distribution of the distance between consecutive elements of the selected minimal subset $X^*$ is in general a non-increasing function. This was formally shown in the case of disjoint minimal sets. Stated otherwise, the $j+1^{th}$ element of the selected minimal subset is likely to be close to the $j^{th}$ element.
\item The probability distributions of the distance between consecutive elements of the selected minimal subset $X^*$ do not heavily depend on which element is considered. In case of disjoint minimal subsets it can even be shown that they are independent.
\end{enumerate}

% ===============================================================
% ALGORITHMS
% ===============================================================

\section{Algorithms}

The input problem is given by:
\begin{itemize}
\item The finite set $U=(u_1,...,u_n)$
\item The upward-closed property ${\cal P}$
\end{itemize}

All the algorithms studied in this paper share the same framework presented on Algorithm \ref{alg:alg1}. 

The input set $U$ is stored as an array of size $n$ where $U[i]=u_i, i \in \{1,...,n\}$.

\begin{algorithm}
	\caption{FindMinimalSubset($U$, ${\cal P}$)}
	\label{alg:alg1}
	\begin{algorithmic}[1]
  %\Procedure{FindMinimalSubset}{$U$, ${\cal P}$}
    \Require ${\cal P}(U)$
		\State $\mathrm{Shuffle}(U)$ \Comment{Called once: $O(n)$}
		\State $X \gets \emptyset$ \Comment{$X$: minimal subset under construction}
		\State $i \gets 0$ \Comment{$i$: index of last element added to $X$}
		\Repeat
			\State $i \gets \mathrm{FindNext}(X,U,i+1,{\cal P})$
			
			\State $X \gets X \cup \{ U[i] \}$
		\Until{$(i=n) \lor {\cal P}(X)$}
		\State \textbf{return} $X$
		%\EndProcedure
	\end{algorithmic}
\end{algorithm}

At line 1, the array $U$ is shuffled, this will rule out any particular structure in the set $U$ and, informally speaking, will ensure that minimal subsets are uniformly distributed in $U$. This shuffling defines a total order $\prec$ among the elements of $U$. Procedure $\mathrm{FindNext}(X,U,i,{\cal P})$ is in charge of finding and returning the largest index $j$ ($i \leq j \leq n$) such that ${\cal P}(X \cup U_{j \rightarrow})$. The algorithms described in this paper differ according to their implementation of this procedure.

Figure \ref{fig:subsets} illustrates a set $U$, the set of all minimal subsets ordered by relation $\prec$ and the subsets $X$ and $U_{i \rightarrow}$ at the iteration of algorithm \ref{alg:alg1} when $U[i]$ is added to the current minimal subset $X$.

\begin{figure}[ht]
\centering
\includegraphics[width=7cm,keepaspectratio,clip]{subsets.pdf}
\caption{Minimal subsets}
\label{fig:subsets}
\end{figure}

\begin{property}[\propertyAlgoCorrectness]
\label{correctness}
Algorithm \ref{alg:alg1} is correct and returns the unique minimal subset $X^* \in {\cal M}$ that is maximal with respect to the total order $\prec$. 
\end{property}

\subsection{Naive Algorithm}

The naive algorithm simply iterates over all elements $u_j$, $i<j$ until the first $j$ such that $\neg {\cal P}(X \cup U_{j \rightarrow})$ in order to return index $j-1$ (See Algorithm \ref{alg:naive}). This algorithm is clearly a correct implementation for procedure $\mathrm{FindNext}$.

\begin{algorithm}
	\caption{FindNext-Naive($X$, $U$, $i$, ${\cal P}$)}
	\label{alg:naive}
	\begin{algorithmic}[1]
	\Require{${\cal P}(X \cup U_{i \rightarrow})$}
	\State $j \gets i$
	  \Repeat
	    \State $j \gets j+1$
	  \Until{$(j=n+1) \lor \neg {\cal P}(X \cup U_{j \rightarrow})$}
		\State \textbf{return} $j-1$
	\end{algorithmic}
\end{algorithm}

\begin{property}[\propertyComplexityNaiveAlgo]
\label{complexity-naive}
If $\pi(X^*,|X^*|)<n$, the naive algorithm performs \smash{$\pi(X^*,|X^*|) +|X^*|$} property checks. Otherwise, if $\pi(X^*,|X^*|)=n$, the naive algorithm performs \smash{$n +|X^*|-2$} property checks.
\end{property}

\subsection{Dichotomy Algorithm}

The dichotomy algorithm corresponds to the DC algorithm proposed in \cite{Hemery2006}. It performs a dichotomic search, see Algorithm \ref{alg:dichotomy}.

\begin{algorithm}
	\caption{FindNext-D($X$, $U$, $i$, ${\cal P}$)}
	\label{alg:dichotomy}
	\begin{algorithmic}[1]
	\Require{${\cal P}(X \cup U_{i \rightarrow})$}
	\State $l \gets i$, $r \gets n$
	  \While {$l \neq r$}
	    \State $m \gets \lceil (l+r)/2 \rceil$
	    \If {${\cal P}(X \cup U_{m \rightarrow})$} 
	      \State $l \gets m$
	    \Else
	      \State $r \gets m-1$
	    \EndIf
	  \EndWhile
		\State \textbf{return} $l$
	\end{algorithmic}
\end{algorithm}

The dichotomy algorithm is clearly a correct implementation for procedure $\mathrm{FindNext}$.

\begin{property}[\propertyCDichotomy]
\label{complexity-dichotomy}
In average, the dichotomy algorithm performs $O(|X^*|\times \log_2(n))$ property checks.
\end{property}

If $|X^*|=1$, the dichotomy algorithm performs $O(\log_2(n))$ checks which is clearly better than the $O(n)$ checks of the naive algorithm. At the other side of the spectrum if $|X^*|=n$, the dichotomy algorithm performs worse ($O(n\ \log_2(n))$) than the naive one ($O(n)$). The purpose of the {\em accelerate \& dichotomize} algorithms described in next sections is to take the best of both worlds.

\subsection{Accelerate \& Dichotomize Algorithm}

The idea of the basic {\em accelerate \& dichotomize} algorithm is to exploit the second conclusion of the statistical study that shows that the index $j$ returned by procedure $\mathrm{FindNext}(X,U,i,{\cal P})$ is likely to be quite close to index $i$. This is of course especially true if the size of the selected minimal subset is large. So it may pay off to search for such an index starting from index $i$ with an acceleration phase (trying $i+1$, $i+2$, $i+4$, ..., $i+2^k$) until the first index such that $\neg {\cal P}(X \cup U_{{i+2^k} \rightarrow})$ and then applying a dichotomic search on the index segment $[i+2^{k-1},i+2^k]$. This idea leads to Algorithm \ref{alg:accelerate_dichotomy}.

\begin{algorithm}
	\caption{FindNext-AD($X$, $U$, $i$, ${\cal P}$)}
	\label{alg:accelerate_dichotomy}
	\begin{algorithmic}[1]
	\Require{${\cal P}(X \cup U_{i \rightarrow})$}
	\State $l \gets i$, $r \gets n$
	\State $s \gets 1$ \Comment{Initial step}
	\State $s_{max} \gets \lfloor (n-i)/2 \rfloor$ \Comment{Maximal step}
	\While {$(s \leq s_{max}) \land {\cal P}(X \cup U_{{i+s} \rightarrow})$} \Comment{Accelerate}
	\State $l \gets i+s$, $s \gets s*2$
	\EndWhile
	\If {$s \leq s_{max}$} 
	\State $r \gets i+s-1$
	\EndIf
	  \While {$l \neq r$} \Comment{Dichotomize}
	    \State $m \gets \lceil (l+r)/2 \rceil$
	    \If {${\cal P}(X \cup U_{m \rightarrow})$} 
	      \State $l \gets m$
	    \Else
	      \State $r \gets m-1$
	    \EndIf
	  \EndWhile
		\State \textbf{return} $l$
	\end{algorithmic}
\end{algorithm}

\subsection{Accelerate, Dichotomize \& Estimate Algorithm}

The {\em estimate, accelerate \& dichotomize} algorithm elaborates further on the previous algorithm by learning the initial step $s$ of the acceleration phase from the previous calls to the $\mathrm{FindNext}$ procedure. This is supported by the third conclusion of the statistical study that shows that the probability distribution of the distances between consecutive elements of $X^*$ does not depend much on which element is considered, thus this information can be estimated from the previous elements. In this context, the initial value of $s$ represents the expected average distance between two successive elements of the minimal set. For the first call to $\mathrm{FindNext}$, we take $s=n$ so, as $s>s_{max}$, this first call boils down to the pure dichotomy algorithm. For the later calls, the initial $s$ is computed as the average of the distance between successive elements already added to the minimal set $X$ under construction. See Algorithms \ref{alg:alg2} and \ref{alg:sa_accelerate_dichotomy}.

\begin{algorithm}
	\caption{FindMinimalSubset-ADE($U$, ${\cal P}$)}
	\label{alg:alg2}
	\begin{algorithmic}[1]
    \Require ${\cal P}(U)$
		\State $\mathrm{Shuffle}(U)$ \Comment{Called once: $O(n)$}
		\State $X \gets \emptyset$ \Comment{$X$: minimal subset under construction}
		\State $i \gets 0$ \Comment{$i$: index of last element added to $X$}
		\State $s_0 \gets n$, $d_1 \gets 0$, $d_0 \gets 0$
		\Repeat
			\State $j \gets$ FindNext-ADE$(X,U,i+1,{\cal P},s_0)$
			\State $d_0 \gets d_0 + 1$
			%\State $d_1 \gets d_1 + j - i$, $s_0 = \lfloor d_1 / d_0 \rfloor - 1$
			% I DON'T THINK THE -1 ABOVE IS RIGHT, WITH IT WE CAN HAVE s0=0
			\State $d_1 \gets d_1 + j - i$, $s_0 = \lfloor d_1 / d_0 \rfloor$
			\State $i \gets j$
			\State $X \gets X \cup \{ U[i] \}$
		\Until{$(i=n) \lor {\cal P}(X)$}
		\State \textbf{return} $X$
		%\EndProcedure
	\end{algorithmic}
\end{algorithm}

\begin{algorithm}
	\caption{FindNext-ADE($X$, $U$, $i$, ${\cal P}$,$s_0$)}
	\label{alg:sa_accelerate_dichotomy}
	\begin{algorithmic}[1]
	\Require{${\cal P}(X \cup U_{i \rightarrow})$}
	\State $l \gets i$, $r \gets n$
	\State $s \gets s_0$ \Comment{Initial step}
	\State $s_{max} \gets \lfloor (n-i)/2 \rfloor$ \Comment{Maximal step}
	\While {$(s \leq s_{max}) \land {\cal P}(X \cup U_{{i+s} \rightarrow})$} \Comment{Accelerate}
	\State $l \gets i+s$, $s \gets s*2$
	\EndWhile
	\If {$s \leq s_{max}$} 
	\State $r \gets i+s-1$
	\EndIf
	  \While {$l \neq r$} \Comment{Dichotomize}
	    \State $m \gets \lceil (l+r)/2 \rceil$
	    \If {${\cal P}(X \cup U_{m \rightarrow})$} 
	      \State $l \gets m$
	    \Else
	      \State $r \gets m-1$
	    \EndIf
	  \EndWhile
		\State \textbf{return} $l$
	\end{algorithmic}
\end{algorithm}

\subsection{Accelerate, Dichotomize, Estimate \& Lazy checks Algorithm}

We finally propose a last improvement to the previous algorithm. Property checks on the subset $X$ under construction in Algorithm \ref{alg:alg2}, line $11$ will be called $m$ times for a minimal set of size $m$. If $m$ is large and typically, getting close to $n$, this may represent a large proportion of the property checks of algorithm {\tt ADE}. On the other side, it is not necessary to stop the algorithm as soon as one has proved the current subset $X$ satisfies the property: one can let the function $\mathrm{FindNext}\text{-}\mathrm{ADE}$  show that the current subset does not have to be extended. It will show it with approximatively $\log_2(n)$ property checks in the acceleration step. So the idea is to consider that once the size of the current subset $X$ is larger than $\log_2(n)$, as the effort to be spent for proving the property for $X^*$ with function $\mathrm{FindNext}\text{-}\mathrm{ADE}$ won't exceed the effort already spent checking the property for each elements added to $X$ so far, we can stop checking the property in algorithm \ref{alg:alg2}, line $11$. This idea results in a {\em lazy} version of algorithm {\tt ADE} denoted {\tt ADEL}\footnote{Note that the same idea can apply to algorithms {\tt D} and {\tt AD}.} and shown in Algorithms \ref{alg:ADEL1} and \ref{alg:ADEL2}.

\begin{algorithm}
	\caption{FindMinimalSubset($U$, ${\cal P}$)}
	\label{alg:ADEL1}
	\begin{algorithmic}[1]
    \Require ${\cal P}(U)$
		\State $\mathrm{Shuffle}(U)$ \Comment{Called once: $O(n)$}
		\State $X \gets \emptyset$ \Comment{$X$: minimal subset under construction}
		\State $i \gets 0$ \Comment{$i$: index of last element added to $X$}
		\State $s_0 \gets n$, $d_1 \gets 0$, $d_0 \gets 0$
		\Repeat
			\State $j \gets$ FindNext$(X,U,i+1,{\cal P},s_0)$
			\State $d_0 \gets d_0 + 1$
			%\State $d_1 \gets d_1 + j - i$, $s_0 = \lfloor d_1 / d_0 \rfloor - 1$
			% I DON'T THINK THE -1 ABOVE IS RIGHT, WITH IT WE CAN HAVE s0=0
			\State $d_1 \gets d_1 + j - i$, $s_0 = \lfloor d_1 / d_0 \rfloor$
			\State $i \gets j$
			\State $X \gets X \cup \{ U[i] \}$
		\Until{$(i=n) \lor (i \leq \log_2(n) \land {\cal P}(X)$)}
		\State \textbf{return} $X$
		%\EndProcedure
	\end{algorithmic}
\end{algorithm}

\begin{algorithm}
	\caption{FindNext($X$, $U$, $i$, ${\cal P}$,$s_0$)}
	\label{alg:ADEL2}
	\begin{algorithmic}[1]
	\Require{${\cal P}(X \cup U_{i \rightarrow})$}
	\State $l \gets i$, $r \gets n$
	\State $s \gets s_0$ \Comment{Initial step}
	\State $s_{max} \gets \lfloor (n-i)/2 \rfloor$ \Comment{Maximal step}
	\While {$(s \leq s_{max}) \land {\cal P}(X \cup U_{{i+s} \rightarrow})$} \Comment{Accelerate}
	\State $l \gets i+s$, $s \gets s*2$
	\EndWhile
	\If {$s \leq s_{max}$} 
	\State $r \gets i+s-1$
	\EndIf
	  \While {$l \neq r$} \Comment{Dichotomize}
	    \State $m \gets \lceil (l+r)/2 \rceil$
	    \If {${\cal P}(X \cup U_{m \rightarrow})$} 
	      \State $l \gets m$
	    \Else
	      \State $r \gets m-1$
	    \EndIf
	  \EndWhile
		\State \textbf{return} $l$
	\end{algorithmic}
\end{algorithm}
% ===============================================================
% EXPERIMENTAL STUDY
% ===============================================================

\section{Experimental Study}

In this section, we experimentally compare the performances of the algorithms described in previous section under the different assumptions analyzed in the statistical study.

\subsection{Unique minimal subset}

We assume $U$, with $n=|U|$, contains a unique minimal subset of size $m$ uniformly distributed in $U$. 

The curves on Figures \ref{fig:experiments_n2048} and \ref{fig:experiments_m8} show the performance of the different algorithms (number of property checks) as a function of the size of the minimal subset $m$ and the size $n$ of set $U$. Each point is computed as the average over $100$ samples for the position of the minimal set in $U$. Note that both axis use a logarithmic scale. For the x-axis this is to give a special importance to small values of $m$.

Figure \ref{fig:experiments_n2048} shows for a fixed value of $n$ ($n=2048$) the average number of property checks performed by the different algorithms as a function of the size $m$ of the unique minimal subset of $U$. 

For the naive algorithm, for minimal subsets of unit size ($m=1$), the average number of checks is roughly $n/2$ which is the average position of the element of the minimal set in $U$. For a minimal subset of size $n$, the naive algorithm has to perform $2\,n$ checks: $n$ checks to verify that removing any element of $U$ leads to a subset that do not check the property and $n$ checks to verify whether the current subset under construction satisfies the property.

The dichotomy algorithm {\tt D} roughly performs $m\,(1+\log_2(n))$ checks which is linear with $m$ for a fixed $n$. For $m=1$ it performs $1+\log_2(n)$ checks and for $m=n$, it requires about $n\,(1+\log_2(n))$ checks.

Algorithm {\tt AD} performs worse than {\tt D} for small values of $m$. This is because when the elements of the selected minimal subset are sparse in $U$, the acceleration phase that starts with a unit step need to perform about $\log_2(n)$ iterations before to give the hand to the dichotomy phase that also will run in about $\log_2(n)$. For instance, for $m=1$, it requires about $1+2\,\log_2(n)$ steps and {\tt AD} is about twice worse as {\tt D}. The situation is the opposite when $m$ is getting closer to $n$: starting the acceleration phase with small values pays off and when $m=n$, the number of check of {\tt AD} is $2\,n$ (same as the naive algorithm) which is a $n/\log_2(n)$ improvement factor over {\tt D}.

As expected, algorithm {\tt ADE} takes the best of both algorithms {\tt D} and {\tt AD}: by learning the initial acceleration step, it can achieve the same performance as {\tt D} for small $m$ and as {\tt AD} for large $m$ values.

The last improvement in algorithm {\tt ADEL} pays off for large values of $m$ by replacing the $m$ checks in the FindNext functions by $\log_2(n)$ checks. For $m=n$, the number of checks is about $n+\log_2(n)$ where $\log_2(n)$ is the initial dichotomy to find the first element of the subset. So the improvement factor compared to {\tt ADE} is about $2$. 

In fact, {\tt ADEL} has {\em optimal complexity} with $n$ both for {\em small} minimal sets ($m=1$) and {\em large} ones ($m=n$). When $m=1$ it boils down to a binary search in $\log_2(n)$. When $m=n$, complexity is in $n + 2\ \log_2(n)$ and any algorithm need indeed to at least check that each of the $n$ elements is member of the minimal subset by performing a check without it. The number of useless checks, due to the fact we do not know {\em a priori} that $m=n$, is only $2\,\log_2(n)$ and is negligible compared to $n$: $\log_2(n)$checks for identifying the first element with the initial binary search and the $\log_2(n)$ initial checks in algorithm \ref{alg:alg2}, line $11$.

\begin{figure}[ht]
\centering
\includegraphics[width=8.5cm,keepaspectratio,clip]{experiments_n2048.pdf}
\caption{Number of property checks for $n=2048$}
\label{fig:experiments_n2048}
\end{figure}

Figure \ref{fig:comparison_n2048_quickxplain} compares the number of property checks performed by algorithms {\tt ADEL} and {\tt QuickXplain} \cite{Junker2004}. {\tt QuickXplain} is recapped in Algorithm \ref{alg:quickxplain}. 

\begin{figure}[ht]
\centering
\includegraphics[width=8.5cm,keepaspectratio,clip]{comparison_n2048_quickxplain.pdf}
\caption{Number of property checks for $n=2048$}
\label{fig:comparison_n2048_quickxplain}
\end{figure}

\begin{algorithm}
	\caption{QuickXplain($U$, ${\cal P}$)}
	\label{alg:quickxplain}
	\begin{algorithmic}[1]
  \Require{${\cal P}(U)$}
	\State \textbf{return} DoQuickXplain($\emptyset$, $\emptyset$, $U$, ${\cal P}$)
	\Statex
	\Function{DoQuickXplain}{$B$, $\Delta$, $C$, ${\cal P}$}
	\If {$\Delta \neq \emptyset$ and ${\cal P}(B)$} 
	\State \textbf{return} $\emptyset$
	\EndIf
	\If {$C = \{ \alpha \} $} 
	\State \textbf{return} $\{ \alpha \}$
	\EndIf
	\State Let $C=\{ \alpha_1,...,\alpha_n \}$ with $\forall i \in [1,n), \alpha_i \prec \alpha_{i+1}$
	\State $k \gets \lceil n/2 \rceil$ \Comment{Split}
	\State $C_1 \gets \{ \alpha_{k},...,\alpha_n \}$
	\State $C_2 \gets \{ \alpha_1,...,\alpha_{k-1} \}$
	\State $\Delta_2 \gets$ DoQuickXplain($B \cup C_1$, $C_1$, $C_2$, ${\cal P}$)
	\State $\Delta_1 \gets$ DoQuickXplain($B \cup \Delta_2$, $\Delta_2$, $C_1$, ${\cal P}$)
	\State \textbf{return} $\Delta_1 \cup \Delta_2$
	\EndFunction
	\end{algorithmic}
\end{algorithm}

We see that {\tt ADEL} consistently performs less checks than {\tt QuickXplain} (between 10\% and 50\% less checks).

Property \ref{prop:average_quickxplain} shows the average complexity for {\tt QuickXplain} in case of a unique minimal subset of unit size. Note that this average complexity turns out to be the average value of the best and worst case complexities (resp. $\log_2(n)$ and 2\ $\log_2(n)$) shown in \cite{Junker2004}. Compared to the $\log_2(n)$ complexity of algorithm {\tt ADEL}, this gives a \nicefrac{3}{2} factor in favor of {\tt ADEL}.

\begin{property}[\propertyAverageQuickXplain]
\label{prop:average_quickxplain} In case of a unique minimal set of size $1$, the average complexity of the {\tt QuickXplain} algorithm is $\nicefrac{3}{2}\ \log_2(n)$. 
\end{property}

In case of a minimal subset of size $n$, {\tt QuickXplain} need to perform $2\ n -2$ property checks that is, the number of nodes of a binary tree with $n$ leaves minus one because the top level node is not checked. Compared with the $n + 2\ \log_2(n)$ complexity of {\tt ADEL} in this case this gives, for large values of $n$ a factor $2$ in favor of {\tt ADEL}.

Figure \ref{fig:experiments_m8} shows, for a fixed value of $m$ ($m=8$) the average number of property checks performed by the different algorithms as a function of the size $n$ of $U$. 

\begin{figure}[ht]
\centering
\includegraphics[width=7cm,keepaspectratio,clip]{experiments_m8.pdf}
\caption{Number of property checks for $m=8$}
\label{fig:experiments_m8}
\end{figure}

\subsection{Random minimal subsets of random size}

For each sample, a set of $c$ subsets of size uniformly selected in $[1,n]$ is generated and the non-minimal subset are eliminated. Curve on Figure \ref{fig:experiments_random_n2048} shows the performance of the different algorithms (number of property checks) for $n=2048$ as a function of the number of subsets $c$ generated. When $c$ grows, the size of the selected minimal subset $X^*$ tends to decrease. The average size of the selected subset is also shown on the figure as an indicator. We can make similar observations as in the previous section: algorithm {\tt ADEL} outperforms all other algorithms.

\begin{figure}[ht]
\centering
\includegraphics[width=8.5cm,keepaspectratio,clip]{experiments_random_n2048.pdf}
\caption{Number of property checks for $n=2048$}
\label{fig:experiments_random_n2048}
\end{figure}

The comparison with algorithm {\tt QuickXplain} is shown on Figure \ref{fig:comparison_random_n2048_quickxplain}. We see that {\tt ADEL} consistently performs less checks than {\tt QuickXplain} (between 10\% and 30\% less checks).

\begin{figure}[ht]
\centering
\includegraphics[width=8.5cm,keepaspectratio,clip]{comparison_random_n2048_quickxplain.pdf}
\caption{Number of property checks for $n=2048$}
\label{fig:comparison_random_n2048_quickxplain}
\end{figure}

\section{Conclusion}
This article studies the problem of finding a minimal subset satisfying an upward-closed property. No assumption is made on the property being checked and the objective is to minimize the number of property checks. The proposed {\tt ADEL} algorithm exploits the statistical properties of the subset element positions. We show that:
\begin{itemize}
\item it is {\em optimal} for small subsets, with a complexity dominated by $\log_2(n)$ for a unique minimal subset of size $1$.
\item it is {\em optimal} for large subsets, with a complexity dominated by $n$ for a minimal subset of size $n$.
\item the average number of checks behaves continuously in between those two extremal cases and outperforms all variants studied in this paper.
\item in the case of a unique minimal set of size one, we show that it performs in average \nicefrac{3}{2} times less checks than the {\tt QuickXplain} algorithm (33\% less checks).
\item in the case of a unique minimal set of size $n$, for large values of $n$, we show that it performs about twice less checks than the {\tt QuickXplain} algorithm (50\% less checks).
\item in the case of a unique minimal set of size $n$, for large values of $n$,  we show that it performs about $\log_2(n)$ less checks than the {\tt DC} algorithm.
\item in between those two extremal cases it consistently performs less property checks than the {\tt QuickXplain} algorithm (between 10\% and 50\% less checks in the instances generated for our experiments).
\item it can trivially be adapted for the case we are looking for a {\em preferred} subset.
\end{itemize} 

\appendix

% ===============================================================
% APPENDIX: PROOFS
% ===============================================================

%\chapter{Proofs}
\section{Proofs}

% ---------------------------------------------------------------
\propertyUniqueSubsetPosition

\begin{proof}
The total number of realizations for positioning the $m$ elements of the unique minimal subset in $U$ is $\binom{n}{m}$. 

From these realizations, the ones that put the $j^{th}$ element of the subset at position $k$ are $\binom{k-1}{j-1}\binom{n-k}{m-j}$. The first term is the number of ways to select the $j-1$ elements before the $j^th$ one and put them before position $k$ while the second term is the number of ways to select the $m-j$ elements after the $j^th$ one and put them after position $k$. 
\end{proof}

% ---------------------------------------------------------------
\propertyUniqueSubsetDistance

\begin{proof}
The total number of realizations for positioning the $m$ elements of the unique minimal subset in $U$ is $\binom{n}{m}$. 

Let $j \in [1,m)$. The number of realizations $r(j,k,i)$ for which the distance between the $j^{th}$ and ${j+1}^{th}$ element of the subset is $k$ and the $j^{th}$ element is at position $i$ is:

\[ r(j,k,i) = \binom{i-1}{j-1} \binom{n-(i+k)}{m-(j+1)} \]

The first factor is the number of ways to distribute the $j-1$ elements before the $j^{th}$ element in the interval $[1,i-1]$ and the second factor is the number of ways to distribute the $m-(j+1)$ elements of the subset after the ${j+1}^{th}$ element in the interval $[i+k+1,n]$.

Thus, the total number of realizations $r(j,k)$ for which the distance between the $j^{th}$ and ${j+1}^{th}$ element of the subset is $k$ is a sum over all possible positions $i$:

\[ r(j,k) = \sum_{i} r(j,k,i) \]

Using the following change of variables: $u=i-1$, $v=j-1$, $\alpha=m-2$, $\beta=n-k-1$, we obtain:

\[ r(j,k) = \sum_{u} \binom{u}{v} \binom{\beta-u}{\alpha-v} \]

Now, we can use the following identity for binomial coefficients:
\[ \forall v \in [0,\alpha], \sum_u \binom{u}{v} \binom{\beta-u}{\alpha-v} = \binom{\beta+1}{\alpha+1} \]

It gives:

\[ r(j,k) = \binom{n-k}{m-1} \]
\end{proof}

% ---------------------------------------------------------------
\propertyDisjointSubsetDistance

\begin{proof}
Let's take a realization for the minimal subset sizes $m_i$, $i \in [1,c]$ so that the minimal subsets are $\{X_1,...,X_c\}$ with $|X_i|=m_i$ and assume $X^*=X_c$. It is clear that if the property holds for each individual realization of subset sizes, it also holds in general.

For $l \in [1,n]$, the number of realizations $r_l(j,k,i)$ for which: (1) the first element of $X^*$ is at position $l$ and (2) the $j^{th}$ element of $X^*$ is at position $i$ and (3) the distance between the $j^{th}$ and ${j+1}^{th}$ element of $X^*$ is $k$ is:

\[ r_l(j,k,i) = \binom{i-l}{j-1} \binom{n-(i+k)}{m-(j+1)} \, A_l \]

The first factor is the number of ways to distribute the $j-1$ elements before the $j^{th}$ element in the interval $[l,i-1]$. The second factor is the number of ways to distribute the $m-(j+1)$ elements of the subset after the ${j+1}^{th}$ element in the interval $[i+k+1,n]$. Finally, $A_l$ is the number of ways to distribute the elements of $X_1 \cup ... \cup X_{c-1}$ such that at least one element of each subset is before position $l$ given that there are $l-1$ free positions before $l$ and $n-l-m_c+1$ free positions after $l$. The key point is that because the minimal subsets are disjoint, $A_l$ does not depend on $i$, $j$ or $k$.

Thus, the number of realizations such that the first element of $X^*$ is at position $l$ and the distance between the $j^{th}$ and ${j+1}^{th}$ element of $X^*$ is $k$ is:

\[r_l(j,k) = \sum_{i} r_l(j,k,i) \] 

Using the following change of variables: $u=i-l$, $v=j-1$, $\alpha=m-2$, $\beta=n-k-l$, we obtain:

\[ r_l(j,k) = A_l\, \sum_{u} \binom{u}{v} \binom{\beta-u}{\alpha-v} \]

Now, we can use the following identity for binomial coefficients:
\[ \forall v \in [0,\alpha], \sum_u \binom{u}{v} \binom{\beta-u}{\alpha-v} = \binom{\beta+1}{\alpha+1} \]

It gives:

\[ r_l(j,k) = A_l\, \binom{n-l-k+1}{m-1} \]

The total number of realizations such that the distance between the $j^{th}$ and ${j+1}^{th}$ element of $X^*$ is $k$ is:

\[ \sum_l r_l(j,k) = \sum_l A_l\, \binom{n-l-k+1}{m-1} \]

This number clearly does not depend on $j$ and is a non-increasing function of $k$. \end{proof}

% ---------------------------------------------------------------
\propertyAlgoCorrectness

\begin{proof}
We need to show that:
\begin{enumerate}
\item the algorithm terminates
\item the subset $X$ returned by Algorithm \ref{alg:alg1} is minimal
\item if $X$ and $Y$ are two minimal subsets such that $X \prec Y$ then $X$ won't be produced by the algorithm
\end{enumerate}

To facilitate the proof we slightly rewrite the algorithm to identify the different iterations $s$ as well as some relevant subsets visited during the iterations (see Algorithm \ref{alg:alg3}).

\begin{algorithm}
	\caption{FindMinimalSubset($U$, ${\cal P}$)}
	\label{alg:alg3}
	\begin{algorithmic}[1]
    \Require ${\cal P}(U)$
		\State $\mathrm{Shuffle}(U)$ 
		\State $s \gets 0$ \Comment{Current step}
		\State $X_0 \gets \emptyset$ 
		\State $i_0 \gets 0$ \Comment{$i_{s}$: index of element added to $X_{s-1}$ at step $s$}
		\Repeat
		  \State $s \gets s+1$
			\State $i_{s} \gets \mathrm{FindNext}(X_{s-1},U,i_{s-1}+1,{\cal P})$
			\State $X_{s} \gets X_{s-1} \cup \{ U[i_{s}] \}$
			\State $V_{s} \gets X_{s-1} \cup U_{i_{s}+1\rightarrow}$
			\State $W_{s} \gets X_{s-1} \cup U_{i_{s}\rightarrow}$ ($ = X_{s} \cup U_{i_{s}+1\rightarrow}$)
		\Until{$(i_s=n) \lor {\cal P}(X_{s})$}
		\State \textbf{return} $X_{s}$
		%\EndProcedure
	\end{algorithmic}
\end{algorithm}

We recap that procedure $\mathrm{FindNext}(X,U,i,{\cal P})$ is in charge of finding and returning the largest index $j$ ($i \leq j \leq n$) such that ${\cal P}(X \cup U_{j\rightarrow})$. This procedure is well defined and guaranteed to return an index as soon as ${\cal P}(X \cup U_{i\rightarrow})$. 

\underline{Termination.}

We will first prove by recursion the existence of $i_{s}$ computed by $\mathrm{FindNext}$ and the fact ${\cal P}(W_s)$. 
\begin{itemize}
\item For iteration $n=1$, $\mathrm{FindNext}(\emptyset,U,1,{\cal P})$ works on the full set $U_{1\rightarrow}=U$ so, given that ${\cal P}(U)$, there exists a largest index $i_1$, $0<i_1$ such that ${\cal P}(\emptyset \cup U_{i_1\rightarrow})$. We have $W_1=\emptyset \cup U_{i_1\rightarrow}$ thus, ${\cal P}(W_1)$.
\item Assuming $i_{s-1}$ exists and ${\cal P}(W_{s-1})$, $\mathrm{FindNext}(X_{s-1},U,i_{s-1}+1,{\cal P})$ works on the set $X_{s-1} \cup U_{i_{s-1}+1 \rightarrow}= W_{s-1}$. As ${\cal P}(W_{s-1})$, there exists a largest index $i_s$, $i_{s-1}< i_s$ such that ${\cal P}(X_{s-1} \cup U_{i_{s} \rightarrow})$. We precisely define $W_s$ as $W_s=X_{s-1} \cup U_{i_{s} \rightarrow}$ thus, ${\cal P}(W_s)$.
\end{itemize}

As for all $s$, $i_{s}$ is well defined and $i_{s-1}<i_{s}$ this shows that $card(X_s)=s$ and that the algorithm necessarily terminates with less than $n$ iterations.

Note that the termination condition $i_s=n$ simply avoids checking ${\cal P}(X_s)$ in a situation when $X_s$ clearly satisfies the property because from the definition of $\mathrm{FindNext}$ we have ${\cal P}(X_{s-1} \cup U_{i_{n} \rightarrow})$ and thus ${\cal P}(X_s)$ because $X_s = X_{s-1} \cup \{ U[i_{s}] \}$.

\underline{Minimality of the returned subset.}

Let $s^*$ denote the number of iterations until termination. We necessarily have ${\cal P}(X_{s^*})$ as this is the stopping condition. Let's show that $X_{s^*}$ is minimal. Suppose there exists a subset $X \subset X_{s^*}$ such that ${\cal P}(X)$. Let $i$ denote the smaller index $i$ such that $u_i \in X_{s^*} \setminus X$ and $s$ the iteration when $u_i$ was added to $X_{s^*}$ ($i_s=i$). Let $V_{s} = X_{s-1} \cup U_{i_{s}+1\rightarrow}$. Because until index $i$ the sets $X$ and $X_{s^*}$ are the same, we have $X \subseteq V_s$. As 
${\cal P}(X)$, we should also have ${\cal P}(V_s)$ but this is inconsistent with the fact $i_s$ is the largest index such that ${\cal P}(X_{s-1} \cup U_{i_{s}\rightarrow})$ and thus $V_{s} = X_{s-1} \cup U_{i_{s}+1\rightarrow}$ does not satisfy ${\cal P}$.

\underline{Maximality with respect to $\prec$.}

Let $X$ and $Y$, $X \neq Y$ be two minimal subsets such that $X \prec Y$. We suppose that $X$ is the subset returned by Algorithm \ref{alg:alg2}. Let $j \in [1,n]$ be the index such that:

\[u_j \in X, u_j \notin Y, \forall i<j, u_i \in X \Leftrightarrow u_i \in Y \]

Let suppose $u_j$ was added to subset $X$ in iteration $s$ of the algorithm. At iteration $s-1$ we had:
$X_{s-1}=\{u_i \in X | i<j \}$. At iteration $s$, as $j$ was selected as the largest index such that ${\cal P}(X_{s-1} \cup U_{j \rightarrow})$, it means $\neg{\cal P}(X_{s-1} \cup U_{j+1 \rightarrow})$. But we have $Y \subseteq X_{s-1} \cup U_{j+1 \rightarrow}$ and this is inconsistent with ${\cal P}(Y)$. 
\end{proof}

% ---------------------------------------------------------------
\propertyComplexityNaiveAlgo

\begin{proof}
Let $l=\pi(X^*,|X^*|)$ and $k=|X^*|$. 

If $l<n$, algorithm \ref{alg:naive} will perform $k$ checks in line 7 of algorithm \ref{alg:alg1} (all index $i$ are such that $i<n$) and $l$ checks in line 4 of algorithm \ref{alg:naive}. 

If $l=n$, the last check in line 7 of algorithm \ref{alg:alg1}  as well as the last check in line 4 of algorithm \ref{alg:naive} are not performed thus the total number of checks is $k+l-2$.
\end{proof}


% ---------------------------------------------------------------
\propertyAverageQuickXplain
\begin{proof}
We suppose $n$ is a power of $2$: $n=2^m$.
We first compute $c(k,n)$ the number of property checks performed by algorithm {\tt QuickXplain} to find a minimal subset of size $1$ at position $k \in [0,n)$.
We clearly have $c(0,1)=0$.
If $k \geq n/2$, the minimal subset belongs to $C_1$ so the first call to {\tt DoQuickXplain} for the first split to compute $\Delta_2$ (left branch) will perform a single check and return $\emptyset$. The call to {\tt DoQuickXplain} to compute $\Delta_1$ (right branch) will require $c(k-n/2, n/2)$ checks.
If $k < n/2$, the left branch will require $1+c(k, n/2)$ checks whereas the right one will only require $1$ check.
So we have:
\begin{eqnarray*}
c(k,n) = \left \lbrace
\begin{array}{l l}
1+c(k-n/2, n/2) & \quad \text{if $k \geq n/2$} \\
2+c(k, n/2)                 & \quad \text{if $k < n/2$}\\
\end{array}
\right.
\end{eqnarray*}

For computing the average over all positions $k$, we compute $s(n)=\sum_{k=0}^{n-1}{c(k,n)}$.

\begin{align*}
s(n) & = \sum_{k=0}^{\nicefrac{n}{2}-1}{c(k,n)} + \sum_{k=\nicefrac{n}{2}}^{n-1}{c(k,n)}\\
     &= 2 (\frac{n}{2}) + \sum_{k=0}^{\nicefrac{n}{2}-1}{c(k,\frac{n}{2})} + (\frac{n}{2}) + \sum_{k=\nicefrac{n}{2}}^{n-1}{c(k-\frac{n}{2},\frac{n}{2})} \\
     &= 3(\frac{n}{2}) + 2\ s(\frac{n}{2})\\
\end{align*}

So we have $s(n)=\nicefrac{3}{2}\ n\ \log_2(n)$ and the average number of checks is $\nicefrac{3}{2}\ \log_2(n)$.
\end{proof}

\noindent

\bibliography{c:/MyOffice/Biblio/biblio}
\end{document}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
